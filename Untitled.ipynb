{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = mx.gpu()\n",
    "batch_size = 100\n",
    "data_shape = (1,28,28)\n",
    "batch_shape = (100,1,28,28)\n",
    "train_iter = mx.io.MNISTIter(\n",
    "        image       = \"../mxnet/mnist/train-images-idx3-ubyte\",\n",
    "        label       = \"../mxnet/mnist/train-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        flat        = False,\n",
    "        ctx = dev)\n",
    "\n",
    "val_iter = mx.io.MNISTIter(\n",
    "        image       = \"../mxnet/mnist/t10k-images-idx3-ubyte\",\n",
    "        label       = \"../mxnet/mnist/t10k-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        flat        = False,\n",
    "        ctx = dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv2d(data, ishape, oshape, kshape, name, stride=(2,2)):\n",
    "    pad0 = (stride[0] * (ishape[-2] - 1) + kshape[0] - oshape[-2])\n",
    "    pad1 = (stride[1] * (ishape[-1] - 1) + kshape[1] - oshape[-1])\n",
    "    assert pad0 >= 0\n",
    "    assert pad1 >= 0\n",
    "    assert pad0 % 2 == 0\n",
    "    assert pad1 % 2 == 0\n",
    "    net = mx.sym.Deconvolution(data,\n",
    "                               kernel=kshape,\n",
    "                               stride=stride,\n",
    "                               pad=(pad0 / 2, pad1/2),\n",
    "                               num_filter=oshape[0],\n",
    "                               no_bias=True,\n",
    "                               name=name)\n",
    "    return net\n",
    "\n",
    "def deconv2d_relu(data, name, **kwargs):\n",
    "    net = deconv2d(data, name=\"%s_deconv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, name = \"%s_act\"%name, act_type = 'relu')\n",
    "    return net\n",
    "def deconv2d_act(data, name, act, **kwargs):\n",
    "    net = deconv2d(data, name=\"%s_deconv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, name = \"%s_act\"%name, act_type = act)\n",
    "    return net\n",
    "\n",
    "def conv2d_relu(data, name, **kwargs):\n",
    "    net = mx.sym.Convolution(data, name = \"%s_conv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, act_type='relu', name = \"%s_act\"%name)\n",
    "    return net\n",
    "def conv2d_act(data, name, act, **kwargs):\n",
    "    net = mx.sym.Convolution(data, name = \"%s_conv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, act_type=act, name = \"%s_act\"%name)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Decoder(oshape, final_act, data = None, ngf=20):\n",
    "    assert oshape[-1]==28\n",
    "    assert oshape[-2]==28\n",
    "    data = mx.sym.Variable('data') if data is None else data\n",
    "    net = mx.sym.FullyConnected(data, name = 'DC_fc1', num_hidden = 8*8*50, no_bias=True)\n",
    "    net = mx.sym.Activation(net, name = 'dc_act1', act_type = 'relu')\n",
    "    net = mx.sym.Reshape(net, shape = (-1, 50, 8, 8))\n",
    "    #net = deconv2d_relu(net, name = 'DC2', ishape = (ngf*4,4,4), oshape=(ngf*2,8,8), kshape = (4,4))\n",
    "    net = deconv2d_relu(net, name = 'DC3', ishape = (50,8,8), oshape=(ngf,14,14), kshape = (4,4))\n",
    "    net = deconv2d_act(net, act = final_act, name = 'DC4', ishape = (ngf,14,14), oshape=oshape, kshape = (4,4))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Encoder(oshape, data=None, ngf=20):\n",
    "    data = mx.sym.Variable('data') if data is None else data\n",
    "    net = conv2d_relu(data, kernel = (4,4), stride = (2,2), pad = (1,1), num_filter=ngf, name = 'EC_conv1')\n",
    "    net = conv2d_relu(net, kernel = (4,4), stride = (2,2), pad = (2,2), num_filter=50, name = 'EC_conv2')\n",
    "    #net = conv2d_relu(net, kernel = (4,4), stride = (2,2), pad = (1,1), num_filter=ngf*4, name = 'EC_conv3')\n",
    "    net = mx.sym.Flatten(net)\n",
    "    net = mx.sym.Activation(net, name = 'EC_act1', act_type = 'relu')\n",
    "    net = mx.sym.FullyConnected(net, num_hidden = oshape[-1], name = 'EC_fc1')\n",
    "    net = mx.sym.Activation(net, name = 'EC_act2', act_type = 'relu')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(data = None, num_hidden=32):\n",
    "    data = mx.sym.Variable('data') if data is None else data\n",
    "    net = mx.sym.FullyConnected(data, num_hidden = num_hidden, name = \"C_fc1\")\n",
    "    net = mx.sym.Activation(net, name = 'C_act1', act_type = 'relu')\n",
    "    net = mx.sym.FullyConnected(net, num_hidden = 10, name = 'C_fc_2')\n",
    "    #net = mx.sym.SoftmaxOutput(net, name = 'C_softmax')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coder_shape = (32,)\n",
    "batch_coder_shape = (batch_size, 32)\n",
    "D_net = Decoder(data_shape, final_act = 'sigmoid')\n",
    "E_net = Encoder(coder_shape)\n",
    "C_net = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_arg_names = D_net.list_arguments()\n",
    "D_arg_shapes, D_output_shapes, D_aux_shapes = D_net.infer_shape(data = batch_coder_shape)\n",
    "D_arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in D_arg_shapes]\n",
    "D_grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in D_arg_shapes]\n",
    "D_aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in D_aux_shapes]\n",
    "D_reqs = [\"write\" for name in D_arg_names]\n",
    "\n",
    "modD = D_net.bind(ctx=dev, args=D_arg_arrays, args_grad = D_grad_arrays, grad_req=D_reqs,  aux_states=D_aux_states)\n",
    "D_arg_map = dict(zip(D_arg_names, D_arg_arrays))\n",
    "D_grad_map = dict(zip(D_arg_names, D_grad_arrays))\n",
    "D_data_grad = D_grad_map[\"data\"]\n",
    "D_out_grad = mx.nd.zeros(modD.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_arg_names = C_net.list_arguments()\n",
    "C_arg_shapes, C_output_shapes, C_aux_shapes = C_net.infer_shape(data = batch_coder_shape)\n",
    "C_arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in C_arg_shapes]\n",
    "C_grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in C_arg_shapes]\n",
    "C_aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in C_aux_shapes]\n",
    "C_reqs = [\"write\" for name in C_arg_names]\n",
    "\n",
    "modC = C_net.bind(ctx=dev, args=C_arg_arrays, args_grad = C_grad_arrays, grad_req=C_reqs,  aux_states=C_aux_states)\n",
    "C_arg_map = dict(zip(C_arg_names, C_arg_arrays))\n",
    "C_grad_map = dict(zip(C_arg_names, C_grad_arrays))\n",
    "C_data_grad = C_grad_map[\"data\"]\n",
    "C_out_grad = mx.nd.zeros(modC.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E_arg_names = E_net.list_arguments()\n",
    "E_arg_shapes, E_output_shapes, E_aux_shapes = E_net.infer_shape(data = batch_shape)\n",
    "E_arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in E_arg_shapes]\n",
    "E_grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in E_arg_shapes]\n",
    "E_aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in E_aux_shapes]\n",
    "E_reqs = [\"write\" for name in E_arg_names]\n",
    "\n",
    "modE = E_net.bind(ctx=dev, args=E_arg_arrays, args_grad = E_grad_arrays, grad_req=E_reqs,  aux_states=E_aux_states)\n",
    "E_arg_map = dict(zip(E_arg_names, E_arg_arrays))\n",
    "E_grad_map = dict(zip(E_arg_names, E_grad_arrays))\n",
    "E_data_grad = C_grad_map[\"data\"]\n",
    "E_out_grad = mx.nd.zeros(modE.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L, 32L)\n",
      "['data', 'DC_fc1_weight', 'DC3_deconv_weight', 'DC4_deconv_weight']\n",
      "(100L, 1L, 28L, 28L)\n"
     ]
    }
   ],
   "source": [
    "print modE.outputs[0].shape\n",
    "print D_arg_names\n",
    "print modD.outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Validate_Adv(val_iter, norm=2, coe= 2):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_sin = 0\n",
    "    num_batch = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train = True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        \n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        C_out_grad[:] = logGrad\n",
    "        modC.backward([C_out_grad])\n",
    "        \n",
    "        modE.backward([C_data_grad])\n",
    "        if norm==2:\n",
    "            noise = E_grad_map['data'].asnumpy()\n",
    "        else:\n",
    "            noise = np.sign(E_grad_map['data'].asnumpy())\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2)==0:\n",
    "                num_sin +=1\n",
    "                noise[j] = mx.rnd.normal(0,0.07, shape = noise[j].shape).asnumpy()\n",
    "            if (label.asnumpy()[j] == np.argmax(alpha[j])):\n",
    "                noise[j] = noise[j]/ np.linalg.norm(noise[j].flatten(), norm)\n",
    "                \n",
    "            else:\n",
    "                noise[j]=0\n",
    "        \n",
    "        data_adv = data.asnumpy() + coe * noise\n",
    "        E_arg_map['data'][:] = data_adv\n",
    "        modE.forward(is_train = False)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy())\n",
    "        num_batch += 1\n",
    "    if num_sin>0: \n",
    "        print ('Number of 0 gradient:', num_sin)\n",
    "    return (val_acc/num_batch)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CalAcc(prob, label):\n",
    "    return np.sum(np.argmax(prob, axis=1)==label)*1.0/batch_size\n",
    "def EDLoss(decoder, data):\n",
    "    res = 0.0\n",
    "    \n",
    "    temp = decoder - data\n",
    "    for j in range(batch_size):\n",
    "        res += mx.nd.norm(temp[j])\n",
    "    return res/batch_size\n",
    "\n",
    "def CLoss(prob, label):\n",
    "    res = 0.0\n",
    "    for j in range(batch_size):\n",
    "        res -= np.log(prob[j][int(label[j])])\n",
    "    return res/batch_size\n",
    "\n",
    "def SGD(weight, grad, lr = 0.05, wd = 0.0001):\n",
    "    weight[:] -= lr*(grad/batch_size + wd*weight) \n",
    "    \n",
    "def softmax(theta):\n",
    "    tmp = theta - np.max(theta, axis=1, keepdims = True)\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims = True)\n",
    "    return exp/norm\n",
    "    \n",
    "def logLossGrad(alpha,label):\n",
    "    res = np.copy(alpha)\n",
    "    for j in range(alpha.shape[0]):\n",
    "        res[j][int(label[j])] -= 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Validate_loss(val_iter):\n",
    "    val_loss = 0.0\n",
    "    val_iter.reset()\n",
    "    nbatch = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train=False)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        D_arg_map['data'][:] = coder\n",
    "        modD.forward(is_train = False)\n",
    "        decoder = modD.outputs[0]\n",
    "        \n",
    "        data_gpu = mx.nd.zeros(shape = data.shape, ctx = dev)\n",
    "        data.copyto(data_gpu)\n",
    "        val_loss += EDLoss(decoder, data_gpu)\n",
    "        nbatch +=1\n",
    "    return val_loss/nbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mx.rnd.seed(17214)\n",
    "for name in E_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = E_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = E_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "for name in C_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = C_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = C_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5195\t Training Accuracy: 0.8312\n",
      "Training Loss: 0.3245\t Training Accuracy: 0.8959\n",
      "epoch: 0 Validation Accuracy: 0.9636\t Adverserial Accuracy: 0.3024\n",
      "Training Loss: 0.0960\t Training Accuracy: 0.9701\n",
      "Training Loss: 0.0837\t Training Accuracy: 0.9742\n",
      "epoch: 1 Validation Accuracy: 0.9759\t Adverserial Accuracy: 0.3529\n",
      "Training Loss: 0.0640\t Training Accuracy: 0.9809\n",
      "Training Loss: 0.0576\t Training Accuracy: 0.9827\n",
      "epoch: 2 Validation Accuracy: 0.9810\t Adverserial Accuracy: 0.3875\n",
      "Training Loss: 0.0467\t Training Accuracy: 0.9861\n",
      "Training Loss: 0.0433\t Training Accuracy: 0.9871\n",
      "epoch: 3 Validation Accuracy: 0.9821\t Adverserial Accuracy: 0.3894\n",
      "Training Loss: 0.0360\t Training Accuracy: 0.9893\n",
      "Training Loss: 0.0335\t Training Accuracy: 0.9900\n",
      "epoch: 4 Validation Accuracy: 0.9851\t Adverserial Accuracy: 0.4067\n",
      "Training Loss: 0.0282\t Training Accuracy: 0.9914\n",
      "Training Loss: 0.0260\t Training Accuracy: 0.9922\n",
      "epoch: 5 Validation Accuracy: 0.9852\t Adverserial Accuracy: 0.4427\n",
      "Training Loss: 0.0229\t Training Accuracy: 0.9928\n",
      "Training Loss: 0.0214\t Training Accuracy: 0.9935\n",
      "epoch: 6 Validation Accuracy: 0.9852\t Adverserial Accuracy: 0.4112\n",
      "Training Loss: 0.0193\t Training Accuracy: 0.9938\n",
      "Training Loss: 0.0179\t Training Accuracy: 0.9944\n",
      "epoch: 7 Validation Accuracy: 0.9845\t Adverserial Accuracy: 0.3832\n",
      "Training Loss: 0.0156\t Training Accuracy: 0.9953\n",
      "Training Loss: 0.0141\t Training Accuracy: 0.9958\n",
      "epoch: 8 Validation Accuracy: 0.9852\t Adverserial Accuracy: 0.4436\n",
      "Training Loss: 0.0152\t Training Accuracy: 0.9953\n",
      "Training Loss: 0.0143\t Training Accuracy: 0.9954\n",
      "epoch: 9 Validation Accuracy: 0.9854\t Adverserial Accuracy: 0.4511\n",
      "Training Loss: 0.0153\t Training Accuracy: 0.9954\n",
      "Training Loss: 0.0136\t Training Accuracy: 0.9959\n",
      "epoch: 10 Validation Accuracy: 0.9844\t Adverserial Accuracy: 0.4787\n",
      "Training Loss: 0.0125\t Training Accuracy: 0.9959\n",
      "Training Loss: 0.0120\t Training Accuracy: 0.9961\n",
      "epoch: 11 Validation Accuracy: 0.9851\t Adverserial Accuracy: 0.4771\n",
      "Training Loss: 0.0092\t Training Accuracy: 0.9971\n",
      "Training Loss: 0.0094\t Training Accuracy: 0.9969\n",
      "epoch: 12 Validation Accuracy: 0.9874\t Adverserial Accuracy: 0.4762\n",
      "Training Loss: 0.0089\t Training Accuracy: 0.9973\n",
      "Training Loss: 0.0079\t Training Accuracy: 0.9976\n",
      "epoch: 13 Validation Accuracy: 0.9873\t Adverserial Accuracy: 0.5115\n",
      "Training Loss: 0.0062\t Training Accuracy: 0.9983\n",
      "Training Loss: 0.0059\t Training Accuracy: 0.9984\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 14 Validation Accuracy: 0.9891\t Adverserial Accuracy: 0.5138\n",
      "Training Loss: 0.0071\t Training Accuracy: 0.9979\n",
      "Training Loss: 0.0058\t Training Accuracy: 0.9983\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 15 Validation Accuracy: 0.9892\t Adverserial Accuracy: 0.5418\n",
      "Training Loss: 0.0038\t Training Accuracy: 0.9990\n",
      "Training Loss: 0.0038\t Training Accuracy: 0.9989\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 16 Validation Accuracy: 0.9866\t Adverserial Accuracy: 0.5424\n",
      "Training Loss: 0.0038\t Training Accuracy: 0.9991\n",
      "Training Loss: 0.0042\t Training Accuracy: 0.9990\n",
      "epoch: 17 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.5410\n",
      "Training Loss: 0.0056\t Training Accuracy: 0.9983\n",
      "Training Loss: 0.0098\t Training Accuracy: 0.9974\n",
      "epoch: 18 Validation Accuracy: 0.9875\t Adverserial Accuracy: 0.5436\n",
      "Training Loss: 0.0054\t Training Accuracy: 0.9983\n",
      "Training Loss: 0.0046\t Training Accuracy: 0.9987\n",
      "epoch: 19 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5498\n",
      "Training Loss: 0.0034\t Training Accuracy: 0.9991\n",
      "Training Loss: 0.0030\t Training Accuracy: 0.9993\n",
      "epoch: 20 Validation Accuracy: 0.9894\t Adverserial Accuracy: 0.5625\n",
      "Training Loss: 0.0014\t Training Accuracy: 0.9999\n",
      "Training Loss: 0.0013\t Training Accuracy: 0.9999\n",
      "epoch: 21 Validation Accuracy: 0.9888\t Adverserial Accuracy: 0.5857\n",
      "Training Loss: 0.0009\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0009\t Training Accuracy: 1.0000\n",
      "epoch: 22 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5970\n",
      "Training Loss: 0.0006\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0007\t Training Accuracy: 1.0000\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 23 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.5901\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0006\t Training Accuracy: 1.0000\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 24 Validation Accuracy: 0.9888\t Adverserial Accuracy: 0.5901\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 25 Validation Accuracy: 0.9888\t Adverserial Accuracy: 0.5855\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "('Number of 0 gradient:', 1)\n",
      "('Number of 0 gradient:', 1)\n",
      "epoch: 26 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5813\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 27 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5786\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 28 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5744\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 29 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5669\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 30 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.5645\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 31 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5628\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 32 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5596\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 33 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5559\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 34 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5536\n",
      "Training Loss: 0.0004\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 35 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5512\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 36 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5477\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 37 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5465\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 38 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5441\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 39 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5410\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 40 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5381\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 41 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5373\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 42 Validation Accuracy: 0.9888\t Adverserial Accuracy: 0.5337\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 43 Validation Accuracy: 0.9887\t Adverserial Accuracy: 0.5323\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 44 Validation Accuracy: 0.9888\t Adverserial Accuracy: 0.5298\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 45 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5267\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 46 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5240\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 47 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5213\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 48 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5195\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 49 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5181\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 50 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5152\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 51 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5143\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 52 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.5119\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 53 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5121\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 54 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.5094\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 55 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5079\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 56 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.5077\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0006\t Training Accuracy: 1.0000\n",
      "epoch: 57 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.5051\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0006\t Training Accuracy: 1.0000\n",
      "epoch: 58 Validation Accuracy: 0.9885\t Adverserial Accuracy: 0.5044\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "Training Loss: 0.0005\t Training Accuracy: 1.0000\n",
      "epoch: 59 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.5009\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 60\n",
    "lr = 0.2\n",
    "\n",
    "Training_normal = np.zeros(shape = (num_epoch))\n",
    "Validation_normal = np.zeros(shape = (num_epoch))\n",
    "Adv_normal = np.zeros(shape = (num_epoch))\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    if i%30==29: \n",
    "        lr = lr/2.0\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        \n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train=True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        #print coder.context, decoder.context, prob.context\n",
    "        \n",
    "        loss_total +=  CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        \n",
    "        C_out_grad[:] = logGrad\n",
    "        modC.backward([C_out_grad])\n",
    "        temp2 = C_grad_map['data']\n",
    "        E_out_grad[:] = temp2\n",
    "        modE.backward([E_out_grad])\n",
    "                \n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "                \n",
    "        for name in E_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(E_arg_map[name], E_grad_map[name], lr)\n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    Training_normal[i] = train_acc/num_batch\n",
    "    Validation_normal[i] = Validate_Adv(val_iter, norm=2, coe=0)\n",
    "    Adv_normal[i] = Validate_Adv(val_iter, norm=2, coe=2)\n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validation_normal[i], Adv_normal[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mx.rnd.seed(3128)\n",
    "for name in E_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = E_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = E_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "for name in D_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = D_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = D_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "for name in C_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = C_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = C_arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.50040483] 0.000837528332179\n",
      "Training Loss: 0.0273\t Training Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.54174566] 0.058087625865\n",
      "Training Loss: inf\t Training Accuracy: 0.9282\n",
      "epoch: 0 Validation Accuracy: 0.9612\t Adverserial Accuracy: 0.3595\n",
      "[ 3.08207774] 0.0867063972341\n",
      "Training Loss: 0.1390\t Training Accuracy: 0.9690\n",
      "[ 2.79468513] 0.016498919891\n",
      "Training Loss: 0.1168\t Training Accuracy: 0.9746\n",
      "epoch: 1 Validation Accuracy: 0.9800\t Adverserial Accuracy: 0.3729\n",
      "[ 2.83800578] 0.0229289301246\n",
      "Training Loss: 0.0818\t Training Accuracy: 0.9836\n",
      "[ 2.62474298] 0.0156534225458\n",
      "Training Loss: 0.0760\t Training Accuracy: 0.9850\n",
      "epoch: 2 Validation Accuracy: 0.9841\t Adverserial Accuracy: 0.3698\n",
      "[ 2.6612916] 0.0189133982289\n",
      "Training Loss: 0.0640\t Training Accuracy: 0.9881\n",
      "[ 2.60176301] 0.00579740682569\n",
      "Training Loss: 0.0608\t Training Accuracy: 0.9892\n",
      "epoch: 3 Validation Accuracy: 0.9855\t Adverserial Accuracy: 0.3543\n",
      "[ 2.52420568] 0.0105934825015\n",
      "Training Loss: 0.0539\t Training Accuracy: 0.9908\n",
      "[ 2.29876471] 0.00501602858311\n",
      "Training Loss: 0.0521\t Training Accuracy: 0.9918\n",
      "epoch: 4 Validation Accuracy: 0.9866\t Adverserial Accuracy: 0.3513\n",
      "[ 2.44141078] 0.00714964611799\n",
      "Training Loss: 0.0473\t Training Accuracy: 0.9926\n",
      "[ 2.268543] 0.00538681568225\n",
      "Training Loss: 0.0460\t Training Accuracy: 0.9934\n",
      "epoch: 5 Validation Accuracy: 0.9875\t Adverserial Accuracy: 0.3686\n",
      "[ 2.35766554] 0.00472495596011\n",
      "Training Loss: 0.0436\t Training Accuracy: 0.9936\n",
      "[ 2.25118828] 0.0035906541912\n",
      "Training Loss: 0.0422\t Training Accuracy: 0.9943\n",
      "epoch: 6 Validation Accuracy: 0.9873\t Adverserial Accuracy: 0.3862\n",
      "[ 2.386379] 0.005465684767\n",
      "Training Loss: 0.0403\t Training Accuracy: 0.9951\n",
      "[ 2.16630459] 0.0139590749331\n",
      "Training Loss: 0.0388\t Training Accuracy: 0.9957\n",
      "epoch: 7 Validation Accuracy: 0.9870\t Adverserial Accuracy: 0.3998\n",
      "[ 2.35244107] 0.00625672410319\n",
      "Training Loss: 0.0365\t Training Accuracy: 0.9966\n",
      "[ 2.44354224] 0.00512670390846\n",
      "Training Loss: 0.0361\t Training Accuracy: 0.9967\n",
      "epoch: 8 Validation Accuracy: 0.9876\t Adverserial Accuracy: 0.4159\n",
      "[ 2.49533606] 0.0102050042346\n",
      "Training Loss: 0.0347\t Training Accuracy: 0.9963\n",
      "[ 2.17356634] 0.00206322544205\n",
      "Training Loss: 0.0356\t Training Accuracy: 0.9962\n",
      "epoch: 9 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.4221\n",
      "[ 2.50452709] 0.00557447153014\n",
      "Training Loss: 0.0339\t Training Accuracy: 0.9966\n",
      "[ 2.11565471] 0.011756230576\n",
      "Training Loss: 0.0349\t Training Accuracy: 0.9964\n",
      "epoch: 10 Validation Accuracy: 0.9866\t Adverserial Accuracy: 0.4550\n",
      "[ 2.30179071] 0.0113498213861\n",
      "Training Loss: 0.0317\t Training Accuracy: 0.9976\n",
      "[ 2.29141235] 0.0113954392693\n",
      "Training Loss: 0.0317\t Training Accuracy: 0.9976\n",
      "epoch: 11 Validation Accuracy: 0.9858\t Adverserial Accuracy: 0.4324\n",
      "[ 2.51191258] 0.00396477857824\n",
      "Training Loss: 0.0319\t Training Accuracy: 0.9971\n",
      "[ 2.09530425] 0.00292059812184\n",
      "Training Loss: 0.0314\t Training Accuracy: 0.9974\n",
      "epoch: 12 Validation Accuracy: 0.9873\t Adverserial Accuracy: 0.4580\n",
      "[ 2.30909014] 0.00113367070278\n",
      "Training Loss: 0.0303\t Training Accuracy: 0.9978\n",
      "[ 2.26635075] 0.000906583833299\n",
      "Training Loss: 0.0299\t Training Accuracy: 0.9981\n",
      "epoch: 13 Validation Accuracy: 0.9871\t Adverserial Accuracy: 0.4947\n",
      "[ 2.2320385] 0.00456976333598\n",
      "Training Loss: 0.0285\t Training Accuracy: 0.9983\n",
      "[ 2.10851908] 0.00331096537161\n",
      "Training Loss: 0.0283\t Training Accuracy: 0.9986\n",
      "epoch: 14 Validation Accuracy: 0.9878\t Adverserial Accuracy: 0.4859\n",
      "[ 2.22679019] 0.00597363369489\n",
      "Training Loss: 0.0280\t Training Accuracy: 0.9986\n",
      "[ 2.19909334] 0.00579200870614\n",
      "Training Loss: 0.0283\t Training Accuracy: 0.9984\n",
      "epoch: 15 Validation Accuracy: 0.9871\t Adverserial Accuracy: 0.5010\n",
      "[ 2.18434167] 0.00157629209077\n",
      "Training Loss: 0.0268\t Training Accuracy: 0.9989\n",
      "[ 2.04872012] 0.00110438866623\n",
      "Training Loss: 0.0273\t Training Accuracy: 0.9988\n",
      "epoch: 16 Validation Accuracy: 0.9876\t Adverserial Accuracy: 0.4871\n",
      "[ 2.34244323] 0.00580605133498\n",
      "Training Loss: 0.0284\t Training Accuracy: 0.9980\n",
      "[ 2.07629585] 0.00419124007775\n",
      "Training Loss: 0.0276\t Training Accuracy: 0.9984\n",
      "epoch: 17 Validation Accuracy: 0.9880\t Adverserial Accuracy: 0.4995\n",
      "[ 2.18878102] 0.0011100996122\n",
      "Training Loss: 0.0260\t Training Accuracy: 0.9991\n",
      "[ 2.09844255] 0.00374004487328\n",
      "Training Loss: 0.0261\t Training Accuracy: 0.9991\n",
      "epoch: 18 Validation Accuracy: 0.9878\t Adverserial Accuracy: 0.4896\n",
      "[ 2.24930763] 0.0230041132673\n",
      "Training Loss: 0.0265\t Training Accuracy: 0.9987\n",
      "[ 2.02483892] 0.00238972588722\n",
      "Training Loss: 0.0263\t Training Accuracy: 0.9988\n",
      "epoch: 19 Validation Accuracy: 0.9889\t Adverserial Accuracy: 0.4852\n",
      "[ 2.34044528] 0.0407846523019\n",
      "Training Loss: 0.0876\t Training Accuracy: 0.9874\n",
      "[ 2.12391567] 0.015477502592\n",
      "Training Loss: 0.0630\t Training Accuracy: 0.9911\n",
      "epoch: 20 Validation Accuracy: 0.9868\t Adverserial Accuracy: 0.3753\n",
      "[ 2.1888864] 0.00359315510149\n",
      "Training Loss: 0.0304\t Training Accuracy: 0.9974\n",
      "[ 2.04438186] 0.00774891705949\n",
      "Training Loss: 0.0290\t Training Accuracy: 0.9980\n",
      "epoch: 21 Validation Accuracy: 0.9876\t Adverserial Accuracy: 0.4657\n",
      "[ 2.27575445] 0.00476325106205\n",
      "Training Loss: 0.0263\t Training Accuracy: 0.9989\n",
      "[ 1.99428892] 0.00386944664949\n",
      "Training Loss: 0.0259\t Training Accuracy: 0.9992\n",
      "epoch: 22 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4719\n",
      "[ 2.15382862] 0.00403414436182\n",
      "Training Loss: 0.0247\t Training Accuracy: 0.9995\n",
      "[ 2.16937017] 0.00251334852562\n",
      "Training Loss: 0.0248\t Training Accuracy: 0.9995\n",
      "epoch: 23 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.4919\n",
      "[ 2.16534185] 0.0115805098583\n",
      "Training Loss: 0.0238\t Training Accuracy: 0.9997\n",
      "[ 1.9857707] 0.00218843195573\n",
      "Training Loss: 0.0240\t Training Accuracy: 0.9997\n",
      "epoch: 24 Validation Accuracy: 0.9886\t Adverserial Accuracy: 0.4784\n",
      "[ 2.26670551] 0.00160120652831\n",
      "Training Loss: 0.0236\t Training Accuracy: 0.9999\n",
      "[ 1.9996115] 0.00434356180512\n",
      "Training Loss: 0.0238\t Training Accuracy: 0.9998\n",
      "epoch: 25 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4895\n",
      "[ 2.19148493] 0.00172061249536\n",
      "Training Loss: 0.0231\t Training Accuracy: 0.9999\n",
      "[ 1.97592688] 0.00101126645572\n",
      "Training Loss: 0.0231\t Training Accuracy: 0.9999\n",
      "epoch: 26 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4822\n",
      "[ 2.15101552] 0.0027320156487\n",
      "Training Loss: 0.0228\t Training Accuracy: 0.9999\n",
      "[ 1.96334624] 0.000837766183007\n",
      "Training Loss: 0.0228\t Training Accuracy: 0.9999\n",
      "epoch: 27 Validation Accuracy: 0.9880\t Adverserial Accuracy: 0.4906\n",
      "[ 2.15345311] 0.000667477630632\n",
      "Training Loss: 0.0225\t Training Accuracy: 1.0000\n",
      "[ 1.98023105] 0.000929219728492\n",
      "Training Loss: 0.0226\t Training Accuracy: 1.0000\n",
      "epoch: 28 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.4801\n",
      "[ 1.99519682] 0.00102928489924\n",
      "Training Loss: 0.0177\t Training Accuracy: 1.0000\n",
      "[ 1.87958109] 0.000554282489951\n",
      "Training Loss: 0.0176\t Training Accuracy: 1.0000\n",
      "epoch: 29 Validation Accuracy: 0.9880\t Adverserial Accuracy: 0.4857\n",
      "[ 1.98670077] 0.000817455481091\n",
      "Training Loss: 0.0175\t Training Accuracy: 1.0000\n",
      "[ 1.87489986] 0.000509177718852\n",
      "Training Loss: 0.0174\t Training Accuracy: 1.0000\n",
      "epoch: 30 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.4933\n",
      "[ 1.9802376] 0.000854234049468\n",
      "Training Loss: 0.0173\t Training Accuracy: 1.0000\n",
      "[ 1.86994028] 0.000508193560954\n",
      "Training Loss: 0.0173\t Training Accuracy: 1.0000\n",
      "epoch: 31 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.4953\n",
      "[ 1.97451246] 0.000827582613426\n",
      "Training Loss: 0.0172\t Training Accuracy: 1.0000\n",
      "[ 1.86521006] 0.000481770572548\n",
      "Training Loss: 0.0172\t Training Accuracy: 1.0000\n",
      "epoch: 32 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.4969\n",
      "[ 1.96908021] 0.000835590321072\n",
      "Training Loss: 0.0171\t Training Accuracy: 1.0000\n",
      "[ 1.86157322] 0.000486146421295\n",
      "Training Loss: 0.0171\t Training Accuracy: 1.0000\n",
      "epoch: 33 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.4972\n",
      "[ 1.96427965] 0.000800863582097\n",
      "Training Loss: 0.0170\t Training Accuracy: 1.0000\n",
      "[ 1.85848796] 0.000489461151424\n",
      "Training Loss: 0.0171\t Training Accuracy: 1.0000\n",
      "epoch: 34 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4954\n",
      "[ 1.96009159] 0.000824285897929\n",
      "Training Loss: 0.0170\t Training Accuracy: 1.0000\n",
      "[ 1.85488605] 0.000463756051969\n",
      "Training Loss: 0.0170\t Training Accuracy: 1.0000\n",
      "epoch: 35 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4964\n",
      "[ 1.95632172] 0.000848266306003\n",
      "Training Loss: 0.0169\t Training Accuracy: 1.0000\n",
      "[ 1.85262251] 0.000472637372457\n",
      "Training Loss: 0.0170\t Training Accuracy: 1.0000\n",
      "epoch: 36 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4949\n",
      "[ 1.95176935] 0.000842616074266\n",
      "Training Loss: 0.0169\t Training Accuracy: 1.0000\n",
      "[ 1.84956419] 0.000447565844\n",
      "Training Loss: 0.0169\t Training Accuracy: 1.0000\n",
      "epoch: 37 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4943\n",
      "[ 1.94855011] 0.00083961216291\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "[ 1.84699571] 0.000454896786273\n",
      "Training Loss: 0.0169\t Training Accuracy: 1.0000\n",
      "epoch: 38 Validation Accuracy: 0.9880\t Adverserial Accuracy: 0.4951\n",
      "[ 1.94543397] 0.000828667998739\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "[ 1.84406948] 0.000439290546\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "epoch: 39 Validation Accuracy: 0.9879\t Adverserial Accuracy: 0.4968\n",
      "[ 1.94119036] 0.000871478114797\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "[ 1.84116197] 0.000444201972302\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "epoch: 40 Validation Accuracy: 0.9878\t Adverserial Accuracy: 0.5012\n",
      "[ 1.93915117] 0.000837261759238\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "[ 1.83788681] 0.000429655850234\n",
      "Training Loss: 0.0168\t Training Accuracy: 1.0000\n",
      "epoch: 41 Validation Accuracy: 0.9878\t Adverserial Accuracy: 0.5010\n",
      "[ 1.93781865] 0.000804000189333\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "[ 1.83517134] 0.000465415873412\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "epoch: 42 Validation Accuracy: 0.9879\t Adverserial Accuracy: 0.4980\n",
      "[ 1.93526781] 0.00088192748069\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "[ 1.832968] 0.000455555994451\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "epoch: 43 Validation Accuracy: 0.9878\t Adverserial Accuracy: 0.4937\n",
      "[ 1.93344784] 0.000809599997691\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "[ 1.82974362] 0.000464047072648\n",
      "Training Loss: 0.0167\t Training Accuracy: 1.0000\n",
      "epoch: 44 Validation Accuracy: 0.9879\t Adverserial Accuracy: 0.4998\n",
      "[ 1.93096852] 0.000844591873478\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "[ 1.82754028] 0.000484696956659\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "epoch: 45 Validation Accuracy: 0.9879\t Adverserial Accuracy: 0.4939\n",
      "[ 1.92900145] 0.000819482246337\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "[ 1.82599247] 0.000491644386345\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "epoch: 46 Validation Accuracy: 0.9879\t Adverserial Accuracy: 0.4984\n",
      "[ 1.92769122] 0.000858570587416\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "[ 1.82320368] 0.000500123613225\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "epoch: 47 Validation Accuracy: 0.9880\t Adverserial Accuracy: 0.4937\n",
      "[ 1.92588663] 0.000836318268927\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "[ 1.82067811] 0.000516847502382\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "epoch: 48 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4899\n",
      "[ 1.92408907] 0.000876419719915\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "[ 1.81849897] 0.000513038428584\n",
      "Training Loss: 0.0166\t Training Accuracy: 1.0000\n",
      "epoch: 49 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4943\n",
      "[ 1.92288387] 0.000847105309755\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "[ 1.81548893] 0.00054347255472\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "epoch: 50 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4997\n",
      "[ 1.92073047] 0.000810313444946\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "[ 1.81329119] 0.000533389489828\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "epoch: 51 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4985\n",
      "[ 1.91783404] 0.000898598050748\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.81089294] 0.000533887183947\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "epoch: 52 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4939\n",
      "[ 1.91732013] 0.000834841434634\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.80852091] 0.000563109959524\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "epoch: 53 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4892\n",
      "[ 1.91572201] 0.000818681870521\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.80783308] 0.000535111083556\n",
      "Training Loss: 0.0165\t Training Accuracy: 1.0000\n",
      "epoch: 54 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4887\n",
      "[ 1.91387451] 0.000844771209358\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.80695629] 0.00057066227073\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "epoch: 55 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4877\n",
      "[ 1.91283214] 0.000817711118296\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.80490375] 0.000607971943085\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "epoch: 56 Validation Accuracy: 0.9884\t Adverserial Accuracy: 0.4876\n",
      "[ 1.91154134] 0.000842590231331\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "[ 1.80484128] 0.000570621792749\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "epoch: 57 Validation Accuracy: 0.9882\t Adverserial Accuracy: 0.4843\n",
      "[ 1.90924823] 0.000799318937898\n",
      "Training Loss: 0.0163\t Training Accuracy: 1.0000\n",
      "[ 1.80052733] 0.000613145659119\n",
      "Training Loss: 0.0164\t Training Accuracy: 1.0000\n",
      "epoch: 58 Validation Accuracy: 0.9883\t Adverserial Accuracy: 0.4858\n",
      "[ 1.8891654] 0.000631992630977\n",
      "Training Loss: 0.0136\t Training Accuracy: 1.0000\n",
      "[ 1.78037143] 0.000432214262606\n",
      "Training Loss: 0.0136\t Training Accuracy: 1.0000\n",
      "epoch: 59 Validation Accuracy: 0.9881\t Adverserial Accuracy: 0.4768\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 60\n",
    "lr = 0.1\n",
    "c = 0.01\n",
    "\n",
    "Training_DC = np.zeros(shape = (num_epoch))\n",
    "Validation_DC = np.zeros(shape = (num_epoch))\n",
    "Adv_DC = np.zeros(shape = (num_epoch))\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    if i%30==29: \n",
    "        lr = lr/2.0\n",
    "        c = c/1.2\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        \n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train=True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        D_arg_map['data'][:] = coder\n",
    "        modD.forward(is_train=True)\n",
    "        decoder = modD.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        #print coder.context, decoder.context, prob.context\n",
    "        \n",
    "        data_gpu = mx.nd.zeros(shape = data.shape, ctx = dev)\n",
    "        data.copyto(data_gpu)\n",
    "        \n",
    "        loss_total += c* EDLoss(decoder, data_gpu).asnumpy() + CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        \n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        D_out_grad[:] = decoder-data_gpu \n",
    "        C_out_grad[:] = logGrad\n",
    "        \n",
    "        modD.backward([D_out_grad])\n",
    "        modC.backward([C_out_grad])\n",
    "        temp1 = D_grad_map['data']\n",
    "        temp2 = C_grad_map['data']\n",
    "        E_out_grad[:] = 2*c*temp1+temp2\n",
    "        modE.backward([E_out_grad])\n",
    "        \n",
    "        for name in D_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(D_arg_map[name], D_grad_map[name], lr)\n",
    "                \n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "                \n",
    "        for name in E_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(E_arg_map[name], E_grad_map[name], lr)\n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print EDLoss(decoder, data_gpu).asnumpy(), CLoss(alpha, label.asnumpy())\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    Training_DC[i] = train_acc/num_batch\n",
    "    Validation_DC[i] = Validate_Adv(val_iter, norm=2, coe=0)\n",
    "    Adv_DC[i] = Validate_Adv(val_iter, norm=2, coe=2)\n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validation_DC[i], Adv_DC[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in E_arg_names:\n",
    "    print np.linalg.norm(E_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(E_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8\n",
    "print \"=\"*18\n",
    "for name in D_arg_names:\n",
    "    print np.linalg.norm(D_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(D_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8\n",
    "print \"=\"*18\n",
    "for name in C_arg_names:\n",
    "    print np.linalg.norm(C_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(C_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mx.rnd.seed(12229)\n",
    "for name in E_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = E_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = E_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "for name in D_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = D_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = D_arg_map[name]\n",
    "        arr[:] = 0.\n",
    "for name in C_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = C_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = C_arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "lr = 0.015\n",
    "c = 0.1\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        \n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train=True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        D_arg_map['data'][:] = coder\n",
    "        modD.forward(is_train=True)\n",
    "        decoder = modD.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        #print coder.context, decoder.context, prob.context\n",
    "        \n",
    "        data_gpu = mx.nd.zeros(shape = data.shape, ctx = dev)\n",
    "        data.copyto(data_gpu)\n",
    "        \n",
    "        loss_total += c* EDLoss(decoder, data_gpu).asnumpy() + CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        \n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        D_out_grad[:] = decoder-data_gpu \n",
    "        C_out_grad[:] = logGrad\n",
    "        \n",
    "        modD.backward([D_out_grad])\n",
    "        modC.backward([C_out_grad])\n",
    "        temp1 = D_grad_map['data']\n",
    "        temp2 = C_grad_map['data']\n",
    "        E_out_grad[:] = 2*c*temp1+temp2\n",
    "        modE.backward([E_out_grad])\n",
    "        \n",
    "        for name in D_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(D_arg_map[name], D_grad_map[name], lr)\n",
    "                \n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "                \n",
    "        for name in E_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(E_arg_map[name], E_grad_map[name], lr)\n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print EDLoss(decoder, data_gpu).asnumpy(), CLoss(alpha, label.asnumpy())\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    \n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validate_Adv(val_iter, norm=2, coe=0),Validate_Adv(val_iter, norm=2, coe=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 200\n",
    "lr = 0.3\n",
    "c = 0.1\n",
    "coe = 0.7\n",
    "\n",
    "Training_ACC_adv = np.zeros(shape = (num_epoch))\n",
    "Validation_ACC_adv = np.zeros(shape = (num_epoch))\n",
    "Adv_ACC_adv = np.zeros(shape = (num_epoch))\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    if i%50==49: \n",
    "        lr = lr/1.5\n",
    "        c = c*1.2\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        ##================Create adv examples\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train = True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        \n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        C_out_grad[:] = logGrad\n",
    "        modC.backward([C_out_grad])\n",
    "        \n",
    "        modE.backward([C_data_grad])\n",
    "        noise = E_grad_map['data'].asnumpy()\n",
    "        \n",
    "        num_sin=0\n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2)==0:\n",
    "                num_sin +=1\n",
    "                noise[j] = mx.rnd.normal(0, 0.07, shape = noise[j].shape).asnumpy()\n",
    "            elif (label.asnumpy()[j] == np.argmax(alpha[j])):\n",
    "                noise[j] = noise[j]/ np.linalg.norm(noise[j].flatten(), 2)\n",
    "                \n",
    "            else:\n",
    "                noise[j]=0\n",
    "        \n",
    "        data_adv = data.asnumpy() + coe * noise\n",
    "        ##===============================================\n",
    "        \n",
    "        E_arg_map['data'][:] = data_adv\n",
    "        modE.forward(is_train=True)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        D_arg_map['data'][:] = coder\n",
    "        modD.forward(is_train=True)\n",
    "        decoder = modD.outputs[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = coder\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        #print coder.context, decoder.context, prob.context\n",
    "        \n",
    "        data_gpu = mx.nd.zeros(shape = data.shape, ctx = dev)\n",
    "        data.copyto(data_gpu)\n",
    "        \n",
    "        loss_total += c* EDLoss(decoder, data_gpu).asnumpy() + CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        \n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        D_out_grad[:] = decoder-data_gpu \n",
    "        C_out_grad[:] = logGrad\n",
    "        \n",
    "        modD.backward([D_out_grad])\n",
    "        modC.backward([C_out_grad])\n",
    "        temp1 = D_grad_map['data']\n",
    "        temp2 = C_grad_map['data']\n",
    "        E_out_grad[:] = 2*c*temp1+temp2\n",
    "        modE.backward([E_out_grad])\n",
    "        \n",
    "        for name in D_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(D_arg_map[name], D_grad_map[name], lr)\n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "        for name in E_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(E_arg_map[name], E_grad_map[name], lr)\n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            if num_sin>0: print \"Number of 0 Gradient: %d\" %num_sin\n",
    "            print EDLoss(decoder, data_gpu).asnumpy(), CLoss(alpha, label.asnumpy())\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    \n",
    "    Training_ACC_adv[i] = train_acc/num_batch\n",
    "    Validation_ACC_adv[i] = Validate_Adv(val_iter, norm=2, coe=0)\n",
    "    Adv_ACC_adv[i] = Validate_Adv(val_iter, norm=2, coe=2)\n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validation_ACC_adv[i],Adv_ACC_adv[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in E_arg_names:\n",
    "    print np.linalg.norm(E_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(E_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8\n",
    "print \"=\"*18\n",
    "for name in D_arg_names:\n",
    "    print np.linalg.norm(D_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(D_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8\n",
    "print \"=\"*18\n",
    "for name in C_arg_names:\n",
    "    print np.linalg.norm(C_arg_map[name].asnumpy().flatten(),2)\n",
    "    print np.linalg.norm(C_grad_map[name].asnumpy().flatten(),2)\n",
    "    print \"=\"*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array([[1,2,3],[2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = tt/13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07692308,  0.15384615,  0.23076923],\n",
       "       [ 0.15384615,  0.23076923,  0.30769231]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.floor(tt*16)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0625,  0.125 ,  0.1875],\n",
       "       [ 0.125 ,  0.1875,  0.25  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
