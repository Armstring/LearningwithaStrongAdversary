{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev = mx.gpu()\n",
    "batch_size = 100\n",
    "data_shape = (1, 28, 28)\n",
    "\n",
    "train_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/train-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/train-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        flat        = False)\n",
    "\n",
    "val_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/t10k-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/t10k-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        flat        = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(theta):\n",
    "    max_val = np.max(theta, axis=1, keepdims=True)\n",
    "    tmp = theta - max_val\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / norm\n",
    "    \n",
    "def LogLossGrad(alpha, label):\n",
    "    grad = np.copy(alpha)\n",
    "    for i in range(alpha.shape[0]):\n",
    "        grad[i, label[i]] -= 1.\n",
    "    return grad\n",
    "\n",
    "def SGD(weight, grad, lr=0.1, grad_norm=batch_size):\n",
    "    weight[:] -= lr * grad / batch_size\n",
    "\n",
    "def CalAcc(pred_prob, label):\n",
    "    pred = np.argmax(pred_prob, axis=1)\n",
    "    return np.sum(pred == label) * 1.0\n",
    "\n",
    "def CalLoss(pred_prob, label):\n",
    "    loss = 0.\n",
    "    for i in range(pred_prob.shape[0]):\n",
    "        loss += -np.log(max(pred_prob[i, label[i]], 1e-10))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acc_normal(model, val_iter, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    return(val_acc / num_samp)\n",
    "    \n",
    "def acc_perb_L0(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = np.sign(grad_map[\"data\"].asnumpy())\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "            \n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    if  nn>0:\n",
    "        print('L0 gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)\n",
    "\n",
    "def acc_perb_L2(model, val_iter, coe_pb, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_batch = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])):\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /  batch_size \n",
    "        num_batch += 1\n",
    "    if  nn>0:\n",
    "        print('L2 gradien being 0 :', nn)\n",
    "    return(val_acc / num_batch)\n",
    "\n",
    "\n",
    "def acc_perb_alpha(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        T = np.zeros((10, batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "        noise = np.zeros(data.shape)\n",
    "        #===================\n",
    "        for i in range(10):\n",
    "            arg_map[\"data\"][:] = data   \n",
    "            model.forward(is_train=True)\n",
    "            theta = model.outputs[0].asnumpy()\n",
    "            alpha = Softmax(theta)\n",
    "            \n",
    "            grad = LogLossGrad(alpha, i*np.ones(alpha.shape[0]))\n",
    "            for j in range(batch_size):\n",
    "                grad[j] = -alpha[j][i]*grad[j]\n",
    "            out_grad[:] = grad\n",
    "            model.backward([out_grad])\n",
    " \n",
    "            T[i] = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])):\n",
    "                perb_scale = np.zeros(10)\n",
    "                for i in range(10):\n",
    "                    if (i == y):\n",
    "                        perb_scale[i] = np.inf\n",
    "                    else:\n",
    "                        perb_scale[i] = (alpha[j][y] - alpha[j][i])/np.linalg.norm((T[i][j]-T[y][j]).flatten(),2)\n",
    "                noise[j] = T[np.argmin(perb_scale)][j]-T[y][j]\n",
    "        #====================\n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            else:\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /batch_size\n",
    "        num_samp += 1\n",
    "    if  nn>0:\n",
    "        print('Alpha gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.symbol.Variable('data')\n",
    "\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=data)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "relu1 = mx.symbol.Activation(data=fc1, act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=100)\n",
    "relu2 = mx.symbol.Activation(data=fc2, act_type=\"relu\")\n",
    "\n",
    "# second fullc\n",
    "fc3 = mx.symbol.FullyConnected(data=relu2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc3.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc3.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc3.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8073\t Train Loss: 0.69494\n",
      "Train Accuracy: 0.9251\t Train Loss: 0.25785\n",
      "Train Accuracy: 0.9454\t Train Loss: 0.18794\n",
      "Train Accuracy: 0.9568\t Train Loss: 0.14847\n",
      "Train Accuracy: 0.9641\t Train Loss: 0.12197\n",
      "Train Accuracy: 0.9701\t Train Loss: 0.10288\n",
      "Train Accuracy: 0.9746\t Train Loss: 0.08853\n",
      "Train Accuracy: 0.9780\t Train Loss: 0.07745\n",
      "Train Accuracy: 0.9808\t Train Loss: 0.06838\n",
      "Train Accuracy: 0.9830\t Train Loss: 0.06078\n",
      "Train Accuracy: 0.9850\t Train Loss: 0.05423\n",
      "Train Accuracy: 0.9868\t Train Loss: 0.04839\n",
      "Train Accuracy: 0.9886\t Train Loss: 0.04338\n",
      "Train Accuracy: 0.9900\t Train Loss: 0.03876\n",
      "Train Accuracy: 0.9911\t Train Loss: 0.03477\n",
      "Train Accuracy: 0.9923\t Train Loss: 0.03109\n",
      "Train Accuracy: 0.9934\t Train Loss: 0.02783\n",
      "Train Accuracy: 0.9943\t Train Loss: 0.02477\n",
      "Train Accuracy: 0.9953\t Train Loss: 0.02215\n",
      "Train Accuracy: 0.9961\t Train Loss: 0.01968\n",
      "Train Accuracy: 0.9969\t Train Loss: 0.01745\n",
      "Train Accuracy: 0.9975\t Train Loss: 0.01553\n",
      "Train Accuracy: 0.9980\t Train Loss: 0.01372\n",
      "Train Accuracy: 0.9986\t Train Loss: 0.01219\n",
      "Train Accuracy: 0.9988\t Train Loss: 0.01082\n",
      "Train Accuracy: 0.9990\t Train Loss: 0.00966\n",
      "Train Accuracy: 0.9991\t Train Loss: 0.00864\n",
      "Train Accuracy: 0.9994\t Train Loss: 0.00772\n",
      "Train Accuracy: 0.9995\t Train Loss: 0.00699\n",
      "Train Accuracy: 0.9996\t Train Loss: 0.00630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    print(\"Train Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy:  0.9782\n",
      "Val Batch Accuracy after pertubation:  0.1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "val_iter.reset()\n",
    "val_acc = 0.0\n",
    "val_acc_pb = 0.0\n",
    "coe_pb = 1.5\n",
    "num_samp = 0\n",
    "\n",
    "perb_data = []\n",
    "perb_lab = []\n",
    "\n",
    "for dbatch in val_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    #########\n",
    "    grad = LogLossGrad(alpha, label.asnumpy())\n",
    "    out_grad[:] = grad\n",
    "    model.backward([out_grad])\n",
    "    noise = data_grad.asnumpy()\n",
    "    for j in range(batch_size):\n",
    "        noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "    pdata = data.asnumpy() + coe_pb * noise\n",
    "    arg_map[\"data\"][:] = pdata\n",
    "    model.forward(is_train=True)\n",
    "    raw_output = model.outputs[0].asnumpy()\n",
    "    pred = Softmax(raw_output)\n",
    "    val_acc_pb += CalAcc(pred, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "    \n",
    "    perb_data.append(pdata)\n",
    "    perb_lab.append(label.asnumpy())\n",
    "print(\"Val Batch Accuracy: \", val_acc / num_samp)\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc_pb / num_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdata = np.concatenate(perb_data, axis = 0)\n",
    "plabel = np.concatenate(perb_lab, axis = 0)\n",
    "perb_iter = mx.io.NDArrayIter(\n",
    "    data = pdata,\n",
    "    label = plabel,\n",
    "    batch_size = 100,\n",
    "    shuffle = False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy after pertubation:  0.1947\n"
     ]
    }
   ],
   "source": [
    "perb_iter.reset()\n",
    "num_samp = 0\n",
    "val_acc = 0.0\n",
    "for dbatch in perb_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.symbol.Variable('data')\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=data)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "relu1 = mx.symbol.Activation(data=fc1, act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=100)\n",
    "relu2 = mx.symbol.Activation(data=fc2, act_type=\"relu\")\n",
    "\n",
    "# second fullc\n",
    "fc3 = mx.symbol.FullyConnected(data=relu2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc3.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc3.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc3.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8096\t Val Accuracy: 0.9143\t Train Loss: 0.65890\n",
      "Train Accuracy: 0.9245\t Val Accuracy: 0.9390\t Train Loss: 0.25878\n",
      "Train Accuracy: 0.9446\t Val Accuracy: 0.9514\t Train Loss: 0.18797\n",
      "Train Accuracy: 0.9568\t Val Accuracy: 0.9584\t Train Loss: 0.14671\n",
      "Train Accuracy: 0.9647\t Val Accuracy: 0.9632\t Train Loss: 0.11978\n",
      "Train Accuracy: 0.9711\t Val Accuracy: 0.9666\t Train Loss: 0.10056\n",
      "Train Accuracy: 0.9753\t Val Accuracy: 0.9690\t Train Loss: 0.08656\n",
      "Train Accuracy: 0.9789\t Val Accuracy: 0.9708\t Train Loss: 0.07536\n",
      "Train Accuracy: 0.9815\t Val Accuracy: 0.9729\t Train Loss: 0.06649\n",
      "Train Accuracy: 0.9836\t Val Accuracy: 0.9728\t Train Loss: 0.05914\n",
      "Train Accuracy: 0.9854\t Val Accuracy: 0.9739\t Train Loss: 0.05305\n",
      "Train Accuracy: 0.9870\t Val Accuracy: 0.9742\t Train Loss: 0.04754\n",
      "Train Accuracy: 0.9886\t Val Accuracy: 0.9742\t Train Loss: 0.04274\n",
      "Train Accuracy: 0.9899\t Val Accuracy: 0.9750\t Train Loss: 0.03850\n",
      "Train Accuracy: 0.9914\t Val Accuracy: 0.9750\t Train Loss: 0.03461\n",
      "Train Accuracy: 0.9925\t Val Accuracy: 0.9754\t Train Loss: 0.03105\n",
      "Train Accuracy: 0.9936\t Val Accuracy: 0.9749\t Train Loss: 0.02800\n",
      "Train Accuracy: 0.9945\t Val Accuracy: 0.9752\t Train Loss: 0.02505\n",
      "Train Accuracy: 0.9953\t Val Accuracy: 0.9757\t Train Loss: 0.02245\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9760\t Train Loss: 0.02009\n",
      "Train Accuracy: 0.9966\t Val Accuracy: 0.9761\t Train Loss: 0.01791\n",
      "Train Accuracy: 0.9971\t Val Accuracy: 0.9763\t Train Loss: 0.01601\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9763\t Train Loss: 0.01425\n",
      "Train Accuracy: 0.9982\t Val Accuracy: 0.9765\t Train Loss: 0.01270\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9766\t Train Loss: 0.01133\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9761\t Train Loss: 0.01007\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9761\t Train Loss: 0.00899\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9762\t Train Loss: 0.00798\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9761\t Train Loss: 0.00714\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9763\t Train Loss: 0.00642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9770\t Train Loss: 0.00614\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9770\t Train Loss: 0.00555\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9770\t Train Loss: 0.00533\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9771\t Train Loss: 0.00518\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9769\t Train Loss: 0.00506\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9770\t Train Loss: 0.00495\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9771\t Train Loss: 0.00485\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9769\t Train Loss: 0.00477\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9769\t Train Loss: 0.00469\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9768\t Train Loss: 0.00461\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9768\t Train Loss: 0.00454\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9768\t Train Loss: 0.00447\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9768\t Train Loss: 0.00441\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9767\t Train Loss: 0.00435\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9767\t Train Loss: 0.00429\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9767\t Train Loss: 0.00423\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9769\t Train Loss: 0.00418\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9768\t Train Loss: 0.00413\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9768\t Train Loss: 0.00408\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9767\t Train Loss: 0.00403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.01\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name],lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.977\n",
      "Fixed set perturbation: 0.361\n",
      "L0 perturbation: 0.287\n",
      "L2 perturbation: 0.201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.symbol.Variable('data')\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=data)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=200)\n",
    "relu1 = mx.symbol.Activation(data=fc1, act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=200)\n",
    "relu2 = mx.symbol.Activation(data=fc2, act_type=\"relu\")\n",
    "dropout1 = mx.symbol.Dropout(data=relu2, p=0.5)\n",
    "# second fullc\n",
    "fc3 = mx.symbol.FullyConnected(data=dropout1, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc3.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc3.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc3.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8128\t Val Accuracy: 0.9254\t Train Loss: 0.64124\n",
      "Train Accuracy: 0.9200\t Val Accuracy: 0.9455\t Train Loss: 0.27623\n",
      "Train Accuracy: 0.9415\t Val Accuracy: 0.9567\t Train Loss: 0.20205\n",
      "Train Accuracy: 0.9525\t Val Accuracy: 0.9622\t Train Loss: 0.16100\n",
      "Train Accuracy: 0.9611\t Val Accuracy: 0.9672\t Train Loss: 0.13313\n",
      "Train Accuracy: 0.9671\t Val Accuracy: 0.9698\t Train Loss: 0.11352\n",
      "Train Accuracy: 0.9712\t Val Accuracy: 0.9716\t Train Loss: 0.09879\n",
      "Train Accuracy: 0.9753\t Val Accuracy: 0.9733\t Train Loss: 0.08655\n",
      "Train Accuracy: 0.9774\t Val Accuracy: 0.9752\t Train Loss: 0.07768\n",
      "Train Accuracy: 0.9792\t Val Accuracy: 0.9773\t Train Loss: 0.06974\n",
      "Train Accuracy: 0.9817\t Val Accuracy: 0.9784\t Train Loss: 0.06287\n",
      "Train Accuracy: 0.9825\t Val Accuracy: 0.9790\t Train Loss: 0.05925\n",
      "Train Accuracy: 0.9843\t Val Accuracy: 0.9782\t Train Loss: 0.05370\n",
      "Train Accuracy: 0.9851\t Val Accuracy: 0.9783\t Train Loss: 0.05061\n",
      "Train Accuracy: 0.9868\t Val Accuracy: 0.9792\t Train Loss: 0.04545\n",
      "Train Accuracy: 0.9879\t Val Accuracy: 0.9797\t Train Loss: 0.04092\n",
      "Train Accuracy: 0.9888\t Val Accuracy: 0.9799\t Train Loss: 0.03850\n",
      "Train Accuracy: 0.9896\t Val Accuracy: 0.9788\t Train Loss: 0.03499\n",
      "Train Accuracy: 0.9907\t Val Accuracy: 0.9820\t Train Loss: 0.03140\n",
      "Train Accuracy: 0.9912\t Val Accuracy: 0.9809\t Train Loss: 0.02956\n",
      "Train Accuracy: 0.9921\t Val Accuracy: 0.9808\t Train Loss: 0.02806\n",
      "Train Accuracy: 0.9923\t Val Accuracy: 0.9799\t Train Loss: 0.02577\n",
      "Train Accuracy: 0.9933\t Val Accuracy: 0.9813\t Train Loss: 0.02339\n",
      "Train Accuracy: 0.9938\t Val Accuracy: 0.9806\t Train Loss: 0.02154\n",
      "Train Accuracy: 0.9937\t Val Accuracy: 0.9808\t Train Loss: 0.02133\n",
      "Train Accuracy: 0.9939\t Val Accuracy: 0.9807\t Train Loss: 0.02014\n",
      "Train Accuracy: 0.9952\t Val Accuracy: 0.9801\t Train Loss: 0.01800\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9799\t Train Loss: 0.01570\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9803\t Train Loss: 0.01596\n",
      "Train Accuracy: 0.9960\t Val Accuracy: 0.9810\t Train Loss: 0.01506\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9808\t Train Loss: 0.01432\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9809\t Train Loss: 0.01311\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9821\t Train Loss: 0.01258\n",
      "Train Accuracy: 0.9973\t Val Accuracy: 0.9819\t Train Loss: 0.01101\n",
      "Train Accuracy: 0.9970\t Val Accuracy: 0.9798\t Train Loss: 0.01106\n",
      "Train Accuracy: 0.9974\t Val Accuracy: 0.9810\t Train Loss: 0.00993\n",
      "Train Accuracy: 0.9974\t Val Accuracy: 0.9820\t Train Loss: 0.00968\n",
      "Train Accuracy: 0.9975\t Val Accuracy: 0.9824\t Train Loss: 0.00984\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9816\t Train Loss: 0.00913\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9812\t Train Loss: 0.00838\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9818\t Train Loss: 0.00810\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9820\t Train Loss: 0.00831\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9816\t Train Loss: 0.00820\n",
      "Train Accuracy: 0.9983\t Val Accuracy: 0.9799\t Train Loss: 0.00693\n",
      "Train Accuracy: 0.9982\t Val Accuracy: 0.9824\t Train Loss: 0.00698\n",
      "Train Accuracy: 0.9985\t Val Accuracy: 0.9818\t Train Loss: 0.00628\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9820\t Train Loss: 0.00554\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9817\t Train Loss: 0.00578\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9816\t Train Loss: 0.00541\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9818\t Train Loss: 0.00511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9825\t Train Loss: 0.00461\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9825\t Train Loss: 0.00412\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9825\t Train Loss: 0.00394\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9825\t Train Loss: 0.00354\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9821\t Train Loss: 0.00392\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9825\t Train Loss: 0.00343\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9826\t Train Loss: 0.00383\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9822\t Train Loss: 0.00338\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9822\t Train Loss: 0.00336\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9824\t Train Loss: 0.00363\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9823\t Train Loss: 0.00328\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9830\t Train Loss: 0.00338\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9825\t Train Loss: 0.00317\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9828\t Train Loss: 0.00332\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9822\t Train Loss: 0.00300\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9826\t Train Loss: 0.00333\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9823\t Train Loss: 0.00347\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9825\t Train Loss: 0.00310\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9824\t Train Loss: 0.00323\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9824\t Train Loss: 0.00316\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9824\t Train Loss: 0.00304\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9826\t Train Loss: 0.00309\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9822\t Train Loss: 0.00335\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9823\t Train Loss: 0.00341\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9820\t Train Loss: 0.00309\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9824\t Train Loss: 0.00315\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9824\t Train Loss: 0.00304\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9818\t Train Loss: 0.00281\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9823\t Train Loss: 0.00303\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9822\t Train Loss: 0.00307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.01\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name],lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.982\n",
      "Fixed set perturbation: 0.446\n",
      "L0 perturbation: 0.440\n",
      "L2 perturbation: 0.321"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ian's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.symbol.Variable('data')\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=data)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "relu1 = mx.symbol.Activation(data=fc1, act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=500)\n",
    "relu2 = mx.symbol.Activation(data=fc2, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc3 = mx.symbol.FullyConnected(data=relu2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc3.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc3.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "sum_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc3.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, sum_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)\n",
    "for name in arg_names:\n",
    "    sum_map[name][:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8632\t Val Accuracy: 0.9369\t Train Loss: 0.56602\n",
      "Train Accuracy: 0.9440\t Val Accuracy: 0.9562\t Train Loss: 0.24647\n",
      "Train Accuracy: 0.9593\t Val Accuracy: 0.9658\t Train Loss: 0.17804\n",
      "Train Accuracy: 0.9665\t Val Accuracy: 0.9718\t Train Loss: 0.14333\n",
      "Train Accuracy: 0.9716\t Val Accuracy: 0.9741\t Train Loss: 0.12186\n",
      "Train Accuracy: 0.9752\t Val Accuracy: 0.9758\t Train Loss: 0.10689\n",
      "Train Accuracy: 0.9777\t Val Accuracy: 0.9772\t Train Loss: 0.09585\n",
      "Train Accuracy: 0.9797\t Val Accuracy: 0.9784\t Train Loss: 0.08720\n",
      "Train Accuracy: 0.9810\t Val Accuracy: 0.9794\t Train Loss: 0.08015\n",
      "Train Accuracy: 0.9825\t Val Accuracy: 0.9808\t Train Loss: 0.07422\n",
      "Train Accuracy: 0.9837\t Val Accuracy: 0.9815\t Train Loss: 0.06926\n",
      "Train Accuracy: 0.9847\t Val Accuracy: 0.9824\t Train Loss: 0.06488\n",
      "Train Accuracy: 0.9857\t Val Accuracy: 0.9827\t Train Loss: 0.06101\n",
      "Train Accuracy: 0.9864\t Val Accuracy: 0.9831\t Train Loss: 0.05753\n",
      "Train Accuracy: 0.9870\t Val Accuracy: 0.9837\t Train Loss: 0.05438\n",
      "Train Accuracy: 0.9877\t Val Accuracy: 0.9840\t Train Loss: 0.05155\n",
      "Train Accuracy: 0.9882\t Val Accuracy: 0.9845\t Train Loss: 0.04893\n",
      "Train Accuracy: 0.9887\t Val Accuracy: 0.9849\t Train Loss: 0.04655\n",
      "Train Accuracy: 0.9891\t Val Accuracy: 0.9851\t Train Loss: 0.04433\n",
      "Train Accuracy: 0.9896\t Val Accuracy: 0.9854\t Train Loss: 0.04228\n",
      "Train Accuracy: 0.9900\t Val Accuracy: 0.9856\t Train Loss: 0.04038\n",
      "Train Accuracy: 0.9904\t Val Accuracy: 0.9858\t Train Loss: 0.03864\n",
      "Train Accuracy: 0.9909\t Val Accuracy: 0.9862\t Train Loss: 0.03698\n",
      "Train Accuracy: 0.9914\t Val Accuracy: 0.9863\t Train Loss: 0.03541\n",
      "Train Accuracy: 0.9918\t Val Accuracy: 0.9869\t Train Loss: 0.03393\n",
      "Train Accuracy: 0.9922\t Val Accuracy: 0.9868\t Train Loss: 0.03255\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.9869\t Train Loss: 0.03127\n",
      "Train Accuracy: 0.9930\t Val Accuracy: 0.9869\t Train Loss: 0.03003\n",
      "Train Accuracy: 0.9934\t Val Accuracy: 0.9874\t Train Loss: 0.02886\n",
      "Train Accuracy: 0.9937\t Val Accuracy: 0.9872\t Train Loss: 0.02773\n",
      "Train Accuracy: 0.9939\t Val Accuracy: 0.9873\t Train Loss: 0.02670\n",
      "Train Accuracy: 0.9941\t Val Accuracy: 0.9875\t Train Loss: 0.02568\n",
      "Train Accuracy: 0.9943\t Val Accuracy: 0.9873\t Train Loss: 0.02472\n",
      "Train Accuracy: 0.9946\t Val Accuracy: 0.9876\t Train Loss: 0.02380\n",
      "Train Accuracy: 0.9948\t Val Accuracy: 0.9877\t Train Loss: 0.02295\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.9880\t Train Loss: 0.02210\n",
      "Train Accuracy: 0.9953\t Val Accuracy: 0.9883\t Train Loss: 0.02130\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.9883\t Train Loss: 0.02052\n",
      "Train Accuracy: 0.9957\t Val Accuracy: 0.9884\t Train Loss: 0.01978\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9886\t Train Loss: 0.01908\n",
      "Train Accuracy: 0.9960\t Val Accuracy: 0.9885\t Train Loss: 0.01839\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.9885\t Train Loss: 0.01773\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9882\t Train Loss: 0.01713\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9884\t Train Loss: 0.01651\n",
      "Train Accuracy: 0.9968\t Val Accuracy: 0.9886\t Train Loss: 0.01595\n",
      "Train Accuracy: 0.9968\t Val Accuracy: 0.9884\t Train Loss: 0.01540\n",
      "Train Accuracy: 0.9969\t Val Accuracy: 0.9887\t Train Loss: 0.01485\n",
      "Train Accuracy: 0.9970\t Val Accuracy: 0.9887\t Train Loss: 0.01435\n",
      "Train Accuracy: 0.9971\t Val Accuracy: 0.9887\t Train Loss: 0.01387\n",
      "Train Accuracy: 0.9972\t Val Accuracy: 0.9887\t Train Loss: 0.01339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.5\n",
    "lr= 0.05\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], sum_map[name], lr)\n",
    "            sum_map[name][:] = 0.\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9976\t Val Accuracy: 0.9900\t Train Loss: 0.01189\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9900\t Train Loss: 0.01148\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9901\t Train Loss: 0.01133\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9902\t Train Loss: 0.01123\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9902\t Train Loss: 0.01115\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9902\t Train Loss: 0.01108\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9902\t Train Loss: 0.01101\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9902\t Train Loss: 0.01095\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9904\t Train Loss: 0.01090\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9904\t Train Loss: 0.01084\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9905\t Train Loss: 0.01078\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9905\t Train Loss: 0.01073\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9906\t Train Loss: 0.01068\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9907\t Train Loss: 0.01063\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01058\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01052\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01047\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01042\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01037\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01032\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01027\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01023\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01018\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9908\t Train Loss: 0.01013\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.01008\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.01003\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.00999\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.00994\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.00989\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9908\t Train Loss: 0.00985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.5\n",
    "lr= 0.005\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        #grad1 = grad_map\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], sum_map[name], lr)\n",
    "            sum_map[name][:] = 0.\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.991\n",
      "Fixed set perturbation: 0.972\n",
      "L0 perturbation: 0.939\n",
      "L2 perturbation: 0.844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.symbol.Variable('data')\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=data)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "relu1 = mx.symbol.Activation(data=fc1, act_type=\"relu\")\n",
    "fc2 = mx.symbol.FullyConnected(data=relu1, num_hidden=500)\n",
    "relu2 = mx.symbol.Activation(data=fc2, act_type=\"relu\")\n",
    "# second fullc\n",
    "fc3 = mx.symbol.FullyConnected(data=relu2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc3.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc3.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc3.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7834\t Val Accuracy: 0.8810\t Train Loss: 0.85596\n",
      "Train Accuracy: 0.9085\t Val Accuracy: 0.9376\t Train Loss: 0.42259\n",
      "Train Accuracy: 0.9372\t Val Accuracy: 0.9543\t Train Loss: 0.30936\n",
      "Train Accuracy: 0.9501\t Val Accuracy: 0.9608\t Train Loss: 0.25102\n",
      "Train Accuracy: 0.9582\t Val Accuracy: 0.9660\t Train Loss: 0.21293\n",
      "Train Accuracy: 0.9639\t Val Accuracy: 0.9692\t Train Loss: 0.18502\n",
      "Train Accuracy: 0.9679\t Val Accuracy: 0.9728\t Train Loss: 0.16432\n",
      "Train Accuracy: 0.9703\t Val Accuracy: 0.9736\t Train Loss: 0.14820\n",
      "Train Accuracy: 0.9729\t Val Accuracy: 0.9754\t Train Loss: 0.13480\n",
      "Train Accuracy: 0.9750\t Val Accuracy: 0.9760\t Train Loss: 0.12353\n",
      "Train Accuracy: 0.9769\t Val Accuracy: 0.9762\t Train Loss: 0.11404\n",
      "Train Accuracy: 0.9783\t Val Accuracy: 0.9771\t Train Loss: 0.10583\n",
      "Train Accuracy: 0.9794\t Val Accuracy: 0.9781\t Train Loss: 0.09876\n",
      "Train Accuracy: 0.9806\t Val Accuracy: 0.9789\t Train Loss: 0.09258\n",
      "Train Accuracy: 0.9815\t Val Accuracy: 0.9793\t Train Loss: 0.08719\n",
      "Train Accuracy: 0.9824\t Val Accuracy: 0.9798\t Train Loss: 0.08229\n",
      "Train Accuracy: 0.9831\t Val Accuracy: 0.9799\t Train Loss: 0.07798\n",
      "Train Accuracy: 0.9837\t Val Accuracy: 0.9805\t Train Loss: 0.07395\n",
      "Train Accuracy: 0.9843\t Val Accuracy: 0.9812\t Train Loss: 0.07035\n",
      "Train Accuracy: 0.9851\t Val Accuracy: 0.9816\t Train Loss: 0.06707\n",
      "Train Accuracy: 0.9856\t Val Accuracy: 0.9818\t Train Loss: 0.06398\n",
      "Train Accuracy: 0.9861\t Val Accuracy: 0.9824\t Train Loss: 0.06116\n",
      "Train Accuracy: 0.9865\t Val Accuracy: 0.9828\t Train Loss: 0.05850\n",
      "Train Accuracy: 0.9869\t Val Accuracy: 0.9830\t Train Loss: 0.05606\n",
      "Train Accuracy: 0.9873\t Val Accuracy: 0.9836\t Train Loss: 0.05369\n",
      "Train Accuracy: 0.9879\t Val Accuracy: 0.9836\t Train Loss: 0.05154\n",
      "Train Accuracy: 0.9883\t Val Accuracy: 0.9840\t Train Loss: 0.04957\n",
      "Train Accuracy: 0.9889\t Val Accuracy: 0.9841\t Train Loss: 0.04763\n",
      "Train Accuracy: 0.9893\t Val Accuracy: 0.9843\t Train Loss: 0.04589\n",
      "Train Accuracy: 0.9895\t Val Accuracy: 0.9844\t Train Loss: 0.04419\n",
      "Train Accuracy: 0.9897\t Val Accuracy: 0.9848\t Train Loss: 0.04261\n",
      "Train Accuracy: 0.9901\t Val Accuracy: 0.9848\t Train Loss: 0.04099\n",
      "Train Accuracy: 0.9905\t Val Accuracy: 0.9848\t Train Loss: 0.03960\n",
      "Train Accuracy: 0.9908\t Val Accuracy: 0.9853\t Train Loss: 0.03818\n",
      "Train Accuracy: 0.9910\t Val Accuracy: 0.9854\t Train Loss: 0.03683\n",
      "Train Accuracy: 0.9913\t Val Accuracy: 0.9857\t Train Loss: 0.03557\n",
      "Train Accuracy: 0.9915\t Val Accuracy: 0.9854\t Train Loss: 0.03437\n",
      "Train Accuracy: 0.9918\t Val Accuracy: 0.9860\t Train Loss: 0.03325\n",
      "Train Accuracy: 0.9921\t Val Accuracy: 0.9858\t Train Loss: 0.03217\n",
      "Train Accuracy: 0.9923\t Val Accuracy: 0.9857\t Train Loss: 0.03110\n",
      "Train Accuracy: 0.9924\t Val Accuracy: 0.9854\t Train Loss: 0.03007\n",
      "Train Accuracy: 0.9926\t Val Accuracy: 0.9856\t Train Loss: 0.02910\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.9854\t Train Loss: 0.02818\n",
      "Train Accuracy: 0.9930\t Val Accuracy: 0.9859\t Train Loss: 0.02734\n",
      "Train Accuracy: 0.9932\t Val Accuracy: 0.9863\t Train Loss: 0.02650\n",
      "Train Accuracy: 0.9934\t Val Accuracy: 0.9863\t Train Loss: 0.02561\n",
      "Train Accuracy: 0.9936\t Val Accuracy: 0.9861\t Train Loss: 0.02480\n",
      "Train Accuracy: 0.9936\t Val Accuracy: 0.9863\t Train Loss: 0.02410\n",
      "Train Accuracy: 0.9939\t Val Accuracy: 0.9865\t Train Loss: 0.02330\n",
      "Train Accuracy: 0.9939\t Val Accuracy: 0.9864\t Train Loss: 0.02262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.7\n",
    "lr = 0.1\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name], lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9951\t Val Accuracy: 0.9884\t Train Loss: 0.01954\n",
      "Train Accuracy: 0.9953\t Val Accuracy: 0.9886\t Train Loss: 0.01884\n",
      "Train Accuracy: 0.9954\t Val Accuracy: 0.9885\t Train Loss: 0.01854\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.9885\t Train Loss: 0.01831\n",
      "Train Accuracy: 0.9956\t Val Accuracy: 0.9884\t Train Loss: 0.01809\n",
      "Train Accuracy: 0.9957\t Val Accuracy: 0.9886\t Train Loss: 0.01788\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9886\t Train Loss: 0.01769\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9885\t Train Loss: 0.01751\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9885\t Train Loss: 0.01734\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9887\t Train Loss: 0.01718\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9888\t Train Loss: 0.01701\n",
      "Train Accuracy: 0.9960\t Val Accuracy: 0.9886\t Train Loss: 0.01685\n",
      "Train Accuracy: 0.9960\t Val Accuracy: 0.9888\t Train Loss: 0.01670\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9890\t Train Loss: 0.01653\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9889\t Train Loss: 0.01637\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9891\t Train Loss: 0.01622\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9891\t Train Loss: 0.01608\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9892\t Train Loss: 0.01593\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.9891\t Train Loss: 0.01579\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9892\t Train Loss: 0.01564\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.9891\t Train Loss: 0.01550\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9892\t Train Loss: 0.01536\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9892\t Train Loss: 0.01523\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9893\t Train Loss: 0.01509\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9893\t Train Loss: 0.01496\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9893\t Train Loss: 0.01483\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9893\t Train Loss: 0.01469\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9894\t Train Loss: 0.01457\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9893\t Train Loss: 0.01444\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9893\t Train Loss: 0.01432\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9893\t Train Loss: 0.01419\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9894\t Train Loss: 0.01406\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9895\t Train Loss: 0.01394\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9894\t Train Loss: 0.01382\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9895\t Train Loss: 0.01371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 35\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.7\n",
    "lr = 0.02\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name],lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.990\n",
      "Fixed set perturbation: 0.974\n",
      "L0 perturbation: 0.944\n",
      "L2 perturbation: 0.867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
