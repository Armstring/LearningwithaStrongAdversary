{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import logging\n",
    "from skimage import io\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dev = mx.gpu()\n",
    "train_iter = mx.io.ImageRecordIter(\n",
    "    shuffle=True,\n",
    "    path_imgrec=\"../data/cifar10/train.rec\",\n",
    "    mean_r = 128,\n",
    "    mean_g = 128,\n",
    "    mean_b = 128,\n",
    "    scale = 0.0078125,\n",
    "    rand_crop=True,\n",
    "    rand_mirror=True,\n",
    "    data_shape=(3, 28, 28),\n",
    "    batch_size=batch_size,\n",
    "    prefetch_buffer=4,\n",
    "    preprocess_threads=2)\n",
    "\n",
    "val_iter = mx.io.ImageRecordIter(\n",
    "    path_imgrec=\"../data/cifar10/test.rec\",\n",
    "    mean_r = 128,\n",
    "    mean_g = 128,\n",
    "    mean_b = 128,\n",
    "    scale = 0.0078125,\n",
    "    rand_crop=False,\n",
    "    rand_mirror=False,\n",
    "    data_shape=(3, 28, 28),\n",
    "    batch_size=batch_size,\n",
    "    prefetch_buffer=4,\n",
    "    preprocess_threads=2,\n",
    "    round_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(arr):\n",
    "    max_val = np.max(arr, axis=1, keepdims=True)\n",
    "    tmp = arr - max_val\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / norm\n",
    "\n",
    "def SoftmaxGrad(arr, idx):\n",
    "    grad = np.copy(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        p = grad[i, idx]\n",
    "        grad[i, :] *= -p\n",
    "        grad[i, idx] = p * (1. - p)\n",
    "    return grad\n",
    "\n",
    "def SGD(weight, grad, lr=0.1, wd=0.0001, grad_norm=batch_size):\n",
    "    grad = mx.nd.clip(grad, -5,5)\n",
    "    weight[:] -= lr * (grad / batch_size + wd*weight)\n",
    "    \n",
    "def LogLossGrad(arr, label):\n",
    "    grad = np.copy(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        grad[i, label[i]] -= 1.\n",
    "    return grad\n",
    "\n",
    "def CalAcc(pred_prob, label):\n",
    "    pred = np.argmax(pred_prob, axis=1)\n",
    "    return np.sum(pred == label) * 1.0\n",
    "\n",
    "def CalLoss(pred_prob, label):\n",
    "    loss = 0.\n",
    "    for i in range(pred_prob.shape[0]):\n",
    "        loss += -np.log(max(pred_prob[i, label[i]], 1e-10))\n",
    "    return loss\n",
    "\n",
    "def ConvFactory(data, kernel, pad, num_filter, stride=1):\n",
    "    conv = mx.sym.Convolution(data=data, kernel=(kernel, kernel), pad=(pad, pad), stride=(stride, stride), num_filter=num_filter)\n",
    "    act = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_normal(model, val_iter, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    return(val_acc / num_samp)\n",
    "    \n",
    "def acc_perb_L0(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = np.sign(grad_map[\"data\"].asnumpy())\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])):\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "            \n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    if  nn>0:\n",
    "        print('L0 gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)\n",
    "\n",
    "def acc_perb_L2(model, val_iter, coe_pb, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_batch = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /  batch_size \n",
    "        num_batch += 1\n",
    "    if  nn>0:\n",
    "        print('L2 gradien being 0 :', nn)\n",
    "    return(val_acc / num_batch)\n",
    "\n",
    "def acc_perb_alpha(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        T = np.zeros((10, batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "        noise = np.zeros(data.shape)\n",
    "        #===================\n",
    "        for i in range(10):\n",
    "            arg_map[\"data\"][:] = data   \n",
    "            model.forward(is_train=True)\n",
    "            theta = model.outputs[0].asnumpy()\n",
    "            alpha = Softmax(theta)\n",
    "            \n",
    "            grad = LogLossGrad(alpha, i*np.ones(alpha.shape[0]))\n",
    "            for j in range(batch_size):\n",
    "                grad[j] = -alpha[j][i]*grad[j]\n",
    "            out_grad[:] = grad\n",
    "            model.backward([out_grad])\n",
    "            T[i] = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])):\n",
    "                perb_scale = np.zeros(10)\n",
    "                for i in range(10):\n",
    "                    if (i == y):\n",
    "                        perb_scale[i] = np.inf\n",
    "                    else:\n",
    "                        perb_scale[i] = (alpha[j][y] - alpha[j][i])/np.linalg.norm((T[i][j]-T[y][j]).flatten(),2)\n",
    "                noise[j] = T[np.argmin(perb_scale)][j]-T[y][j]\n",
    "        #====================\n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            else:\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /batch_size\n",
    "        num_samp += 1\n",
    "    return(val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "conv1 = ConvFactory(data, 3, 1, 64)\n",
    "conv2 = ConvFactory(conv1, 3, 1, 64)\n",
    "conv3 = ConvFactory(conv2, 3, 1, 64)\n",
    "mp1 = mx.sym.Pooling(data=conv3, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "conv4 = ConvFactory(mp1, 3, 1, 128)\n",
    "conv5 = ConvFactory(conv4, 3, 1, 128)\n",
    "conv6 = ConvFactory(conv5, 3, 1, 128)\n",
    "mp1 = mx.sym.Pooling(data=conv6, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "fl = mx.sym.Flatten(data=mp1)\n",
    "fc1 = mx.sym.FullyConnected(data=fl, num_hidden=2048)\n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=act1, num_hidden=2048)\n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "\n",
    "flatten = mx.sym.FullyConnected(data=act2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 3, 28, 28)\n",
    "arg_names = flatten.list_arguments()\n",
    "arg_shapes, output_shapes, aux_shapes = flatten.infer_shape(data=data_shape)\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_sum = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in aux_shapes]\n",
    "pred = mx.nd.zeros(output_shapes[0])\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = flatten.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs, aux_states=aux_states)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, grad_sum))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(6 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    elif \"gamma\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 1.0\n",
    "    else:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.3759\t Val Accuracy: 0.5005\t Train Loss: 1.74886\n",
      "Train Accuracy: 0.5101\t Val Accuracy: 0.5941\t Train Loss: 1.37674\n",
      "Train Accuracy: 0.5821\t Val Accuracy: 0.5932\t Train Loss: 1.18168\n",
      "Train Accuracy: 0.6405\t Val Accuracy: 0.6725\t Train Loss: 1.02275\n",
      "Train Accuracy: 0.6872\t Val Accuracy: 0.7162\t Train Loss: 0.89907\n",
      "Train Accuracy: 0.7202\t Val Accuracy: 0.7031\t Train Loss: 0.80818\n",
      "Train Accuracy: 0.7480\t Val Accuracy: 0.7371\t Train Loss: 0.72595\n",
      "Train Accuracy: 0.7663\t Val Accuracy: 0.7621\t Train Loss: 0.67368\n",
      "Train Accuracy: 0.7843\t Val Accuracy: 0.7171\t Train Loss: 0.62520\n",
      "Train Accuracy: 0.8014\t Val Accuracy: 0.7722\t Train Loss: 0.57382\n",
      "Train Accuracy: 0.8106\t Val Accuracy: 0.7660\t Train Loss: 0.54033\n",
      "Train Accuracy: 0.8262\t Val Accuracy: 0.7682\t Train Loss: 0.50015\n",
      "Train Accuracy: 0.8380\t Val Accuracy: 0.8021\t Train Loss: 0.46571\n",
      "Train Accuracy: 0.8467\t Val Accuracy: 0.7888\t Train Loss: 0.43971\n",
      "Train Accuracy: 0.8581\t Val Accuracy: 0.7737\t Train Loss: 0.40931\n",
      "Train Accuracy: 0.8662\t Val Accuracy: 0.8094\t Train Loss: 0.38430\n",
      "Train Accuracy: 0.8757\t Val Accuracy: 0.8043\t Train Loss: 0.35733\n",
      "Train Accuracy: 0.8841\t Val Accuracy: 0.8174\t Train Loss: 0.33410\n",
      "Train Accuracy: 0.8913\t Val Accuracy: 0.7996\t Train Loss: 0.31311\n",
      "Train Accuracy: 0.8975\t Val Accuracy: 0.8200\t Train Loss: 0.29207\n",
      "Train Accuracy: 0.9036\t Val Accuracy: 0.8213\t Train Loss: 0.27562\n",
      "Train Accuracy: 0.9124\t Val Accuracy: 0.7978\t Train Loss: 0.25314\n",
      "Train Accuracy: 0.9181\t Val Accuracy: 0.8240\t Train Loss: 0.23470\n",
      "Train Accuracy: 0.9237\t Val Accuracy: 0.8180\t Train Loss: 0.22334\n",
      "Train Accuracy: 0.9294\t Val Accuracy: 0.8138\t Train Loss: 0.20680\n",
      "Train Accuracy: 0.9324\t Val Accuracy: 0.8149\t Train Loss: 0.19134\n",
      "Train Accuracy: 0.9376\t Val Accuracy: 0.8091\t Train Loss: 0.18310\n",
      "Train Accuracy: 0.9415\t Val Accuracy: 0.8184\t Train Loss: 0.16888\n",
      "Train Accuracy: 0.9455\t Val Accuracy: 0.8243\t Train Loss: 0.15731\n",
      "Train Accuracy: 0.9484\t Val Accuracy: 0.8228\t Train Loss: 0.14798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.03\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9705\t Val Accuracy: 0.8359\t Train Loss: 0.08914\n",
      "Train Accuracy: 0.9750\t Val Accuracy: 0.8360\t Train Loss: 0.07748\n",
      "Train Accuracy: 0.9781\t Val Accuracy: 0.8394\t Train Loss: 0.06774\n",
      "Train Accuracy: 0.9791\t Val Accuracy: 0.8398\t Train Loss: 0.06494\n",
      "Train Accuracy: 0.9811\t Val Accuracy: 0.8384\t Train Loss: 0.05873\n",
      "Train Accuracy: 0.9821\t Val Accuracy: 0.8402\t Train Loss: 0.05575\n",
      "Train Accuracy: 0.9831\t Val Accuracy: 0.8404\t Train Loss: 0.05350\n",
      "Train Accuracy: 0.9837\t Val Accuracy: 0.8394\t Train Loss: 0.05075\n",
      "Train Accuracy: 0.9841\t Val Accuracy: 0.8395\t Train Loss: 0.04863\n",
      "Train Accuracy: 0.9845\t Val Accuracy: 0.8414\t Train Loss: 0.04891\n",
      "Train Accuracy: 0.9862\t Val Accuracy: 0.8419\t Train Loss: 0.04476\n",
      "Train Accuracy: 0.9864\t Val Accuracy: 0.8400\t Train Loss: 0.04285\n",
      "Train Accuracy: 0.9861\t Val Accuracy: 0.8427\t Train Loss: 0.04301\n",
      "Train Accuracy: 0.9869\t Val Accuracy: 0.8419\t Train Loss: 0.04014\n",
      "Train Accuracy: 0.9886\t Val Accuracy: 0.8404\t Train Loss: 0.03700\n",
      "Train Accuracy: 0.9883\t Val Accuracy: 0.8401\t Train Loss: 0.03731\n",
      "Train Accuracy: 0.9891\t Val Accuracy: 0.8424\t Train Loss: 0.03506\n",
      "Train Accuracy: 0.9887\t Val Accuracy: 0.8423\t Train Loss: 0.03436\n",
      "Train Accuracy: 0.9896\t Val Accuracy: 0.8431\t Train Loss: 0.03354\n",
      "Train Accuracy: 0.9900\t Val Accuracy: 0.8423\t Train Loss: 0.03199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.003\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:26: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy:  0.842266613924\n",
      "Val Batch Accuracy after pertubation:  0.405557753165\n",
      "0.842266613924\n"
     ]
    }
   ],
   "source": [
    "val_iter.reset()\n",
    "val_acc = 0.0\n",
    "val_acc_pb = 0.0\n",
    "coe_pb = 0.5\n",
    "num_samp = 0\n",
    "\n",
    "perb_data = []\n",
    "perb_lab = []\n",
    "\n",
    "for dbatch in val_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    batch_size = label.asnumpy().shape[0]\n",
    "    \n",
    "    model.forward(is_train=False)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    #########\n",
    "    grad = LogLossGrad(alpha, label.asnumpy())\n",
    "    out_grad[:] = grad\n",
    "    model.backward([out_grad])\n",
    "    noise = data_grad.asnumpy()\n",
    "    for j in range(batch_size):\n",
    "        noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "    pdata = data.asnumpy() + coe_pb * noise\n",
    "    arg_map[\"data\"][:] = pdata\n",
    "    model.forward(is_train=True)\n",
    "    raw_output = model.outputs[0].asnumpy()\n",
    "    pred = Softmax(raw_output)\n",
    "    val_acc_pb += CalAcc(pred, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "    \n",
    "    perb_data.append(pdata)\n",
    "    perb_lab.append(label.asnumpy())\n",
    "print(\"Val Batch Accuracy: \", val_acc / num_samp)\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc_pb / num_samp)\n",
    "print(acc_normal(model, val_iter,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdata = np.concatenate(perb_data, axis = 0)\n",
    "plabel = np.concatenate(perb_lab, axis = 0)\n",
    "perb_iter = mx.io.NDArrayIter(\n",
    "    data = pdata,\n",
    "    label = plabel,\n",
    "    batch_size = 128,\n",
    "    shuffle = False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy after pertubation:  0.405557753165\n"
     ]
    }
   ],
   "source": [
    "perb_iter.reset()\n",
    "num_samp = 0\n",
    "val_acc = 0.0\n",
    "for dbatch in perb_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "conv1 = ConvFactory(data, 3, 1, 64)\n",
    "conv2 = ConvFactory(conv1, 3, 1, 64)\n",
    "conv3 = ConvFactory(conv2, 3, 1, 64)\n",
    "mp1 = mx.sym.Pooling(data=conv3, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "conv4 = ConvFactory(mp1, 3, 1, 128)\n",
    "conv5 = ConvFactory(conv4, 3, 1, 128)\n",
    "conv6 = ConvFactory(conv5, 3, 1, 128)\n",
    "mp1 = mx.sym.Pooling(data=conv6, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "fl = mx.sym.Flatten(data=mp1)\n",
    "fc1 = mx.sym.FullyConnected(data=fl, num_hidden=2048)\n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=act1, num_hidden=2048)\n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "\n",
    "flatten = mx.sym.FullyConnected(data=act2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 3, 28, 28)\n",
    "arg_names = flatten.list_arguments()\n",
    "arg_shapes, output_shapes, aux_shapes = flatten.infer_shape(data=data_shape)\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_sum = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in aux_shapes]\n",
    "pred = mx.nd.zeros(output_shapes[0])\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = flatten.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs, aux_states=aux_states)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, grad_sum))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(6 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    elif \"gamma\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 1.0\n",
    "    else:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.3731\t Val Accuracy: 0.4895\t Train Loss: 1.75075\n",
      "Train Accuracy: 0.5160\t Val Accuracy: 0.5870\t Train Loss: 1.36211\n",
      "Train Accuracy: 0.5890\t Val Accuracy: 0.5893\t Train Loss: 1.16560\n",
      "Train Accuracy: 0.6423\t Val Accuracy: 0.6799\t Train Loss: 1.01737\n",
      "Train Accuracy: 0.6873\t Val Accuracy: 0.6473\t Train Loss: 0.89851\n",
      "Train Accuracy: 0.7167\t Val Accuracy: 0.7130\t Train Loss: 0.81132\n",
      "Train Accuracy: 0.7432\t Val Accuracy: 0.7487\t Train Loss: 0.74035\n",
      "Train Accuracy: 0.7616\t Val Accuracy: 0.7664\t Train Loss: 0.68549\n",
      "Train Accuracy: 0.7795\t Val Accuracy: 0.7693\t Train Loss: 0.63441\n",
      "Train Accuracy: 0.7937\t Val Accuracy: 0.7812\t Train Loss: 0.58743\n",
      "Train Accuracy: 0.8094\t Val Accuracy: 0.7865\t Train Loss: 0.54928\n",
      "Train Accuracy: 0.8212\t Val Accuracy: 0.7949\t Train Loss: 0.51527\n",
      "Train Accuracy: 0.8330\t Val Accuracy: 0.7402\t Train Loss: 0.47993\n",
      "Train Accuracy: 0.8406\t Val Accuracy: 0.8044\t Train Loss: 0.45415\n",
      "Train Accuracy: 0.8548\t Val Accuracy: 0.8021\t Train Loss: 0.41732\n",
      "Train Accuracy: 0.8612\t Val Accuracy: 0.8166\t Train Loss: 0.39683\n",
      "Train Accuracy: 0.8707\t Val Accuracy: 0.8176\t Train Loss: 0.36977\n",
      "Train Accuracy: 0.8801\t Val Accuracy: 0.8113\t Train Loss: 0.34288\n",
      "Train Accuracy: 0.8877\t Val Accuracy: 0.8161\t Train Loss: 0.32103\n",
      "Train Accuracy: 0.8938\t Val Accuracy: 0.8117\t Train Loss: 0.30457\n",
      "Train Accuracy: 0.9003\t Val Accuracy: 0.8121\t Train Loss: 0.28058\n",
      "Train Accuracy: 0.9082\t Val Accuracy: 0.8172\t Train Loss: 0.26522\n",
      "Train Accuracy: 0.9130\t Val Accuracy: 0.8178\t Train Loss: 0.25131\n",
      "Train Accuracy: 0.9214\t Val Accuracy: 0.8216\t Train Loss: 0.22563\n",
      "Train Accuracy: 0.9263\t Val Accuracy: 0.8098\t Train Loss: 0.21216\n",
      "Train Accuracy: 0.9289\t Val Accuracy: 0.7988\t Train Loss: 0.20426\n",
      "Train Accuracy: 0.9353\t Val Accuracy: 0.8267\t Train Loss: 0.18522\n",
      "Train Accuracy: 0.9388\t Val Accuracy: 0.8110\t Train Loss: 0.17740\n",
      "Train Accuracy: 0.9415\t Val Accuracy: 0.8303\t Train Loss: 0.16764\n",
      "Train Accuracy: 0.9468\t Val Accuracy: 0.8169\t Train Loss: 0.15221\n",
      "Train Accuracy: 0.9483\t Val Accuracy: 0.8216\t Train Loss: 0.14953\n",
      "Train Accuracy: 0.9530\t Val Accuracy: 0.8226\t Train Loss: 0.13537\n",
      "Train Accuracy: 0.9542\t Val Accuracy: 0.8275\t Train Loss: 0.13367\n",
      "Train Accuracy: 0.9594\t Val Accuracy: 0.8241\t Train Loss: 0.11720\n",
      "Train Accuracy: 0.9595\t Val Accuracy: 0.8236\t Train Loss: 0.11799\n",
      "Train Accuracy: 0.9623\t Val Accuracy: 0.8289\t Train Loss: 0.10838\n",
      "Train Accuracy: 0.9665\t Val Accuracy: 0.8266\t Train Loss: 0.09734\n",
      "Train Accuracy: 0.9650\t Val Accuracy: 0.8341\t Train Loss: 0.10112\n",
      "Train Accuracy: 0.9675\t Val Accuracy: 0.8348\t Train Loss: 0.09338\n",
      "Train Accuracy: 0.9688\t Val Accuracy: 0.8155\t Train Loss: 0.09067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 40\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.03\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9823\t Val Accuracy: 0.8415\t Train Loss: 0.05530\n",
      "Train Accuracy: 0.9873\t Val Accuracy: 0.8454\t Train Loss: 0.04009\n",
      "Train Accuracy: 0.9893\t Val Accuracy: 0.8450\t Train Loss: 0.03425\n",
      "Train Accuracy: 0.9891\t Val Accuracy: 0.8467\t Train Loss: 0.03493\n",
      "Train Accuracy: 0.9908\t Val Accuracy: 0.8470\t Train Loss: 0.03046\n",
      "Train Accuracy: 0.9902\t Val Accuracy: 0.8469\t Train Loss: 0.03044\n",
      "Train Accuracy: 0.9914\t Val Accuracy: 0.8472\t Train Loss: 0.02756\n",
      "Train Accuracy: 0.9927\t Val Accuracy: 0.8487\t Train Loss: 0.02404\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.8478\t Train Loss: 0.02403\n",
      "Train Accuracy: 0.9929\t Val Accuracy: 0.8479\t Train Loss: 0.02338\n",
      "Train Accuracy: 0.9934\t Val Accuracy: 0.8487\t Train Loss: 0.02173\n",
      "Train Accuracy: 0.9933\t Val Accuracy: 0.8478\t Train Loss: 0.02121\n",
      "Train Accuracy: 0.9933\t Val Accuracy: 0.8497\t Train Loss: 0.02200\n",
      "Train Accuracy: 0.9938\t Val Accuracy: 0.8485\t Train Loss: 0.02014\n",
      "Train Accuracy: 0.9943\t Val Accuracy: 0.8505\t Train Loss: 0.01860\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.8476\t Train Loss: 0.01715\n",
      "Train Accuracy: 0.9940\t Val Accuracy: 0.8491\t Train Loss: 0.01897\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.8499\t Train Loss: 0.01736\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.8499\t Train Loss: 0.01550\n",
      "Train Accuracy: 0.9952\t Val Accuracy: 0.8492\t Train Loss: 0.01610\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.8473\t Train Loss: 0.01488\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.8494\t Train Loss: 0.01525\n",
      "Train Accuracy: 0.9953\t Val Accuracy: 0.8491\t Train Loss: 0.01444\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.8497\t Train Loss: 0.01366\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.8500\t Train Loss: 0.01282\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.8494\t Train Loss: 0.01372\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.8492\t Train Loss: 0.01318\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.8489\t Train Loss: 0.01267\n",
      "Train Accuracy: 0.9966\t Val Accuracy: 0.8498\t Train Loss: 0.01185\n",
      "Train Accuracy: 0.9967\t Val Accuracy: 0.8514\t Train Loss: 0.01142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.003\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.851\n",
      "Fixed set perturbation: 0.747\n",
      "L0 perturbation: 0.517"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:84: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2 gradien being 0 : 16\n",
      "L2 perturbation: 0.429\n",
      "Alpha perturbation: 0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "conv1 = ConvFactory(data, 3, 1, 64)\n",
    "conv2 = ConvFactory(conv1, 3, 1, 64)\n",
    "conv3 = ConvFactory(conv2, 3, 1, 64)\n",
    "mp1 = mx.sym.Pooling(data=conv3, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "conv4 = ConvFactory(mp1, 3, 1, 128)\n",
    "conv5 = ConvFactory(conv4, 3, 1, 128)\n",
    "conv6 = ConvFactory(conv5, 3, 1, 128)\n",
    "mp1 = mx.sym.Pooling(data=conv6, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "fl = mx.sym.Flatten(data=mp1)\n",
    "fc1 = mx.sym.FullyConnected(data=fl, num_hidden=2048)\n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "dp1 = mx.sym.Dropout(data=act1, p=0.5)\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=dp1, num_hidden=2048)\n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "dp2 = mx.sym.Dropout(data=act2, p=0.5)\n",
    "\n",
    "flatten = mx.sym.FullyConnected(data=dp2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 3, 28, 28)\n",
    "arg_names = flatten.list_arguments()\n",
    "arg_shapes, output_shapes, aux_shapes = flatten.infer_shape(data=data_shape)\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_sum = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in aux_shapes]\n",
    "pred = mx.nd.zeros(output_shapes[0])\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = flatten.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs, aux_states=aux_states)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, grad_sum))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(6 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    elif \"gamma\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 1.0\n",
    "    else:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.3072\t Val Accuracy: 0.4179\t Train Loss: 1.90753\n",
      "Train Accuracy: 0.4391\t Val Accuracy: 0.5167\t Train Loss: 1.54575\n",
      "Train Accuracy: 0.5007\t Val Accuracy: 0.5916\t Train Loss: 1.38349\n",
      "Train Accuracy: 0.5532\t Val Accuracy: 0.5941\t Train Loss: 1.24992\n",
      "Train Accuracy: 0.5914\t Val Accuracy: 0.6552\t Train Loss: 1.14376\n",
      "Train Accuracy: 0.6264\t Val Accuracy: 0.6889\t Train Loss: 1.04894\n",
      "Train Accuracy: 0.6602\t Val Accuracy: 0.7114\t Train Loss: 0.96482\n",
      "Train Accuracy: 0.6805\t Val Accuracy: 0.7366\t Train Loss: 0.90941\n",
      "Train Accuracy: 0.6986\t Val Accuracy: 0.7392\t Train Loss: 0.85371\n",
      "Train Accuracy: 0.7158\t Val Accuracy: 0.7316\t Train Loss: 0.80954\n",
      "Train Accuracy: 0.7288\t Val Accuracy: 0.7541\t Train Loss: 0.77440\n",
      "Train Accuracy: 0.7422\t Val Accuracy: 0.7641\t Train Loss: 0.74084\n",
      "Train Accuracy: 0.7542\t Val Accuracy: 0.7800\t Train Loss: 0.71008\n",
      "Train Accuracy: 0.7586\t Val Accuracy: 0.7704\t Train Loss: 0.68524\n",
      "Train Accuracy: 0.7686\t Val Accuracy: 0.7754\t Train Loss: 0.66487\n",
      "Train Accuracy: 0.7782\t Val Accuracy: 0.7916\t Train Loss: 0.63870\n",
      "Train Accuracy: 0.7825\t Val Accuracy: 0.8047\t Train Loss: 0.62123\n",
      "Train Accuracy: 0.7894\t Val Accuracy: 0.8036\t Train Loss: 0.60165\n",
      "Train Accuracy: 0.7991\t Val Accuracy: 0.8006\t Train Loss: 0.57879\n",
      "Train Accuracy: 0.8029\t Val Accuracy: 0.7953\t Train Loss: 0.56299\n",
      "Train Accuracy: 0.8096\t Val Accuracy: 0.8190\t Train Loss: 0.55264\n",
      "Train Accuracy: 0.8133\t Val Accuracy: 0.8167\t Train Loss: 0.53641\n",
      "Train Accuracy: 0.8187\t Val Accuracy: 0.8172\t Train Loss: 0.52002\n",
      "Train Accuracy: 0.8222\t Val Accuracy: 0.8146\t Train Loss: 0.50931\n",
      "Train Accuracy: 0.8251\t Val Accuracy: 0.8231\t Train Loss: 0.50002\n",
      "Train Accuracy: 0.8320\t Val Accuracy: 0.8315\t Train Loss: 0.48497\n",
      "Train Accuracy: 0.8357\t Val Accuracy: 0.8353\t Train Loss: 0.46908\n",
      "Train Accuracy: 0.8408\t Val Accuracy: 0.8332\t Train Loss: 0.45727\n",
      "Train Accuracy: 0.8423\t Val Accuracy: 0.8369\t Train Loss: 0.44913\n",
      "Train Accuracy: 0.8466\t Val Accuracy: 0.8444\t Train Loss: 0.44215\n",
      "Train Accuracy: 0.8517\t Val Accuracy: 0.8429\t Train Loss: 0.42734\n",
      "Train Accuracy: 0.8556\t Val Accuracy: 0.8408\t Train Loss: 0.41824\n",
      "Train Accuracy: 0.8567\t Val Accuracy: 0.8471\t Train Loss: 0.41102\n",
      "Train Accuracy: 0.8602\t Val Accuracy: 0.8445\t Train Loss: 0.40003\n",
      "Train Accuracy: 0.8623\t Val Accuracy: 0.8390\t Train Loss: 0.38952\n",
      "Train Accuracy: 0.8656\t Val Accuracy: 0.8434\t Train Loss: 0.38399\n",
      "Train Accuracy: 0.8709\t Val Accuracy: 0.8470\t Train Loss: 0.36991\n",
      "Train Accuracy: 0.8707\t Val Accuracy: 0.8465\t Train Loss: 0.36474\n",
      "Train Accuracy: 0.8744\t Val Accuracy: 0.8504\t Train Loss: 0.36059\n",
      "Train Accuracy: 0.8795\t Val Accuracy: 0.8546\t Train Loss: 0.34718\n",
      "Train Accuracy: 0.8798\t Val Accuracy: 0.8530\t Train Loss: 0.34287\n",
      "Train Accuracy: 0.8844\t Val Accuracy: 0.8558\t Train Loss: 0.33388\n",
      "Train Accuracy: 0.8862\t Val Accuracy: 0.8485\t Train Loss: 0.32561\n",
      "Train Accuracy: 0.8871\t Val Accuracy: 0.8501\t Train Loss: 0.32133\n",
      "Train Accuracy: 0.8882\t Val Accuracy: 0.8473\t Train Loss: 0.31868\n",
      "Train Accuracy: 0.8921\t Val Accuracy: 0.8528\t Train Loss: 0.30634\n",
      "Train Accuracy: 0.8936\t Val Accuracy: 0.8585\t Train Loss: 0.30233\n",
      "Train Accuracy: 0.8973\t Val Accuracy: 0.8573\t Train Loss: 0.29385\n",
      "Train Accuracy: 0.8985\t Val Accuracy: 0.8563\t Train Loss: 0.29033\n",
      "Train Accuracy: 0.9002\t Val Accuracy: 0.8586\t Train Loss: 0.28436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.03\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9180\t Val Accuracy: 0.8722\t Train Loss: 0.23360\n",
      "Train Accuracy: 0.9219\t Val Accuracy: 0.8734\t Train Loss: 0.22317\n",
      "Train Accuracy: 0.9233\t Val Accuracy: 0.8745\t Train Loss: 0.21908\n",
      "Train Accuracy: 0.9244\t Val Accuracy: 0.8750\t Train Loss: 0.21594\n",
      "Train Accuracy: 0.9273\t Val Accuracy: 0.8734\t Train Loss: 0.20904\n",
      "Train Accuracy: 0.9264\t Val Accuracy: 0.8730\t Train Loss: 0.20780\n",
      "Train Accuracy: 0.9257\t Val Accuracy: 0.8746\t Train Loss: 0.21051\n",
      "Train Accuracy: 0.9293\t Val Accuracy: 0.8741\t Train Loss: 0.20336\n",
      "Train Accuracy: 0.9286\t Val Accuracy: 0.8751\t Train Loss: 0.20496\n",
      "Train Accuracy: 0.9290\t Val Accuracy: 0.8769\t Train Loss: 0.20252\n",
      "Train Accuracy: 0.9304\t Val Accuracy: 0.8755\t Train Loss: 0.19973\n",
      "Train Accuracy: 0.9291\t Val Accuracy: 0.8752\t Train Loss: 0.20103\n",
      "Train Accuracy: 0.9310\t Val Accuracy: 0.8758\t Train Loss: 0.19991\n",
      "Train Accuracy: 0.9305\t Val Accuracy: 0.8747\t Train Loss: 0.19617\n",
      "Train Accuracy: 0.9316\t Val Accuracy: 0.8767\t Train Loss: 0.19446\n",
      "Train Accuracy: 0.9321\t Val Accuracy: 0.8777\t Train Loss: 0.19328\n",
      "Train Accuracy: 0.9316\t Val Accuracy: 0.8776\t Train Loss: 0.19400\n",
      "Train Accuracy: 0.9328\t Val Accuracy: 0.8748\t Train Loss: 0.19007\n",
      "Train Accuracy: 0.9333\t Val Accuracy: 0.8761\t Train Loss: 0.19294\n",
      "Train Accuracy: 0.9333\t Val Accuracy: 0.8754\t Train Loss: 0.18810\n",
      "Train Accuracy: 0.9349\t Val Accuracy: 0.8774\t Train Loss: 0.18808\n",
      "Train Accuracy: 0.9343\t Val Accuracy: 0.8772\t Train Loss: 0.18743\n",
      "Train Accuracy: 0.9356\t Val Accuracy: 0.8783\t Train Loss: 0.18389\n",
      "Train Accuracy: 0.9350\t Val Accuracy: 0.8774\t Train Loss: 0.18613\n",
      "Train Accuracy: 0.9345\t Val Accuracy: 0.8783\t Train Loss: 0.18398\n",
      "Train Accuracy: 0.9350\t Val Accuracy: 0.8767\t Train Loss: 0.18713\n",
      "Train Accuracy: 0.9366\t Val Accuracy: 0.8770\t Train Loss: 0.17818\n",
      "Train Accuracy: 0.9381\t Val Accuracy: 0.8769\t Train Loss: 0.17856\n",
      "Train Accuracy: 0.9355\t Val Accuracy: 0.8768\t Train Loss: 0.18090\n",
      "Train Accuracy: 0.9378\t Val Accuracy: 0.8772\t Train Loss: 0.17914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.003\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.877\n",
      "Fixed set perturbation: 0.777\n",
      "L0 perturbation: 0.593\n",
      "L2 perturbation: 0.495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ian's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "conv1 = ConvFactory(data, 3, 1, 128)\n",
    "conv2 = ConvFactory(conv1, 3, 1, 128)\n",
    "conv3 = ConvFactory(conv2, 3, 1, 128)\n",
    "mp1 = mx.sym.Pooling(data=conv3, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "conv4 = ConvFactory(mp1, 3, 1, 256)\n",
    "conv5 = ConvFactory(conv4, 3, 1, 256)\n",
    "conv6 = ConvFactory(conv5, 3, 1, 256)\n",
    "mp1 = mx.sym.Pooling(data=conv6, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "fl = mx.sym.Flatten(data=mp1)\n",
    "fc1 = mx.sym.FullyConnected(data=fl, num_hidden=2048)\n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "dp1 = mx.sym.Dropout(data=act1, p=0.5)\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=dp1, num_hidden=2048)\n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "dp2 = mx.sym.Dropout(data=act2, p=0.5)\n",
    "\n",
    "flatten = mx.sym.FullyConnected(data=dp2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 3, 28, 28)\n",
    "arg_names = flatten.list_arguments()\n",
    "arg_shapes, output_shapes, aux_shapes = flatten.infer_shape(data=data_shape)\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_sum = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in aux_shapes]\n",
    "pred = mx.nd.zeros(output_shapes[0])\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = flatten.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs, aux_states=aux_states)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, grad_sum))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(6 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    elif \"gamma\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 1.0\n",
    "    else:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.2969\t Val Accuracy: 0.4384\t Train Loss: 1.91756\n",
      "Train Accuracy: 0.4114\t Val Accuracy: 0.4983\t Train Loss: 1.61419\n",
      "Train Accuracy: 0.4634\t Val Accuracy: 0.5268\t Train Loss: 1.48178\n",
      "Train Accuracy: 0.5031\t Val Accuracy: 0.5618\t Train Loss: 1.38250\n",
      "Train Accuracy: 0.5342\t Val Accuracy: 0.5938\t Train Loss: 1.30510\n",
      "Train Accuracy: 0.5593\t Val Accuracy: 0.5995\t Train Loss: 1.23167\n",
      "Train Accuracy: 0.5832\t Val Accuracy: 0.6178\t Train Loss: 1.16884\n",
      "Train Accuracy: 0.6058\t Val Accuracy: 0.6402\t Train Loss: 1.11001\n",
      "Train Accuracy: 0.6256\t Val Accuracy: 0.6727\t Train Loss: 1.05707\n",
      "Train Accuracy: 0.6452\t Val Accuracy: 0.6890\t Train Loss: 1.00764\n",
      "Train Accuracy: 0.6598\t Val Accuracy: 0.6973\t Train Loss: 0.96993\n",
      "Train Accuracy: 0.6750\t Val Accuracy: 0.7116\t Train Loss: 0.93040\n",
      "Train Accuracy: 0.6876\t Val Accuracy: 0.7144\t Train Loss: 0.89760\n",
      "Train Accuracy: 0.6996\t Val Accuracy: 0.7222\t Train Loss: 0.86359\n",
      "Train Accuracy: 0.7076\t Val Accuracy: 0.7446\t Train Loss: 0.83492\n",
      "Train Accuracy: 0.7160\t Val Accuracy: 0.7506\t Train Loss: 0.81220\n",
      "Train Accuracy: 0.7249\t Val Accuracy: 0.7571\t Train Loss: 0.78780\n",
      "Train Accuracy: 0.7335\t Val Accuracy: 0.7606\t Train Loss: 0.76903\n",
      "Train Accuracy: 0.7397\t Val Accuracy: 0.7530\t Train Loss: 0.74706\n",
      "Train Accuracy: 0.7458\t Val Accuracy: 0.7723\t Train Loss: 0.73099\n",
      "Train Accuracy: 0.7531\t Val Accuracy: 0.7792\t Train Loss: 0.71183\n",
      "Train Accuracy: 0.7605\t Val Accuracy: 0.7703\t Train Loss: 0.69149\n",
      "Train Accuracy: 0.7671\t Val Accuracy: 0.7727\t Train Loss: 0.67559\n",
      "Train Accuracy: 0.7704\t Val Accuracy: 0.7875\t Train Loss: 0.66424\n",
      "Train Accuracy: 0.7788\t Val Accuracy: 0.7926\t Train Loss: 0.64720\n",
      "Train Accuracy: 0.7831\t Val Accuracy: 0.7923\t Train Loss: 0.63203\n",
      "Train Accuracy: 0.7842\t Val Accuracy: 0.7873\t Train Loss: 0.62633\n",
      "Train Accuracy: 0.7898\t Val Accuracy: 0.7908\t Train Loss: 0.60780\n",
      "Train Accuracy: 0.7953\t Val Accuracy: 0.7955\t Train Loss: 0.59621\n",
      "Train Accuracy: 0.7986\t Val Accuracy: 0.8066\t Train Loss: 0.58701\n",
      "Train Accuracy: 0.8035\t Val Accuracy: 0.8025\t Train Loss: 0.57145\n",
      "Train Accuracy: 0.8067\t Val Accuracy: 0.8074\t Train Loss: 0.56306\n",
      "Train Accuracy: 0.8092\t Val Accuracy: 0.8142\t Train Loss: 0.55432\n",
      "Train Accuracy: 0.8145\t Val Accuracy: 0.8032\t Train Loss: 0.54034\n",
      "Train Accuracy: 0.8194\t Val Accuracy: 0.8159\t Train Loss: 0.52610\n",
      "Train Accuracy: 0.8224\t Val Accuracy: 0.8118\t Train Loss: 0.51599\n",
      "Train Accuracy: 0.8247\t Val Accuracy: 0.8182\t Train Loss: 0.51228\n",
      "Train Accuracy: 0.8276\t Val Accuracy: 0.8080\t Train Loss: 0.50009\n",
      "Train Accuracy: 0.8309\t Val Accuracy: 0.8268\t Train Loss: 0.49392\n",
      "Train Accuracy: 0.8349\t Val Accuracy: 0.8162\t Train Loss: 0.48098\n",
      "Train Accuracy: 0.8397\t Val Accuracy: 0.8174\t Train Loss: 0.47193\n",
      "Train Accuracy: 0.8413\t Val Accuracy: 0.8259\t Train Loss: 0.46316\n",
      "Train Accuracy: 0.8432\t Val Accuracy: 0.8322\t Train Loss: 0.45741\n",
      "Train Accuracy: 0.8468\t Val Accuracy: 0.8235\t Train Loss: 0.44212\n",
      "Train Accuracy: 0.8501\t Val Accuracy: 0.8324\t Train Loss: 0.43890\n",
      "Train Accuracy: 0.8530\t Val Accuracy: 0.8379\t Train Loss: 0.42969\n",
      "Train Accuracy: 0.8548\t Val Accuracy: 0.8228\t Train Loss: 0.42780\n",
      "Train Accuracy: 0.8570\t Val Accuracy: 0.8309\t Train Loss: 0.42000\n",
      "Train Accuracy: 0.8581\t Val Accuracy: 0.8297\t Train Loss: 0.41100\n",
      "Train Accuracy: 0.8607\t Val Accuracy: 0.8382\t Train Loss: 0.40433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.4\n",
    "lr= 0.015\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.838\n",
      "Fixed set perturbation: 0.799\n",
      "L0 perturbation: 0.776\n",
      "L2 perturbation: 0.731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8746\t Val Accuracy: 0.8461\t Train Loss: 0.36430\n",
      "Train Accuracy: 0.8808\t Val Accuracy: 0.8478\t Train Loss: 0.35136\n",
      "Train Accuracy: 0.8818\t Val Accuracy: 0.8459\t Train Loss: 0.34501\n",
      "Train Accuracy: 0.8835\t Val Accuracy: 0.8504\t Train Loss: 0.34393\n",
      "Train Accuracy: 0.8856\t Val Accuracy: 0.8485\t Train Loss: 0.33943\n",
      "Train Accuracy: 0.8844\t Val Accuracy: 0.8515\t Train Loss: 0.33794\n",
      "Train Accuracy: 0.8846\t Val Accuracy: 0.8517\t Train Loss: 0.33622\n",
      "Train Accuracy: 0.8877\t Val Accuracy: 0.8515\t Train Loss: 0.33323\n",
      "Train Accuracy: 0.8887\t Val Accuracy: 0.8505\t Train Loss: 0.32724\n",
      "Train Accuracy: 0.8883\t Val Accuracy: 0.8516\t Train Loss: 0.32712\n",
      "Train Accuracy: 0.8893\t Val Accuracy: 0.8515\t Train Loss: 0.32750\n",
      "Train Accuracy: 0.8872\t Val Accuracy: 0.8508\t Train Loss: 0.32553\n",
      "Train Accuracy: 0.8895\t Val Accuracy: 0.8510\t Train Loss: 0.32351\n",
      "Train Accuracy: 0.8910\t Val Accuracy: 0.8497\t Train Loss: 0.32014\n",
      "Train Accuracy: 0.8921\t Val Accuracy: 0.8509\t Train Loss: 0.31807\n",
      "Train Accuracy: 0.8944\t Val Accuracy: 0.8502\t Train Loss: 0.31041\n",
      "Train Accuracy: 0.8929\t Val Accuracy: 0.8535\t Train Loss: 0.31499\n",
      "Train Accuracy: 0.8916\t Val Accuracy: 0.8508\t Train Loss: 0.31195\n",
      "Train Accuracy: 0.8954\t Val Accuracy: 0.8521\t Train Loss: 0.30998\n",
      "Train Accuracy: 0.8946\t Val Accuracy: 0.8539\t Train Loss: 0.30843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.4\n",
    "lr= 0.0025\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        #grad1 = grad_map\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.854\n",
      "Fixed set perturbation: 0.817\n",
      "L0 perturbation: 0.793\n",
      "L2 perturbation: 0.744"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8973\t Val Accuracy: 0.8549\t Train Loss: 0.30192\n",
      "Train Accuracy: 0.8968\t Val Accuracy: 0.8542\t Train Loss: 0.30178\n",
      "Train Accuracy: 0.8970\t Val Accuracy: 0.8531\t Train Loss: 0.30111\n",
      "Train Accuracy: 0.8984\t Val Accuracy: 0.8550\t Train Loss: 0.29923\n",
      "Train Accuracy: 0.8980\t Val Accuracy: 0.8544\t Train Loss: 0.30101\n",
      "Train Accuracy: 0.8981\t Val Accuracy: 0.8537\t Train Loss: 0.29952\n",
      "Train Accuracy: 0.8977\t Val Accuracy: 0.8536\t Train Loss: 0.29809\n",
      "Train Accuracy: 0.8999\t Val Accuracy: 0.8537\t Train Loss: 0.29744\n",
      "Train Accuracy: 0.8996\t Val Accuracy: 0.8539\t Train Loss: 0.29531\n",
      "Train Accuracy: 0.8996\t Val Accuracy: 0.8546\t Train Loss: 0.29555\n",
      "Train Accuracy: 0.9005\t Val Accuracy: 0.8561\t Train Loss: 0.29381\n",
      "Train Accuracy: 0.8999\t Val Accuracy: 0.8555\t Train Loss: 0.29446\n",
      "Train Accuracy: 0.9022\t Val Accuracy: 0.8555\t Train Loss: 0.28851\n",
      "Train Accuracy: 0.9015\t Val Accuracy: 0.8560\t Train Loss: 0.28932\n",
      "Train Accuracy: 0.9008\t Val Accuracy: 0.8557\t Train Loss: 0.29056\n",
      "Train Accuracy: 0.9012\t Val Accuracy: 0.8548\t Train Loss: 0.29108\n",
      "Train Accuracy: 0.9015\t Val Accuracy: 0.8556\t Train Loss: 0.28795\n",
      "Train Accuracy: 0.9018\t Val Accuracy: 0.8549\t Train Loss: 0.28739\n",
      "Train Accuracy: 0.9007\t Val Accuracy: 0.8541\t Train Loss: 0.28661\n",
      "Train Accuracy: 0.9024\t Val Accuracy: 0.8562\t Train Loss: 0.28695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.4\n",
    "lr= 0.001\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        #grad1 = grad_map\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.856\n",
      "Fixed set perturbation: 0.818\n",
      "L0 perturbation: 0.794\n",
      "L2 perturbation: 0.745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "conv1 = ConvFactory(data, 3, 1, 128)\n",
    "conv2 = ConvFactory(conv1, 3, 1, 128)\n",
    "conv3 = ConvFactory(conv2, 3, 1, 128)\n",
    "mp1 = mx.sym.Pooling(data=conv3, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "conv4 = ConvFactory(mp1, 3, 1, 256)\n",
    "conv5 = ConvFactory(conv4, 3, 1, 256)\n",
    "conv6 = ConvFactory(conv5, 3, 1, 256)\n",
    "mp1 = mx.sym.Pooling(data=conv6, pool_type=\"max\", kernel=(3,3), stride=(2,2))\n",
    "fl = mx.sym.Flatten(data=mp1)\n",
    "fc1 = mx.sym.FullyConnected(data=fl, num_hidden=2048)\n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n",
    "dp1 = mx.sym.Dropout(data=act1, p=0.5)\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=dp1, num_hidden=2048)\n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n",
    "dp2 = mx.sym.Dropout(data=act2, p=0.5)\n",
    "\n",
    "flatten = mx.sym.FullyConnected(data=dp2, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 3, 28, 28)\n",
    "arg_names = flatten.list_arguments()\n",
    "arg_shapes, output_shapes, aux_shapes = flatten.infer_shape(data=data_shape)\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_sum = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in aux_shapes]\n",
    "pred = mx.nd.zeros(output_shapes[0])\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = flatten.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs, aux_states=aux_states)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, grad_sum))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(6 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    elif \"gamma\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 1.0\n",
    "    else:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.3130\t Val Accuracy: 0.4404\t Train Loss: 1.88494\n",
      "Train Accuracy: 0.4303\t Val Accuracy: 0.5049\t Train Loss: 1.56248\n",
      "Train Accuracy: 0.4868\t Val Accuracy: 0.5517\t Train Loss: 1.42107\n",
      "Train Accuracy: 0.5322\t Val Accuracy: 0.5834\t Train Loss: 1.30721\n",
      "Train Accuracy: 0.5680\t Val Accuracy: 0.6208\t Train Loss: 1.20917\n",
      "Train Accuracy: 0.5999\t Val Accuracy: 0.6240\t Train Loss: 1.13124\n",
      "Train Accuracy: 0.6234\t Val Accuracy: 0.6675\t Train Loss: 1.06487\n",
      "Train Accuracy: 0.6484\t Val Accuracy: 0.6961\t Train Loss: 1.00828\n",
      "Train Accuracy: 0.6635\t Val Accuracy: 0.7184\t Train Loss: 0.95761\n",
      "Train Accuracy: 0.6799\t Val Accuracy: 0.7108\t Train Loss: 0.91757\n",
      "Train Accuracy: 0.6965\t Val Accuracy: 0.7263\t Train Loss: 0.87111\n",
      "Train Accuracy: 0.7119\t Val Accuracy: 0.7387\t Train Loss: 0.83575\n",
      "Train Accuracy: 0.7198\t Val Accuracy: 0.7536\t Train Loss: 0.80711\n",
      "Train Accuracy: 0.7324\t Val Accuracy: 0.7497\t Train Loss: 0.77855\n",
      "Train Accuracy: 0.7388\t Val Accuracy: 0.7583\t Train Loss: 0.75184\n",
      "Train Accuracy: 0.7477\t Val Accuracy: 0.7682\t Train Loss: 0.72553\n",
      "Train Accuracy: 0.7553\t Val Accuracy: 0.7755\t Train Loss: 0.70329\n",
      "Train Accuracy: 0.7661\t Val Accuracy: 0.7811\t Train Loss: 0.67822\n",
      "Train Accuracy: 0.7711\t Val Accuracy: 0.7680\t Train Loss: 0.66377\n",
      "Train Accuracy: 0.7777\t Val Accuracy: 0.7860\t Train Loss: 0.64447\n",
      "Train Accuracy: 0.7837\t Val Accuracy: 0.7943\t Train Loss: 0.62911\n",
      "Train Accuracy: 0.7886\t Val Accuracy: 0.7896\t Train Loss: 0.61013\n",
      "Train Accuracy: 0.7921\t Val Accuracy: 0.7929\t Train Loss: 0.59893\n",
      "Train Accuracy: 0.8021\t Val Accuracy: 0.7995\t Train Loss: 0.57978\n",
      "Train Accuracy: 0.8061\t Val Accuracy: 0.8110\t Train Loss: 0.56226\n",
      "Train Accuracy: 0.8116\t Val Accuracy: 0.8109\t Train Loss: 0.54880\n",
      "Train Accuracy: 0.8172\t Val Accuracy: 0.8080\t Train Loss: 0.53647\n",
      "Train Accuracy: 0.8203\t Val Accuracy: 0.8074\t Train Loss: 0.52365\n",
      "Train Accuracy: 0.8241\t Val Accuracy: 0.8151\t Train Loss: 0.51051\n",
      "Train Accuracy: 0.8304\t Val Accuracy: 0.8215\t Train Loss: 0.49669\n",
      "Train Accuracy: 0.8347\t Val Accuracy: 0.8201\t Train Loss: 0.48160\n",
      "Train Accuracy: 0.8378\t Val Accuracy: 0.8208\t Train Loss: 0.47743\n",
      "Train Accuracy: 0.8429\t Val Accuracy: 0.8249\t Train Loss: 0.46214\n",
      "Train Accuracy: 0.8451\t Val Accuracy: 0.8234\t Train Loss: 0.45290\n",
      "Train Accuracy: 0.8494\t Val Accuracy: 0.8286\t Train Loss: 0.43757\n",
      "Train Accuracy: 0.8520\t Val Accuracy: 0.8355\t Train Loss: 0.43141\n",
      "Train Accuracy: 0.8564\t Val Accuracy: 0.8352\t Train Loss: 0.41995\n",
      "Train Accuracy: 0.8581\t Val Accuracy: 0.8252\t Train Loss: 0.41435\n",
      "Train Accuracy: 0.8638\t Val Accuracy: 0.8298\t Train Loss: 0.39884\n",
      "Train Accuracy: 0.8652\t Val Accuracy: 0.8395\t Train Loss: 0.39310\n",
      "Train Accuracy: 0.8692\t Val Accuracy: 0.8372\t Train Loss: 0.38239\n",
      "Train Accuracy: 0.8730\t Val Accuracy: 0.8343\t Train Loss: 0.37397\n",
      "Train Accuracy: 0.8748\t Val Accuracy: 0.8427\t Train Loss: 0.36681\n",
      "Train Accuracy: 0.8793\t Val Accuracy: 0.8382\t Train Loss: 0.35369\n",
      "Train Accuracy: 0.8822\t Val Accuracy: 0.8432\t Train Loss: 0.34571\n",
      "Train Accuracy: 0.8848\t Val Accuracy: 0.8424\t Train Loss: 0.33628\n",
      "Train Accuracy: 0.8863\t Val Accuracy: 0.8457\t Train Loss: 0.33079\n",
      "Train Accuracy: 0.8913\t Val Accuracy: 0.8433\t Train Loss: 0.32005\n",
      "Train Accuracy: 0.8936\t Val Accuracy: 0.8466\t Train Loss: 0.31437\n",
      "Train Accuracy: 0.8971\t Val Accuracy: 0.8490\t Train Loss: 0.30434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 50\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.3\n",
    "lr = 0.025\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.849\n",
      "Fixed set perturbation: 0.817\n",
      "L0 perturbation: 0.786\n",
      "L2 perturbation: 0.746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9218\t Val Accuracy: 0.8611\t Train Loss: 0.23065\n",
      "Train Accuracy: 0.9233\t Val Accuracy: 0.8623\t Train Loss: 0.22957\n",
      "Train Accuracy: 0.9240\t Val Accuracy: 0.8614\t Train Loss: 0.22575\n",
      "Train Accuracy: 0.9260\t Val Accuracy: 0.8612\t Train Loss: 0.22299\n",
      "Train Accuracy: 0.9233\t Val Accuracy: 0.8616\t Train Loss: 0.22349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 5\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.3\n",
    "lr = 0.003\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.862\n",
      "Fixed set perturbation: 0.828\n",
      "L0 perturbation: 0.798\n",
      "L2 perturbation: 0.760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9239\t Val Accuracy: 0.8619\t Train Loss: 0.22008\n",
      "Train Accuracy: 0.9273\t Val Accuracy: 0.8633\t Train Loss: 0.21952\n",
      "Train Accuracy: 0.9261\t Val Accuracy: 0.8628\t Train Loss: 0.21955\n",
      "Train Accuracy: 0.9265\t Val Accuracy: 0.8631\t Train Loss: 0.21647\n",
      "Train Accuracy: 0.9277\t Val Accuracy: 0.8639\t Train Loss: 0.21512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 5\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.3\n",
    "lr = 0.001\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.864\n",
      "Fixed set perturbation: 0.830\n",
      "L0 perturbation: 0.802\n",
      "L2 perturbation: 0.762"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9307\t Val Accuracy: 0.8642\t Train Loss: 0.20556\n",
      "Train Accuracy: 0.9308\t Val Accuracy: 0.8643\t Train Loss: 0.20587\n",
      "Train Accuracy: 0.9299\t Val Accuracy: 0.8643\t Train Loss: 0.20876\n",
      "Train Accuracy: 0.9296\t Val Accuracy: 0.8639\t Train Loss: 0.20961\n",
      "Train Accuracy: 0.9287\t Val Accuracy: 0.8641\t Train Loss: 0.21030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 5\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 0.3\n",
    "lr = 0.0001\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                if name.endswith(\"weight\"):\n",
    "                    SGD(arg_map[name], grad_map[name], lr)\n",
    "                else:\n",
    "                    SGD(arg_map[name], grad_map[name], lr, 0)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.864\n",
      "Fixed set perturbation: 0.831\n",
      "L0 perturbation: 0.803\n",
      "L2 perturbation: 0.760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:135: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha perturbation: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:136: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.3f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.3f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.3f' % acc_perb_L0(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.3f' % acc_perb_L2(model, val_iter, 0.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.3f' % acc_perb_alpha(model, val_iter, 0.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
