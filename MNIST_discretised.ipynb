{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = mx.gpu()\n",
    "batch_size = 100\n",
    "data_shape = (1,28,28)\n",
    "batch_shape = (100,1,28,28)\n",
    "train_iter = mx.io.MNISTIter(\n",
    "        image       = \"../mxnet/mnist/train-images-idx3-ubyte\",\n",
    "        label       = \"../mxnet/mnist/train-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        flat        = False,\n",
    "        ctx = dev)\n",
    "\n",
    "val_iter = mx.io.MNISTIter(\n",
    "        image       = \"../mxnet/mnist/t10k-images-idx3-ubyte\",\n",
    "        label       = \"../mxnet/mnist/t10k-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        flat        = False,\n",
    "        ctx = dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_relu(data, name, **kwargs):\n",
    "    net = mx.sym.Convolution(data, name = \"%s_conv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, act_type='relu', name = \"%s_act\"%name)\n",
    "    return net\n",
    "def conv2d_act(data, name, act, **kwargs):\n",
    "    net = mx.sym.Convolution(data, name = \"%s_conv\"%name, **kwargs)\n",
    "    net = mx.sym.Activation(net, act_type=act, name = \"%s_act\"%name)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(data = None, num_hidden=256, ngf=20):\n",
    "    data = mx.sym.Variable('data') if data is None else data\n",
    "    net = conv2d_relu(data, kernel = (4,4), stride = (2,2), pad = (1,1), num_filter=ngf, name = 'EC_conv1')\n",
    "    net = conv2d_relu(net, kernel = (4,4), stride = (2,2), pad = (2,2), num_filter=50, name = 'EC_conv2')\n",
    "    #net = conv2d_relu(net, kernel = (4,4), stride = (2,2), pad = (1,1), num_filter=ngf*4, name = 'EC_conv3')\n",
    "    net = mx.sym.Flatten(net)\n",
    "    net = mx.sym.Activation(net, name = 'EC_act1', act_type = 'relu')    \n",
    "    net = mx.sym.FullyConnected(data, num_hidden = num_hidden, name = \"C_fc1\")\n",
    "    net = mx.sym.Activation(net, name = 'EC_act1', act_type = 'relu')    \n",
    "    net = mx.sym.FullyConnected(data, num_hidden = 64, name = \"C_fc1\")\n",
    "    net = mx.sym.Activation(net, name = 'C_act1', act_type = 'relu')\n",
    "    net = mx.sym.FullyConnected(net, num_hidden = 10, name = 'C_fc_2')\n",
    "    #net = mx.sym.SoftmaxOutput(net, name = 'C_softmax')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CalAcc(prob, label):\n",
    "    return np.sum(np.argmax(prob, axis=1)==label)*1.0/batch_size\n",
    "def EDLoss(decoder, data):\n",
    "    res = 0.0\n",
    "    \n",
    "    temp = decoder - data\n",
    "    for j in range(batch_size):\n",
    "        res += mx.nd.norm(temp[j])\n",
    "    return res/batch_size\n",
    "\n",
    "def CLoss(prob, label):\n",
    "    res = 0.0\n",
    "    for j in range(batch_size):\n",
    "        res -= np.log(prob[j][int(label[j])])\n",
    "    return res/batch_size\n",
    "\n",
    "def SGD(weight, grad, lr = 0.05, wd = 0.0001):\n",
    "    weight[:] -= lr*(grad/batch_size + wd*weight) \n",
    "    \n",
    "def softmax(theta):\n",
    "    tmp = theta - np.max(theta, axis=1, keepdims = True)\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims = True)\n",
    "    return exp/norm\n",
    "    \n",
    "def logLossGrad(alpha,label):\n",
    "    res = np.copy(alpha)\n",
    "    for j in range(alpha.shape[0]):\n",
    "        res[j][int(label[j])] -= 1\n",
    "    return res\n",
    "\n",
    "def Validate_loss(val_iter):\n",
    "    val_loss = 0.0\n",
    "    val_iter.reset()\n",
    "    nbatch = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        E_arg_map['data'][:] = data\n",
    "        modE.forward(is_train=False)\n",
    "        coder = modE.outputs[0]\n",
    "        \n",
    "        D_arg_map['data'][:] = coder\n",
    "        modD.forward(is_train = False)\n",
    "        decoder = modD.outputs[0]\n",
    "        \n",
    "        data_gpu = mx.nd.zeros(shape = data.shape, ctx = dev)\n",
    "        data.copyto(data_gpu)\n",
    "        val_loss += EDLoss(decoder, data_gpu)\n",
    "        nbatch +=1\n",
    "    return val_loss/nbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_normal(model, val_iter, arg_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        #print data.shape\n",
    "        #print arg_map['data'].shape\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += 1\n",
    "    return(val_acc / num_samp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4254\t Training Accuracy: 0.8816\n",
      "Training Loss: 0.2664\t Training Accuracy: 0.9250\n",
      "0.9753\n",
      "Training Loss: 0.0782\t Training Accuracy: 0.9762\n",
      "Training Loss: 0.0680\t Training Accuracy: 0.9798\n",
      "0.9846\n",
      "Training Loss: 0.0509\t Training Accuracy: 0.9853\n",
      "Training Loss: 0.0460\t Training Accuracy: 0.9868\n",
      "0.9868\n",
      "Training Loss: 0.0378\t Training Accuracy: 0.9895\n",
      "Training Loss: 0.0348\t Training Accuracy: 0.9904\n",
      "0.9881\n",
      "Training Loss: 0.0295\t Training Accuracy: 0.9919\n",
      "Training Loss: 0.0274\t Training Accuracy: 0.9927\n",
      "0.9898\n",
      "Training Loss: 0.0235\t Training Accuracy: 0.9938\n",
      "Training Loss: 0.0221\t Training Accuracy: 0.9944\n",
      "0.9901\n",
      "Training Loss: 0.0189\t Training Accuracy: 0.9951\n",
      "Training Loss: 0.0179\t Training Accuracy: 0.9956\n",
      "0.9907\n",
      "Training Loss: 0.0154\t Training Accuracy: 0.9963\n",
      "Training Loss: 0.0148\t Training Accuracy: 0.9967\n",
      "0.9911\n",
      "Training Loss: 0.0128\t Training Accuracy: 0.9972\n",
      "Training Loss: 0.0123\t Training Accuracy: 0.9975\n",
      "0.991\n",
      "Training Loss: 0.0107\t Training Accuracy: 0.9979\n",
      "Training Loss: 0.0104\t Training Accuracy: 0.9981\n",
      "0.9912\n",
      "Training Loss: 0.0090\t Training Accuracy: 0.9984\n",
      "Training Loss: 0.0088\t Training Accuracy: 0.9986\n",
      "0.9912\n",
      "Training Loss: 0.0076\t Training Accuracy: 0.9988\n",
      "Training Loss: 0.0076\t Training Accuracy: 0.9989\n",
      "0.9913\n",
      "Training Loss: 0.0066\t Training Accuracy: 0.9991\n",
      "Training Loss: 0.0066\t Training Accuracy: 0.9991\n",
      "0.991\n",
      "Training Loss: 0.0057\t Training Accuracy: 0.9994\n",
      "Training Loss: 0.0058\t Training Accuracy: 0.9994\n",
      "0.9911\n",
      "Training Loss: 0.0051\t Training Accuracy: 0.9996\n",
      "Training Loss: 0.0052\t Training Accuracy: 0.9995\n",
      "0.9912\n",
      "Training Loss: 0.0045\t Training Accuracy: 0.9996\n",
      "Training Loss: 0.0047\t Training Accuracy: 0.9996\n",
      "0.9912\n",
      "Training Loss: 0.0041\t Training Accuracy: 0.9997\n",
      "Training Loss: 0.0042\t Training Accuracy: 0.9997\n",
      "0.9912\n",
      "Training Loss: 0.0038\t Training Accuracy: 0.9998\n",
      "Training Loss: 0.0039\t Training Accuracy: 0.9997\n",
      "0.9912\n",
      "Training Loss: 0.0035\t Training Accuracy: 0.9998\n",
      "Training Loss: 0.0036\t Training Accuracy: 0.9998\n",
      "0.9913\n",
      "Training Loss: 0.0032\t Training Accuracy: 0.9999\n",
      "Training Loss: 0.0034\t Training Accuracy: 0.9999\n",
      "0.9913\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "lr = 0.2\n",
    "\n",
    "Training_normal = np.zeros(shape = (num_round))\n",
    "Validation_normal = np.zeros(shape = (num_round))\n",
    "\n",
    "for i in range(num_round):\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        arg_map['data'][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        \n",
    "        loss_total +=  CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        \n",
    "        out_grad[:] = logGrad\n",
    "        model.backward([out_grad])\n",
    "                \n",
    "        for name in arg_names:\n",
    "            if name!='data':\n",
    "                SGD(arg_map[name], grad_map[name], lr)      \n",
    "\n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    Training_normal[i] = train_acc/num_batch\n",
    "    print acc_normal(model, val_iter, arg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Val Batch Accuracy: ', 0.99129999999999951)\n",
      "('Val Batch Accuracy after pertubation: ', 0.25420000000000009)\n"
     ]
    }
   ],
   "source": [
    "val_iter.reset()\n",
    "val_acc = 0.0\n",
    "val_acc_pb = 0.0\n",
    "coe_pb = 2\n",
    "num_batch = 0\n",
    "\n",
    "perb_data = []\n",
    "perb_lab = []\n",
    "\n",
    "for dbatch in val_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = softmax(theta)\n",
    "    \n",
    "    val_acc += CalAcc(alpha, label.asnumpy())\n",
    "    logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        \n",
    "    out_grad[:] = logGrad\n",
    "    model.backward([out_grad])\n",
    "    noise = data_grad.asnumpy()\n",
    "    for j in range(batch_size):\n",
    "        #noise[j] = np.sign(noise[j])\n",
    "        if np.linalg.norm(noise[j].flatten(),2) != 0:\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "    pdata = data.asnumpy() + coe_pb * noise\n",
    "    arg_map[\"data\"][:] = pdata\n",
    "    model.forward(is_train=True)\n",
    "    raw_output = model.outputs[0].asnumpy()\n",
    "    pred = softmax(raw_output)\n",
    "    val_acc_pb += CalAcc(pred, label.asnumpy()) \n",
    "    num_batch += 1\n",
    "    pdata = np.minimum(np.maximum(pdata,0),1)\n",
    "    perb_data.append(pdata)\n",
    "    perb_lab.append(label.asnumpy())\n",
    "    #print np.max(np.absolute(noise[3][0]))\n",
    "print(\"Val Batch Accuracy: \", val_acc / num_batch)\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc_pb / num_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdata = np.concatenate(perb_data, axis = 0)\n",
    "plabel = np.concatenate(perb_lab, axis = 0)\n",
    "perb_iter = mx.io.NDArrayIter(\n",
    "    data = pdata,\n",
    "    label = plabel,\n",
    "    batch_size = 100,\n",
    "    shuffle = False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Val Batch Accuracy after pertubation: ', 0.64260000000000006)\n"
     ]
    }
   ],
   "source": [
    "perb_iter.reset()\n",
    "num_samp = 0\n",
    "val_acc = 0.0\n",
    "for dbatch in perb_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    num_samp += 1\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_net = Classifier()\n",
    "C_arg_names = C_net.list_arguments()\n",
    "C_arg_shapes, C_output_shapes, C_aux_shapes = C_net.infer_shape(data = batch_shape)\n",
    "C_arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in C_arg_shapes]\n",
    "C_grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in C_arg_shapes]\n",
    "C_aux_states =  [mx.nd.zeros(shape, ctx=dev) for shape in C_aux_shapes]\n",
    "C_reqs = [\"write\" for name in C_arg_names]\n",
    "\n",
    "modC = C_net.bind(ctx=dev, args=C_arg_arrays, args_grad = C_grad_arrays, grad_req=C_reqs,  aux_states=C_aux_states)\n",
    "C_arg_map = dict(zip(C_arg_names, C_arg_arrays))\n",
    "C_grad_map = dict(zip(C_arg_names, C_grad_arrays))\n",
    "C_data_grad = C_grad_map[\"data\"]\n",
    "C_out_grad = mx.nd.zeros(modC.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mx.rnd.seed(17214)\n",
    "for name in C_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = C_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = C_arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 90\n",
    "lr = 0.1\n",
    "\n",
    "Training_normal = np.zeros(shape = (num_epoch))\n",
    "Validation_normal = np.zeros(shape = (num_epoch))\n",
    "Adv_normal = np.zeros(shape = (num_epoch))\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    if i%30==29: \n",
    "        lr = lr/2.0\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        C_arg_map['data'][:] = data\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        \n",
    "        loss_total +=  CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        \n",
    "        C_out_grad[:] = logGrad\n",
    "        modC.backward([C_out_grad])\n",
    "                \n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "                \n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    Training_normal[i] = train_acc/num_batch\n",
    "    Validation_normal[i] = acc_normal(modC, val_iter, C_arg_map)\n",
    "    Adv_normal[i] = acc_normal(modC, perb_iter, C_arg_map)\n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validation_normal[i], Adv_normal[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discretize(data, p):\n",
    "    tmp = np.zeros(shape = data.shape)\n",
    "    tmp[:] = np.floor(data*p)\n",
    "    tmp = tmp/p\n",
    "    return tmp\n",
    "\n",
    "def acc_dis(model, val_iter, arg_map,p):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        \n",
    "        tmp = discretize(data.asnumpy(),p)\n",
    "        arg_map[\"data\"][:] = tmp    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += 1\n",
    "    return(val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx.rnd.seed(214)\n",
    "for name in C_arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = C_arg_map[name]\n",
    "        shape = arr.shape\n",
    "        fan_in, fan_out = np.prod(shape[1:]), shape[0]\n",
    "        factor = fan_in\n",
    "        scale = np.sqrt(2.34 / factor)\n",
    "        arr[:] = mx.rnd.uniform(-scale, scale, arr.shape)\n",
    "    else:\n",
    "        arr = C_arg_map[name]\n",
    "        arr[:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7884\t Training Accuracy: 0.8111\n",
      "Training Loss: 0.5817\t Training Accuracy: 0.8525\n",
      "epoch: 0 Validation Accuracy: 0.9089\t Adverserial Accuracy: 0.8504\n",
      "Training Loss: 0.3116\t Training Accuracy: 0.9123\n",
      "Training Loss: 0.3028\t Training Accuracy: 0.9143\n",
      "epoch: 1 Validation Accuracy: 0.9262\t Adverserial Accuracy: 0.8637\n",
      "Training Loss: 0.2583\t Training Accuracy: 0.9275\n",
      "Training Loss: 0.2536\t Training Accuracy: 0.9286\n",
      "epoch: 2 Validation Accuracy: 0.9340\t Adverserial Accuracy: 0.8721\n",
      "Training Loss: 0.2229\t Training Accuracy: 0.9367\n",
      "Training Loss: 0.2196\t Training Accuracy: 0.9381\n",
      "epoch: 3 Validation Accuracy: 0.9401\t Adverserial Accuracy: 0.8788\n",
      "Training Loss: 0.1970\t Training Accuracy: 0.9442\n",
      "Training Loss: 0.1942\t Training Accuracy: 0.9452\n",
      "epoch: 4 Validation Accuracy: 0.9469\t Adverserial Accuracy: 0.8830\n",
      "Training Loss: 0.1762\t Training Accuracy: 0.9503\n",
      "Training Loss: 0.1738\t Training Accuracy: 0.9511\n",
      "epoch: 5 Validation Accuracy: 0.9513\t Adverserial Accuracy: 0.8871\n",
      "Training Loss: 0.1593\t Training Accuracy: 0.9552\n",
      "Training Loss: 0.1572\t Training Accuracy: 0.9559\n",
      "epoch: 6 Validation Accuracy: 0.9553\t Adverserial Accuracy: 0.8910\n",
      "Training Loss: 0.1454\t Training Accuracy: 0.9589\n",
      "Training Loss: 0.1434\t Training Accuracy: 0.9600\n",
      "epoch: 7 Validation Accuracy: 0.9580\t Adverserial Accuracy: 0.8949\n",
      "Training Loss: 0.1337\t Training Accuracy: 0.9623\n",
      "Training Loss: 0.1318\t Training Accuracy: 0.9636\n",
      "epoch: 8 Validation Accuracy: 0.9599\t Adverserial Accuracy: 0.8978\n",
      "Training Loss: 0.1237\t Training Accuracy: 0.9659\n",
      "Training Loss: 0.1219\t Training Accuracy: 0.9666\n",
      "epoch: 9 Validation Accuracy: 0.9623\t Adverserial Accuracy: 0.9011\n",
      "Training Loss: 0.1151\t Training Accuracy: 0.9684\n",
      "Training Loss: 0.1133\t Training Accuracy: 0.9689\n",
      "epoch: 10 Validation Accuracy: 0.9639\t Adverserial Accuracy: 0.9038\n",
      "Training Loss: 0.1076\t Training Accuracy: 0.9708\n",
      "Training Loss: 0.1059\t Training Accuracy: 0.9713\n",
      "epoch: 11 Validation Accuracy: 0.9645\t Adverserial Accuracy: 0.9053\n",
      "Training Loss: 0.1010\t Training Accuracy: 0.9724\n",
      "Training Loss: 0.0993\t Training Accuracy: 0.9729\n",
      "epoch: 12 Validation Accuracy: 0.9661\t Adverserial Accuracy: 0.9073\n",
      "Training Loss: 0.0951\t Training Accuracy: 0.9744\n",
      "Training Loss: 0.0935\t Training Accuracy: 0.9746\n",
      "epoch: 13 Validation Accuracy: 0.9676\t Adverserial Accuracy: 0.9082\n",
      "Training Loss: 0.0899\t Training Accuracy: 0.9762\n",
      "Training Loss: 0.0884\t Training Accuracy: 0.9762\n",
      "epoch: 14 Validation Accuracy: 0.9685\t Adverserial Accuracy: 0.9096\n",
      "Training Loss: 0.0852\t Training Accuracy: 0.9773\n",
      "Training Loss: 0.0838\t Training Accuracy: 0.9775\n",
      "epoch: 15 Validation Accuracy: 0.9690\t Adverserial Accuracy: 0.9103\n",
      "Training Loss: 0.0810\t Training Accuracy: 0.9784\n",
      "Training Loss: 0.0796\t Training Accuracy: 0.9786\n",
      "epoch: 16 Validation Accuracy: 0.9705\t Adverserial Accuracy: 0.9113\n",
      "Training Loss: 0.0772\t Training Accuracy: 0.9795\n",
      "Training Loss: 0.0758\t Training Accuracy: 0.9797\n",
      "epoch: 17 Validation Accuracy: 0.9709\t Adverserial Accuracy: 0.9124\n",
      "Training Loss: 0.0737\t Training Accuracy: 0.9803\n",
      "Training Loss: 0.0723\t Training Accuracy: 0.9806\n",
      "epoch: 18 Validation Accuracy: 0.9714\t Adverserial Accuracy: 0.9125\n",
      "Training Loss: 0.0705\t Training Accuracy: 0.9812\n",
      "Training Loss: 0.0692\t Training Accuracy: 0.9815\n",
      "epoch: 19 Validation Accuracy: 0.9716\t Adverserial Accuracy: 0.9126\n",
      "Training Loss: 0.0676\t Training Accuracy: 0.9823\n",
      "Training Loss: 0.0663\t Training Accuracy: 0.9826\n",
      "epoch: 20 Validation Accuracy: 0.9720\t Adverserial Accuracy: 0.9129\n",
      "Training Loss: 0.0648\t Training Accuracy: 0.9830\n",
      "Training Loss: 0.0636\t Training Accuracy: 0.9834\n",
      "epoch: 21 Validation Accuracy: 0.9723\t Adverserial Accuracy: 0.9141\n",
      "Training Loss: 0.0623\t Training Accuracy: 0.9837\n",
      "Training Loss: 0.0611\t Training Accuracy: 0.9841\n",
      "epoch: 22 Validation Accuracy: 0.9729\t Adverserial Accuracy: 0.9149\n",
      "Training Loss: 0.0600\t Training Accuracy: 0.9844\n",
      "Training Loss: 0.0587\t Training Accuracy: 0.9848\n",
      "epoch: 23 Validation Accuracy: 0.9733\t Adverserial Accuracy: 0.9148\n",
      "Training Loss: 0.0578\t Training Accuracy: 0.9853\n",
      "Training Loss: 0.0566\t Training Accuracy: 0.9856\n",
      "epoch: 24 Validation Accuracy: 0.9738\t Adverserial Accuracy: 0.9147\n",
      "Training Loss: 0.0557\t Training Accuracy: 0.9858\n",
      "Training Loss: 0.0545\t Training Accuracy: 0.9862\n",
      "epoch: 25 Validation Accuracy: 0.9740\t Adverserial Accuracy: 0.9149\n",
      "Training Loss: 0.0538\t Training Accuracy: 0.9866\n",
      "Training Loss: 0.0526\t Training Accuracy: 0.9868\n",
      "epoch: 26 Validation Accuracy: 0.9741\t Adverserial Accuracy: 0.9149\n",
      "Training Loss: 0.0520\t Training Accuracy: 0.9873\n",
      "Training Loss: 0.0509\t Training Accuracy: 0.9874\n",
      "epoch: 27 Validation Accuracy: 0.9746\t Adverserial Accuracy: 0.9149\n",
      "Training Loss: 0.0502\t Training Accuracy: 0.9876\n",
      "Training Loss: 0.0492\t Training Accuracy: 0.9880\n",
      "epoch: 28 Validation Accuracy: 0.9747\t Adverserial Accuracy: 0.9149\n",
      "Training Loss: 0.0478\t Training Accuracy: 0.9881\n",
      "Training Loss: 0.0463\t Training Accuracy: 0.9888\n",
      "epoch: 29 Validation Accuracy: 0.9743\t Adverserial Accuracy: 0.9163\n",
      "Training Loss: 0.0466\t Training Accuracy: 0.9885\n",
      "Training Loss: 0.0454\t Training Accuracy: 0.9891\n",
      "epoch: 30 Validation Accuracy: 0.9747\t Adverserial Accuracy: 0.9159\n",
      "Training Loss: 0.0458\t Training Accuracy: 0.9888\n",
      "Training Loss: 0.0446\t Training Accuracy: 0.9893\n",
      "epoch: 31 Validation Accuracy: 0.9749\t Adverserial Accuracy: 0.9163\n",
      "Training Loss: 0.0450\t Training Accuracy: 0.9891\n",
      "Training Loss: 0.0439\t Training Accuracy: 0.9896\n",
      "epoch: 32 Validation Accuracy: 0.9750\t Adverserial Accuracy: 0.9163\n",
      "Training Loss: 0.0442\t Training Accuracy: 0.9894\n",
      "Training Loss: 0.0432\t Training Accuracy: 0.9898\n",
      "epoch: 33 Validation Accuracy: 0.9751\t Adverserial Accuracy: 0.9162\n",
      "Training Loss: 0.0435\t Training Accuracy: 0.9896\n",
      "Training Loss: 0.0425\t Training Accuracy: 0.9900\n",
      "epoch: 34 Validation Accuracy: 0.9752\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0428\t Training Accuracy: 0.9899\n",
      "Training Loss: 0.0419\t Training Accuracy: 0.9903\n",
      "epoch: 35 Validation Accuracy: 0.9749\t Adverserial Accuracy: 0.9164\n",
      "Training Loss: 0.0422\t Training Accuracy: 0.9903\n",
      "Training Loss: 0.0412\t Training Accuracy: 0.9906\n",
      "epoch: 36 Validation Accuracy: 0.9749\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0415\t Training Accuracy: 0.9905\n",
      "Training Loss: 0.0406\t Training Accuracy: 0.9907\n",
      "epoch: 37 Validation Accuracy: 0.9750\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0409\t Training Accuracy: 0.9906\n",
      "Training Loss: 0.0400\t Training Accuracy: 0.9910\n",
      "epoch: 38 Validation Accuracy: 0.9752\t Adverserial Accuracy: 0.9174\n",
      "Training Loss: 0.0403\t Training Accuracy: 0.9908\n",
      "Training Loss: 0.0394\t Training Accuracy: 0.9912\n",
      "epoch: 39 Validation Accuracy: 0.9754\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0397\t Training Accuracy: 0.9911\n",
      "Training Loss: 0.0388\t Training Accuracy: 0.9914\n",
      "epoch: 40 Validation Accuracy: 0.9754\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0392\t Training Accuracy: 0.9913\n",
      "Training Loss: 0.0383\t Training Accuracy: 0.9915\n",
      "epoch: 41 Validation Accuracy: 0.9755\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0386\t Training Accuracy: 0.9915\n",
      "Training Loss: 0.0377\t Training Accuracy: 0.9917\n",
      "epoch: 42 Validation Accuracy: 0.9756\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0381\t Training Accuracy: 0.9918\n",
      "Training Loss: 0.0372\t Training Accuracy: 0.9919\n",
      "epoch: 43 Validation Accuracy: 0.9756\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0376\t Training Accuracy: 0.9921\n",
      "Training Loss: 0.0367\t Training Accuracy: 0.9922\n",
      "epoch: 44 Validation Accuracy: 0.9756\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0371\t Training Accuracy: 0.9923\n",
      "Training Loss: 0.0362\t Training Accuracy: 0.9924\n",
      "epoch: 45 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0366\t Training Accuracy: 0.9924\n",
      "Training Loss: 0.0357\t Training Accuracy: 0.9926\n",
      "epoch: 46 Validation Accuracy: 0.9756\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0361\t Training Accuracy: 0.9925\n",
      "Training Loss: 0.0353\t Training Accuracy: 0.9928\n",
      "epoch: 47 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0356\t Training Accuracy: 0.9927\n",
      "Training Loss: 0.0348\t Training Accuracy: 0.9929\n",
      "epoch: 48 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0351\t Training Accuracy: 0.9929\n",
      "Training Loss: 0.0343\t Training Accuracy: 0.9931\n",
      "epoch: 49 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0347\t Training Accuracy: 0.9930\n",
      "Training Loss: 0.0339\t Training Accuracy: 0.9933\n",
      "epoch: 50 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0343\t Training Accuracy: 0.9932\n",
      "Training Loss: 0.0335\t Training Accuracy: 0.9934\n",
      "epoch: 51 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0338\t Training Accuracy: 0.9933\n",
      "Training Loss: 0.0331\t Training Accuracy: 0.9935\n",
      "epoch: 52 Validation Accuracy: 0.9759\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0334\t Training Accuracy: 0.9934\n",
      "Training Loss: 0.0326\t Training Accuracy: 0.9936\n",
      "epoch: 53 Validation Accuracy: 0.9759\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0330\t Training Accuracy: 0.9935\n",
      "Training Loss: 0.0322\t Training Accuracy: 0.9937\n",
      "epoch: 54 Validation Accuracy: 0.9759\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0326\t Training Accuracy: 0.9937\n",
      "Training Loss: 0.0319\t Training Accuracy: 0.9938\n",
      "epoch: 55 Validation Accuracy: 0.9760\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0322\t Training Accuracy: 0.9938\n",
      "Training Loss: 0.0315\t Training Accuracy: 0.9940\n",
      "epoch: 56 Validation Accuracy: 0.9759\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0318\t Training Accuracy: 0.9940\n",
      "Training Loss: 0.0311\t Training Accuracy: 0.9941\n",
      "epoch: 57 Validation Accuracy: 0.9758\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0315\t Training Accuracy: 0.9942\n",
      "Training Loss: 0.0307\t Training Accuracy: 0.9943\n",
      "epoch: 58 Validation Accuracy: 0.9757\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0308\t Training Accuracy: 0.9942\n",
      "Training Loss: 0.0299\t Training Accuracy: 0.9945\n",
      "epoch: 59 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0305\t Training Accuracy: 0.9944\n",
      "Training Loss: 0.0297\t Training Accuracy: 0.9946\n",
      "epoch: 60 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0303\t Training Accuracy: 0.9946\n",
      "Training Loss: 0.0295\t Training Accuracy: 0.9948\n",
      "epoch: 61 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0301\t Training Accuracy: 0.9947\n",
      "Training Loss: 0.0293\t Training Accuracy: 0.9949\n",
      "epoch: 62 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9173\n",
      "Training Loss: 0.0299\t Training Accuracy: 0.9948\n",
      "Training Loss: 0.0292\t Training Accuracy: 0.9949\n",
      "epoch: 63 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9174\n",
      "Training Loss: 0.0297\t Training Accuracy: 0.9949\n",
      "Training Loss: 0.0290\t Training Accuracy: 0.9950\n",
      "epoch: 64 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9174\n",
      "Training Loss: 0.0295\t Training Accuracy: 0.9950\n",
      "Training Loss: 0.0288\t Training Accuracy: 0.9950\n",
      "epoch: 65 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9175\n",
      "Training Loss: 0.0294\t Training Accuracy: 0.9951\n",
      "Training Loss: 0.0286\t Training Accuracy: 0.9951\n",
      "epoch: 66 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9175\n",
      "Training Loss: 0.0292\t Training Accuracy: 0.9952\n",
      "Training Loss: 0.0285\t Training Accuracy: 0.9951\n",
      "epoch: 67 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9175\n",
      "Training Loss: 0.0290\t Training Accuracy: 0.9952\n",
      "Training Loss: 0.0283\t Training Accuracy: 0.9952\n",
      "epoch: 68 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9175\n",
      "Training Loss: 0.0289\t Training Accuracy: 0.9952\n",
      "Training Loss: 0.0282\t Training Accuracy: 0.9952\n",
      "epoch: 69 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9176\n",
      "Training Loss: 0.0287\t Training Accuracy: 0.9953\n",
      "Training Loss: 0.0280\t Training Accuracy: 0.9952\n",
      "epoch: 70 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9176\n",
      "Training Loss: 0.0285\t Training Accuracy: 0.9954\n",
      "Training Loss: 0.0279\t Training Accuracy: 0.9953\n",
      "epoch: 71 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0284\t Training Accuracy: 0.9955\n",
      "Training Loss: 0.0277\t Training Accuracy: 0.9954\n",
      "epoch: 72 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0282\t Training Accuracy: 0.9955\n",
      "Training Loss: 0.0276\t Training Accuracy: 0.9954\n",
      "epoch: 73 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0281\t Training Accuracy: 0.9956\n",
      "Training Loss: 0.0274\t Training Accuracy: 0.9954\n",
      "epoch: 74 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0279\t Training Accuracy: 0.9956\n",
      "Training Loss: 0.0273\t Training Accuracy: 0.9955\n",
      "epoch: 75 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0278\t Training Accuracy: 0.9957\n",
      "Training Loss: 0.0271\t Training Accuracy: 0.9955\n",
      "epoch: 76 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0276\t Training Accuracy: 0.9957\n",
      "Training Loss: 0.0270\t Training Accuracy: 0.9955\n",
      "epoch: 77 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0275\t Training Accuracy: 0.9958\n",
      "Training Loss: 0.0268\t Training Accuracy: 0.9956\n",
      "epoch: 78 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9173\n",
      "Training Loss: 0.0273\t Training Accuracy: 0.9958\n",
      "Training Loss: 0.0267\t Training Accuracy: 0.9956\n",
      "epoch: 79 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0272\t Training Accuracy: 0.9959\n",
      "Training Loss: 0.0266\t Training Accuracy: 0.9957\n",
      "epoch: 80 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0271\t Training Accuracy: 0.9959\n",
      "Training Loss: 0.0264\t Training Accuracy: 0.9957\n",
      "epoch: 81 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0269\t Training Accuracy: 0.9960\n",
      "Training Loss: 0.0263\t Training Accuracy: 0.9958\n",
      "epoch: 82 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0268\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0262\t Training Accuracy: 0.9959\n",
      "epoch: 83 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0266\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0260\t Training Accuracy: 0.9959\n",
      "epoch: 84 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0265\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0259\t Training Accuracy: 0.9959\n",
      "epoch: 85 Validation Accuracy: 0.9763\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0264\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0258\t Training Accuracy: 0.9960\n",
      "epoch: 86 Validation Accuracy: 0.9762\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0262\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0256\t Training Accuracy: 0.9960\n",
      "epoch: 87 Validation Accuracy: 0.9765\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0261\t Training Accuracy: 0.9961\n",
      "Training Loss: 0.0255\t Training Accuracy: 0.9960\n",
      "epoch: 88 Validation Accuracy: 0.9764\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0259\t Training Accuracy: 0.9964\n",
      "Training Loss: 0.0252\t Training Accuracy: 0.9962\n",
      "epoch: 89 Validation Accuracy: 0.9767\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0257\t Training Accuracy: 0.9964\n",
      "Training Loss: 0.0251\t Training Accuracy: 0.9962\n",
      "epoch: 90 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0257\t Training Accuracy: 0.9964\n",
      "Training Loss: 0.0250\t Training Accuracy: 0.9962\n",
      "epoch: 91 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9165\n",
      "Training Loss: 0.0256\t Training Accuracy: 0.9964\n",
      "Training Loss: 0.0250\t Training Accuracy: 0.9962\n",
      "epoch: 92 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9165\n",
      "Training Loss: 0.0255\t Training Accuracy: 0.9964\n",
      "Training Loss: 0.0249\t Training Accuracy: 0.9962\n",
      "epoch: 93 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9164\n",
      "Training Loss: 0.0254\t Training Accuracy: 0.9965\n",
      "Training Loss: 0.0249\t Training Accuracy: 0.9963\n",
      "epoch: 94 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9164\n",
      "Training Loss: 0.0254\t Training Accuracy: 0.9965\n",
      "Training Loss: 0.0248\t Training Accuracy: 0.9963\n",
      "epoch: 95 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9165\n",
      "Training Loss: 0.0253\t Training Accuracy: 0.9965\n",
      "Training Loss: 0.0247\t Training Accuracy: 0.9963\n",
      "epoch: 96 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9166\n",
      "Training Loss: 0.0253\t Training Accuracy: 0.9965\n",
      "Training Loss: 0.0247\t Training Accuracy: 0.9964\n",
      "epoch: 97 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9166\n",
      "Training Loss: 0.0252\t Training Accuracy: 0.9965\n",
      "Training Loss: 0.0246\t Training Accuracy: 0.9964\n",
      "epoch: 98 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0251\t Training Accuracy: 0.9966\n",
      "Training Loss: 0.0246\t Training Accuracy: 0.9964\n",
      "epoch: 99 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0251\t Training Accuracy: 0.9966\n",
      "Training Loss: 0.0245\t Training Accuracy: 0.9964\n",
      "epoch: 100 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9166\n",
      "Training Loss: 0.0250\t Training Accuracy: 0.9966\n",
      "Training Loss: 0.0244\t Training Accuracy: 0.9965\n",
      "epoch: 101 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0249\t Training Accuracy: 0.9966\n",
      "Training Loss: 0.0244\t Training Accuracy: 0.9965\n",
      "epoch: 102 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9166\n",
      "Training Loss: 0.0249\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0243\t Training Accuracy: 0.9965\n",
      "epoch: 103 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0248\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0243\t Training Accuracy: 0.9966\n",
      "epoch: 104 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0248\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0242\t Training Accuracy: 0.9966\n",
      "epoch: 105 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0247\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0242\t Training Accuracy: 0.9966\n",
      "epoch: 106 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9167\n",
      "Training Loss: 0.0246\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0241\t Training Accuracy: 0.9966\n",
      "epoch: 107 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9168\n",
      "Training Loss: 0.0246\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0240\t Training Accuracy: 0.9966\n",
      "epoch: 108 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9169\n",
      "Training Loss: 0.0245\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0240\t Training Accuracy: 0.9966\n",
      "epoch: 109 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0245\t Training Accuracy: 0.9967\n",
      "Training Loss: 0.0239\t Training Accuracy: 0.9966\n",
      "epoch: 110 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0244\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0239\t Training Accuracy: 0.9966\n",
      "epoch: 111 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0244\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0238\t Training Accuracy: 0.9967\n",
      "epoch: 112 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0243\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0238\t Training Accuracy: 0.9967\n",
      "epoch: 113 Validation Accuracy: 0.9768\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0242\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0237\t Training Accuracy: 0.9967\n",
      "epoch: 114 Validation Accuracy: 0.9770\t Adverserial Accuracy: 0.9171\n",
      "Training Loss: 0.0242\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0237\t Training Accuracy: 0.9967\n",
      "epoch: 115 Validation Accuracy: 0.9769\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0241\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0236\t Training Accuracy: 0.9967\n",
      "epoch: 116 Validation Accuracy: 0.9770\t Adverserial Accuracy: 0.9173\n",
      "Training Loss: 0.0241\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0236\t Training Accuracy: 0.9968\n",
      "epoch: 117 Validation Accuracy: 0.9770\t Adverserial Accuracy: 0.9172\n",
      "Training Loss: 0.0240\t Training Accuracy: 0.9968\n",
      "Training Loss: 0.0235\t Training Accuracy: 0.9968\n",
      "epoch: 118 Validation Accuracy: 0.9770\t Adverserial Accuracy: 0.9170\n",
      "Training Loss: 0.0239\t Training Accuracy: 0.9969\n",
      "Training Loss: 0.0234\t Training Accuracy: 0.9969\n",
      "epoch: 119 Validation Accuracy: 0.9770\t Adverserial Accuracy: 0.9169\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 120\n",
    "lr = 0.1\n",
    "p = 3.0\n",
    "\n",
    "Training_dis = np.zeros(shape = (num_epoch))\n",
    "Validation_dis = np.zeros(shape = (num_epoch))\n",
    "Adv_dis = np.zeros(shape = (num_epoch))\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    if i%30==29: \n",
    "        lr = lr/2.0\n",
    "    train_iter.reset()\n",
    "    train_acc = 0.0\n",
    "    loss_total = 0.0\n",
    "    num_batch = 0\n",
    "    \n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        \n",
    "        tmp = discretize(data.asnumpy(),p)\n",
    "        \n",
    "        C_arg_map['data'][:] = tmp\n",
    "        modC.forward(is_train=True)\n",
    "        theta = modC.outputs[0].asnumpy()\n",
    "        alpha = softmax(theta)\n",
    "        \n",
    "        loss_total +=  CLoss(alpha, label.asnumpy())\n",
    "        train_acc += CalAcc(alpha, label.asnumpy())\n",
    "        logGrad = logLossGrad(alpha, label.asnumpy())\n",
    "        \n",
    "        C_out_grad[:] = logGrad\n",
    "        modC.backward([C_out_grad])\n",
    "                \n",
    "        for name in C_arg_names:\n",
    "            if name!='data':\n",
    "                SGD(C_arg_map[name], C_grad_map[name], lr)\n",
    "                \n",
    "        num_batch +=1\n",
    "        if num_batch % 300==299:\n",
    "            print \"Training Loss: %.4f\\t Training Accuracy: %.4f\" %(loss_total/num_batch,train_acc/num_batch)\n",
    "    Training_dis[i] = train_acc/num_batch\n",
    "    Validation_dis[i] = acc_dis(modC, val_iter, C_arg_map,p)\n",
    "    Adv_dis[i] = acc_dis(modC, perb_iter, C_arg_map,p)\n",
    "    print \"epoch: %d Validation Accuracy: %.4f\\t Adverserial Accuracy: %.4f\" \\\n",
    "        %(i, Validation_dis[i], Adv_dis[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_iter.reset()\n",
    "perb_iter.reset()\n",
    "data1 = val_iter.next().data[0]\n",
    "data2 = perb_iter.next().data[0]\n",
    "sample1 = data1[3][0].asnumpy()\n",
    "sample2 = data2[3][0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
