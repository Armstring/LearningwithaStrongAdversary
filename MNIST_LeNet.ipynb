{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev = mx.gpu()\n",
    "batch_size = 100\n",
    "data_shape = (1, 28, 28)\n",
    "\n",
    "train_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/train-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/train-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        flat        = False)\n",
    "\n",
    "val_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/t10k-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/t10k-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        flat        = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(theta):\n",
    "    max_val = np.max(theta, axis=1, keepdims=True)\n",
    "    tmp = theta - max_val\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / norm\n",
    "\n",
    "def MultiHinge(theta, label):\n",
    "    tmp1 = theta-theta[label]\n",
    "    tmp1[label] = -np.inf\n",
    "    tmp2 = np.max(tmp1)+1\n",
    "    return np.max(tmp2, 0)\n",
    "    \n",
    "def MultiHingeGrad(theta, label):\n",
    "    tmp = np.zeros(theta.shape)\n",
    "    tmp1 = theta-theta[label]\n",
    "    tmp1[label] = -np.inf\n",
    "    if np.max(tmp1)+1 < 0:\n",
    "        return tmp\n",
    "    else:\n",
    "        ind = np.argmax(tmp1)\n",
    "        tmp[ind] = 1\n",
    "        tmp[label] = -1\n",
    "        return tmp\n",
    "    \n",
    "def LogLossGrad(alpha, label):\n",
    "    grad = np.copy(alpha)\n",
    "    for i in range(alpha.shape[0]):\n",
    "        grad[i, label[i]] -= 1.\n",
    "    return grad\n",
    "\n",
    "def SGD(weight, grad, lr=0.1, grad_norm=batch_size):\n",
    "    weight[:] -= lr * grad / batch_size\n",
    "\n",
    "def CalAcc(pred_prob, label):\n",
    "    pred = np.argmax(pred_prob, axis=1)\n",
    "    return np.sum(pred == label) * 1.0\n",
    "\n",
    "def CalLoss(pred_prob, label):\n",
    "    loss = 0.\n",
    "    for i in range(pred_prob.shape[0]):\n",
    "        loss += -np.log(max(pred_prob[i, label[i]], 1e-10))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_normal(model, val_iter, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    return(val_acc / num_samp)\n",
    "    \n",
    "def acc_perb_L0(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = np.sign(grad_map[\"data\"].asnumpy())\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "            \n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    if  nn>0:\n",
    "        print('L0 gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)\n",
    "\n",
    "def acc_perb_L2(model, val_iter, coe_pb, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_batch = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /  batch_size \n",
    "        num_batch += 1\n",
    "    if  nn>0:\n",
    "        print('L2 gradien being 0 :', nn)\n",
    "    return(val_acc / num_batch)\n",
    "\n",
    "def acc_perb_alpha(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        T = np.zeros((10, batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "        noise = np.zeros(data.shape)\n",
    "        #===================\n",
    "        for i in range(10):\n",
    "            arg_map[\"data\"][:] = data   \n",
    "            model.forward(is_train=True)\n",
    "            theta = model.outputs[0].asnumpy()\n",
    "            alpha = Softmax(theta)\n",
    "            \n",
    "            grad = LogLossGrad(alpha, i*np.ones(alpha.shape[0]))\n",
    "            for j in range(batch_size):\n",
    "                grad[j] = -alpha[j][i]*grad[j]\n",
    "            out_grad[:] = grad\n",
    "            model.backward([out_grad])\n",
    "            T[i] = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                perb_scale = np.zeros(10)\n",
    "                for i in range(10):\n",
    "                    if (i == y):\n",
    "                        perb_scale[i] = np.inf\n",
    "                    else:\n",
    "                        perb_scale[i] = (alpha[j][y] - alpha[j][i])/np.linalg.norm((T[i][j]-T[y][j]).flatten(),2)\n",
    "                noise[j] = T[np.argmin(perb_scale)][j]-T[y][j]\n",
    "        #====================\n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            else:\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /batch_size\n",
    "        num_samp += 1\n",
    "    if  nn>0:\n",
    "        print('Alpha gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8846\t Train Loss: 0.41799\n",
      "Train Accuracy: 0.9713\t Train Loss: 0.09705\n",
      "Train Accuracy: 0.9808\t Train Loss: 0.06569\n",
      "Train Accuracy: 0.9856\t Train Loss: 0.05111\n",
      "Train Accuracy: 0.9883\t Train Loss: 0.04211\n",
      "Train Accuracy: 0.9902\t Train Loss: 0.03571\n",
      "Train Accuracy: 0.9916\t Train Loss: 0.03078\n",
      "Train Accuracy: 0.9930\t Train Loss: 0.02681\n",
      "Train Accuracy: 0.9940\t Train Loss: 0.02350\n",
      "Train Accuracy: 0.9948\t Train Loss: 0.02069\n",
      "Train Accuracy: 0.9957\t Train Loss: 0.01825\n",
      "Train Accuracy: 0.9962\t Train Loss: 0.01614\n",
      "Train Accuracy: 0.9968\t Train Loss: 0.01433\n",
      "Train Accuracy: 0.9973\t Train Loss: 0.01275\n",
      "Train Accuracy: 0.9978\t Train Loss: 0.01139\n",
      "Train Accuracy: 0.9982\t Train Loss: 0.01021\n",
      "Train Accuracy: 0.9985\t Train Loss: 0.00918\n",
      "Train Accuracy: 0.9988\t Train Loss: 0.00827\n",
      "Train Accuracy: 0.9989\t Train Loss: 0.00748\n",
      "Train Accuracy: 0.9991\t Train Loss: 0.00678\n",
      "Train Accuracy: 0.9992\t Train Loss: 0.00617\n",
      "Train Accuracy: 0.9994\t Train Loss: 0.00563\n",
      "Train Accuracy: 0.9994\t Train Loss: 0.00515\n",
      "Train Accuracy: 0.9995\t Train Loss: 0.00474\n",
      "Train Accuracy: 0.9996\t Train Loss: 0.00437\n",
      "Train Accuracy: 0.9997\t Train Loss: 0.00404\n",
      "Train Accuracy: 0.9998\t Train Loss: 0.00375\n",
      "Train Accuracy: 0.9998\t Train Loss: 0.00349\n",
      "Train Accuracy: 0.9998\t Train Loss: 0.00326\n",
      "Train Accuracy: 0.9999\t Train Loss: 0.00305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    print(\"Train Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy:  0.9921\n",
      "Val Batch Accuracy after pertubation:  0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "val_iter.reset()\n",
    "val_acc = 0.0\n",
    "val_acc_pb = 0.0\n",
    "coe_pb = 1.5\n",
    "num_samp = 0\n",
    "\n",
    "perb_data = []\n",
    "perb_lab = []\n",
    "\n",
    "for dbatch in val_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    #########\n",
    "    grad = LogLossGrad(alpha, label.asnumpy())\n",
    "    out_grad[:] = grad\n",
    "    model.backward([out_grad])\n",
    "    noise = data_grad.asnumpy()\n",
    "    for j in range(batch_size):\n",
    "        noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "    pdata = data.asnumpy() + coe_pb * noise\n",
    "    arg_map[\"data\"][:] = pdata\n",
    "    model.forward(is_train=True)\n",
    "    raw_output = model.outputs[0].asnumpy()\n",
    "    pred = Softmax(raw_output)\n",
    "    val_acc_pb += CalAcc(pred, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "    \n",
    "    perb_data.append(pdata)\n",
    "    perb_lab.append(label.asnumpy())\n",
    "print(\"Val Batch Accuracy: \", val_acc / num_samp)\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc_pb / num_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdata = np.concatenate(perb_data, axis = 0)\n",
    "plabel = np.concatenate(perb_lab, axis = 0)\n",
    "perb_iter = mx.io.NDArrayIter(\n",
    "    data = pdata,\n",
    "    label = plabel,\n",
    "    batch_size = 100,\n",
    "    shuffle = False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Accuracy after pertubation:  0.543\n"
     ]
    }
   ],
   "source": [
    "perb_iter.reset()\n",
    "num_samp = 0\n",
    "val_acc = 0.0\n",
    "for dbatch in perb_iter:\n",
    "    data = dbatch.data[0]\n",
    "    label = dbatch.label[0]\n",
    "    arg_map[\"data\"][:] = data    \n",
    "    \n",
    "    model.forward(is_train=True)\n",
    "    theta = model.outputs[0].asnumpy()\n",
    "    alpha = Softmax(theta)\n",
    "    val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "    num_samp += batch_size\n",
    "print(\"Val Batch Accuracy after pertubation: \", val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8919\t Val Accuracy: 0.9689\t Train Loss: 0.38585\n",
      "Train Accuracy: 0.9732\t Val Accuracy: 0.9795\t Train Loss: 0.09228\n",
      "Train Accuracy: 0.9816\t Val Accuracy: 0.9843\t Train Loss: 0.06318\n",
      "Train Accuracy: 0.9860\t Val Accuracy: 0.9866\t Train Loss: 0.04923\n",
      "Train Accuracy: 0.9887\t Val Accuracy: 0.9871\t Train Loss: 0.04052\n",
      "Train Accuracy: 0.9906\t Val Accuracy: 0.9883\t Train Loss: 0.03427\n",
      "Train Accuracy: 0.9920\t Val Accuracy: 0.9892\t Train Loss: 0.02944\n",
      "Train Accuracy: 0.9932\t Val Accuracy: 0.9895\t Train Loss: 0.02553\n",
      "Train Accuracy: 0.9943\t Val Accuracy: 0.9896\t Train Loss: 0.02228\n",
      "Train Accuracy: 0.9952\t Val Accuracy: 0.9898\t Train Loss: 0.01953\n",
      "Train Accuracy: 0.9959\t Val Accuracy: 0.9907\t Train Loss: 0.01721\n",
      "Train Accuracy: 0.9968\t Val Accuracy: 0.9908\t Train Loss: 0.01523\n",
      "Train Accuracy: 0.9972\t Val Accuracy: 0.9911\t Train Loss: 0.01351\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9912\t Train Loss: 0.01201\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9912\t Train Loss: 0.01072\n",
      "Train Accuracy: 0.9984\t Val Accuracy: 0.9910\t Train Loss: 0.00959\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9910\t Train Loss: 0.00861\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9911\t Train Loss: 0.00776\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9910\t Train Loss: 0.00701\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9911\t Train Loss: 0.00636\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9912\t Train Loss: 0.00579\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9911\t Train Loss: 0.00529\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9912\t Train Loss: 0.00484\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9912\t Train Loss: 0.00446\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9910\t Train Loss: 0.00411\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9911\t Train Loss: 0.00380\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9911\t Train Loss: 0.00353\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9912\t Train Loss: 0.00329\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9913\t Train Loss: 0.00307\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9912\t Train Loss: 0.00287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 30\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00265\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00257\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00254\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00251\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00249\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00247\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00245\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00244\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9913\t Train Loss: 0.00242\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9912\t Train Loss: 0.00240\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9912\t Train Loss: 0.00239\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9912\t Train Loss: 0.00237\n",
      "Train Accuracy: 0.9999\t Val Accuracy: 0.9912\t Train Loss: 0.00236\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00234\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00233\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00231\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00230\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00228\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00227\n",
      "Train Accuracy: 1.0000\t Val Accuracy: 0.9912\t Train Loss: 0.00226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 20\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.01\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name],lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.9912\n",
      "Fixed set perturbation: 0.7456\n",
      "L0 perturbation: 0.8082\n",
      "L2 perturbation: 0.5194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha gradien being 0 : 88\n",
      "Alpha perturbation: 0.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.4f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.4f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.4f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.4f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.4f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=200)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "dropout1 = mx.symbol.Dropout(data=tanh3, p=0.5)\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=dropout1, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8906\t Val Accuracy: 0.9654\t Train Loss: 0.37487\n",
      "Train Accuracy: 0.9666\t Val Accuracy: 0.9790\t Train Loss: 0.11069\n",
      "Train Accuracy: 0.9748\t Val Accuracy: 0.9846\t Train Loss: 0.08163\n",
      "Train Accuracy: 0.9801\t Val Accuracy: 0.9861\t Train Loss: 0.06559\n",
      "Train Accuracy: 0.9827\t Val Accuracy: 0.9874\t Train Loss: 0.05679\n",
      "Train Accuracy: 0.9851\t Val Accuracy: 0.9885\t Train Loss: 0.04814\n",
      "Train Accuracy: 0.9867\t Val Accuracy: 0.9886\t Train Loss: 0.04364\n",
      "Train Accuracy: 0.9880\t Val Accuracy: 0.9895\t Train Loss: 0.03850\n",
      "Train Accuracy: 0.9890\t Val Accuracy: 0.9894\t Train Loss: 0.03575\n",
      "Train Accuracy: 0.9902\t Val Accuracy: 0.9895\t Train Loss: 0.03230\n",
      "Train Accuracy: 0.9910\t Val Accuracy: 0.9892\t Train Loss: 0.02969\n",
      "Train Accuracy: 0.9918\t Val Accuracy: 0.9902\t Train Loss: 0.02639\n",
      "Train Accuracy: 0.9918\t Val Accuracy: 0.9904\t Train Loss: 0.02586\n",
      "Train Accuracy: 0.9929\t Val Accuracy: 0.9902\t Train Loss: 0.02344\n",
      "Train Accuracy: 0.9932\t Val Accuracy: 0.9902\t Train Loss: 0.02105\n",
      "Train Accuracy: 0.9936\t Val Accuracy: 0.9900\t Train Loss: 0.01999\n",
      "Train Accuracy: 0.9943\t Val Accuracy: 0.9894\t Train Loss: 0.01887\n",
      "Train Accuracy: 0.9945\t Val Accuracy: 0.9905\t Train Loss: 0.01751\n",
      "Train Accuracy: 0.9950\t Val Accuracy: 0.9916\t Train Loss: 0.01639\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.9908\t Train Loss: 0.01516\n",
      "Train Accuracy: 0.9956\t Val Accuracy: 0.9907\t Train Loss: 0.01436\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9913\t Train Loss: 0.01289\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9909\t Train Loss: 0.01246\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9907\t Train Loss: 0.01133\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9906\t Train Loss: 0.01151\n",
      "Train Accuracy: 0.9967\t Val Accuracy: 0.9912\t Train Loss: 0.01019\n",
      "Train Accuracy: 0.9973\t Val Accuracy: 0.9906\t Train Loss: 0.00954\n",
      "Train Accuracy: 0.9976\t Val Accuracy: 0.9912\t Train Loss: 0.00814\n",
      "Train Accuracy: 0.9976\t Val Accuracy: 0.9913\t Train Loss: 0.00815\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9906\t Train Loss: 0.00748\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9921\t Train Loss: 0.00700\n",
      "Train Accuracy: 0.9982\t Val Accuracy: 0.9918\t Train Loss: 0.00660\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9920\t Train Loss: 0.00670\n",
      "Train Accuracy: 0.9983\t Val Accuracy: 0.9918\t Train Loss: 0.00592\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9916\t Train Loss: 0.00561\n",
      "Train Accuracy: 0.9985\t Val Accuracy: 0.9923\t Train Loss: 0.00563\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9922\t Train Loss: 0.00516\n",
      "Train Accuracy: 0.9984\t Val Accuracy: 0.9921\t Train Loss: 0.00530\n",
      "Train Accuracy: 0.9985\t Val Accuracy: 0.9919\t Train Loss: 0.00510\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9923\t Train Loss: 0.00434\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9926\t Train Loss: 0.00389\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9914\t Train Loss: 0.00404\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9912\t Train Loss: 0.00384\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9913\t Train Loss: 0.00369\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9920\t Train Loss: 0.00354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 45\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9926\t Train Loss: 0.00275\n",
      "Train Accuracy: 0.9993\t Val Accuracy: 0.9925\t Train Loss: 0.00294\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9925\t Train Loss: 0.00260\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9925\t Train Loss: 0.00233\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9925\t Train Loss: 0.00274\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9923\t Train Loss: 0.00245\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9925\t Train Loss: 0.00241\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9926\t Train Loss: 0.00219\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9925\t Train Loss: 0.00246\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9922\t Train Loss: 0.00230\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9924\t Train Loss: 0.00240\n",
      "Train Accuracy: 0.9994\t Val Accuracy: 0.9925\t Train Loss: 0.00231\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9924\t Train Loss: 0.00214\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9926\t Train Loss: 0.00231\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9922\t Train Loss: 0.00207\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9924\t Train Loss: 0.00206\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9924\t Train Loss: 0.00204\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9923\t Train Loss: 0.00208\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9921\t Train Loss: 0.00199\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00195\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9924\t Train Loss: 0.00203\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00213\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9922\t Train Loss: 0.00216\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00196\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9921\t Train Loss: 0.00181\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00197\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9926\t Train Loss: 0.00189\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9922\t Train Loss: 0.00204\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9923\t Train Loss: 0.00185\n",
      "Train Accuracy: 0.9998\t Val Accuracy: 0.9923\t Train Loss: 0.00180\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9922\t Train Loss: 0.00178\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9921\t Train Loss: 0.00203\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9923\t Train Loss: 0.00184\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00193\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9923\t Train Loss: 0.00188\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9923\t Train Loss: 0.00191\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9925\t Train Loss: 0.00199\n",
      "Train Accuracy: 0.9995\t Val Accuracy: 0.9917\t Train Loss: 0.00219\n",
      "Train Accuracy: 0.9996\t Val Accuracy: 0.9920\t Train Loss: 0.00183\n",
      "Train Accuracy: 0.9997\t Val Accuracy: 0.9922\t Train Loss: 0.00189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 40\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "lr = 0.01\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name],lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.9922\n",
      "Fixed set perturbation: 0.7217\n",
      "L0 perturbation: 0.7694\n",
      "L2 perturbation: 0.5322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha gradien being 0 : 86\n",
      "Alpha perturbation: 0.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.4f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.4f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.4f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.4f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.4f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ian's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=200)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "dropout1 = mx.symbol.Dropout(data=tanh3, p=0.5)\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=dropout1, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "sum_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "sum_map = dict(zip(arg_names, sum_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)\n",
    "for name in arg_names:\n",
    "    sum_map[name][:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8979\t Val Accuracy: 0.9706\t Train Loss: 0.37182\n",
      "Train Accuracy: 0.9708\t Val Accuracy: 0.9814\t Train Loss: 0.10097\n",
      "Train Accuracy: 0.9787\t Val Accuracy: 0.9839\t Train Loss: 0.07308\n",
      "Train Accuracy: 0.9826\t Val Accuracy: 0.9859\t Train Loss: 0.05980\n",
      "Train Accuracy: 0.9843\t Val Accuracy: 0.9874\t Train Loss: 0.05306\n",
      "Train Accuracy: 0.9863\t Val Accuracy: 0.9884\t Train Loss: 0.04731\n",
      "Train Accuracy: 0.9870\t Val Accuracy: 0.9889\t Train Loss: 0.04398\n",
      "Train Accuracy: 0.9880\t Val Accuracy: 0.9887\t Train Loss: 0.03935\n",
      "Train Accuracy: 0.9897\t Val Accuracy: 0.9891\t Train Loss: 0.03555\n",
      "Train Accuracy: 0.9900\t Val Accuracy: 0.9898\t Train Loss: 0.03411\n",
      "Train Accuracy: 0.9910\t Val Accuracy: 0.9902\t Train Loss: 0.03197\n",
      "Train Accuracy: 0.9912\t Val Accuracy: 0.9910\t Train Loss: 0.02973\n",
      "Train Accuracy: 0.9916\t Val Accuracy: 0.9913\t Train Loss: 0.02859\n",
      "Train Accuracy: 0.9924\t Val Accuracy: 0.9918\t Train Loss: 0.02640\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.9912\t Train Loss: 0.02452\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.9910\t Train Loss: 0.02424\n",
      "Train Accuracy: 0.9934\t Val Accuracy: 0.9916\t Train Loss: 0.02317\n",
      "Train Accuracy: 0.9935\t Val Accuracy: 0.9917\t Train Loss: 0.02196\n",
      "Train Accuracy: 0.9942\t Val Accuracy: 0.9914\t Train Loss: 0.02033\n",
      "Train Accuracy: 0.9943\t Val Accuracy: 0.9914\t Train Loss: 0.01963\n",
      "Train Accuracy: 0.9944\t Val Accuracy: 0.9911\t Train Loss: 0.01865\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.9922\t Train Loss: 0.01716\n",
      "Train Accuracy: 0.9951\t Val Accuracy: 0.9923\t Train Loss: 0.01700\n",
      "Train Accuracy: 0.9957\t Val Accuracy: 0.9923\t Train Loss: 0.01532\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9920\t Train Loss: 0.01493\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9920\t Train Loss: 0.01407\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.9928\t Train Loss: 0.01416\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9926\t Train Loss: 0.01332\n",
      "Train Accuracy: 0.9963\t Val Accuracy: 0.9930\t Train Loss: 0.01252\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9926\t Train Loss: 0.01219\n",
      "Train Accuracy: 0.9966\t Val Accuracy: 0.9924\t Train Loss: 0.01191\n",
      "Train Accuracy: 0.9971\t Val Accuracy: 0.9930\t Train Loss: 0.01086\n",
      "Train Accuracy: 0.9972\t Val Accuracy: 0.9927\t Train Loss: 0.01021\n",
      "Train Accuracy: 0.9969\t Val Accuracy: 0.9932\t Train Loss: 0.01077\n",
      "Train Accuracy: 0.9973\t Val Accuracy: 0.9932\t Train Loss: 0.00960\n",
      "Train Accuracy: 0.9972\t Val Accuracy: 0.9930\t Train Loss: 0.00976\n",
      "Train Accuracy: 0.9975\t Val Accuracy: 0.9927\t Train Loss: 0.00922\n",
      "Train Accuracy: 0.9975\t Val Accuracy: 0.9928\t Train Loss: 0.00863\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9929\t Train Loss: 0.00845\n",
      "Train Accuracy: 0.9976\t Val Accuracy: 0.9921\t Train Loss: 0.00864\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9930\t Train Loss: 0.00814\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9929\t Train Loss: 0.00796\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9935\t Train Loss: 0.00722\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9933\t Train Loss: 0.00719\n",
      "Train Accuracy: 0.9980\t Val Accuracy: 0.9932\t Train Loss: 0.00687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 45\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.75\n",
    "lr= 0.05\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        #grad1 = grad_map\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], sum_map[name], lr)\n",
    "            sum_map[name][:] = 0.\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9937\t Train Loss: 0.00561\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9937\t Train Loss: 0.00533\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9938\t Train Loss: 0.00527\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9938\t Train Loss: 0.00502\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9937\t Train Loss: 0.00440\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9936\t Train Loss: 0.00487\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9936\t Train Loss: 0.00496\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9935\t Train Loss: 0.00501\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9935\t Train Loss: 0.00485\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9937\t Train Loss: 0.00478\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9935\t Train Loss: 0.00463\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9936\t Train Loss: 0.00496\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9936\t Train Loss: 0.00450\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9935\t Train Loss: 0.00457\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00447\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9936\t Train Loss: 0.00463\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00461\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9937\t Train Loss: 0.00467\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9937\t Train Loss: 0.00432\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00421\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00446\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9935\t Train Loss: 0.00410\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9936\t Train Loss: 0.00441\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00437\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00403\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9934\t Train Loss: 0.00387\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9934\t Train Loss: 0.00429\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9937\t Train Loss: 0.00439\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00425\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00419\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9935\t Train Loss: 0.00393\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00401\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9938\t Train Loss: 0.00435\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9937\t Train Loss: 0.00413\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00418\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9937\t Train Loss: 0.00373\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9935\t Train Loss: 0.00406\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9936\t Train Loss: 0.00417\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9937\t Train Loss: 0.00410\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9937\t Train Loss: 0.00391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 40\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.75\n",
    "lr= 0.005\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "        #grad1 = grad_map\n",
    "        \n",
    "        noise = np.sign(data_grad.asnumpy())\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                sum_map[name][:] += grad_map[name]\n",
    "\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], sum_map[name], lr)\n",
    "            sum_map[name][:] = 0.\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.9937\n",
      "Fixed set perturbation: 0.9744\n",
      "L0 perturbation: 0.9755\n",
      "L2 perturbation: 0.9066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha gradien being 0 : 78\n",
      "Alpha perturbation: 0.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.4f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.4f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.4f' % acc_perb_L0(model, val_iter, 2,arg_map, grad_map))\n",
    "print('L2 perturbation: %.4f' % acc_perb_L2(model, val_iter, 2,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.4f' % acc_perb_alpha(model, val_iter, 2,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=400)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "dropout1 = mx.symbol.Dropout(data=tanh3, p=0.5)\n",
    "\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=dropout1, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8749\t Val Accuracy: 0.9691\t Train Loss: 0.41803\n",
      "Train Accuracy: 0.9707\t Val Accuracy: 0.9801\t Train Loss: 0.10303\n",
      "Train Accuracy: 0.9767\t Val Accuracy: 0.9830\t Train Loss: 0.07761\n",
      "Train Accuracy: 0.9804\t Val Accuracy: 0.9843\t Train Loss: 0.06567\n",
      "Train Accuracy: 0.9831\t Val Accuracy: 0.9856\t Train Loss: 0.05692\n",
      "Train Accuracy: 0.9849\t Val Accuracy: 0.9870\t Train Loss: 0.05127\n",
      "Train Accuracy: 0.9860\t Val Accuracy: 0.9880\t Train Loss: 0.04645\n",
      "Train Accuracy: 0.9871\t Val Accuracy: 0.9873\t Train Loss: 0.04308\n",
      "Train Accuracy: 0.9883\t Val Accuracy: 0.9881\t Train Loss: 0.04005\n",
      "Train Accuracy: 0.9889\t Val Accuracy: 0.9888\t Train Loss: 0.03809\n",
      "Train Accuracy: 0.9897\t Val Accuracy: 0.9895\t Train Loss: 0.03504\n",
      "Train Accuracy: 0.9900\t Val Accuracy: 0.9889\t Train Loss: 0.03339\n",
      "Train Accuracy: 0.9906\t Val Accuracy: 0.9898\t Train Loss: 0.03234\n",
      "Train Accuracy: 0.9916\t Val Accuracy: 0.9897\t Train Loss: 0.02991\n",
      "Train Accuracy: 0.9917\t Val Accuracy: 0.9900\t Train Loss: 0.02870\n",
      "Train Accuracy: 0.9916\t Val Accuracy: 0.9896\t Train Loss: 0.02712\n",
      "Train Accuracy: 0.9920\t Val Accuracy: 0.9909\t Train Loss: 0.02614\n",
      "Train Accuracy: 0.9925\t Val Accuracy: 0.9902\t Train Loss: 0.02531\n",
      "Train Accuracy: 0.9928\t Val Accuracy: 0.9900\t Train Loss: 0.02377\n",
      "Train Accuracy: 0.9934\t Val Accuracy: 0.9908\t Train Loss: 0.02209\n",
      "Train Accuracy: 0.9937\t Val Accuracy: 0.9907\t Train Loss: 0.02199\n",
      "Train Accuracy: 0.9938\t Val Accuracy: 0.9915\t Train Loss: 0.02116\n",
      "Train Accuracy: 0.9940\t Val Accuracy: 0.9914\t Train Loss: 0.02035\n",
      "Train Accuracy: 0.9941\t Val Accuracy: 0.9914\t Train Loss: 0.01980\n",
      "Train Accuracy: 0.9944\t Val Accuracy: 0.9905\t Train Loss: 0.01932\n",
      "Train Accuracy: 0.9946\t Val Accuracy: 0.9916\t Train Loss: 0.01866\n",
      "Train Accuracy: 0.9950\t Val Accuracy: 0.9915\t Train Loss: 0.01747\n",
      "Train Accuracy: 0.9953\t Val Accuracy: 0.9909\t Train Loss: 0.01653\n",
      "Train Accuracy: 0.9954\t Val Accuracy: 0.9911\t Train Loss: 0.01631\n",
      "Train Accuracy: 0.9952\t Val Accuracy: 0.9918\t Train Loss: 0.01611\n",
      "Train Accuracy: 0.9955\t Val Accuracy: 0.9920\t Train Loss: 0.01531\n",
      "Train Accuracy: 0.9957\t Val Accuracy: 0.9916\t Train Loss: 0.01507\n",
      "Train Accuracy: 0.9958\t Val Accuracy: 0.9919\t Train Loss: 0.01418\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9917\t Train Loss: 0.01370\n",
      "Train Accuracy: 0.9962\t Val Accuracy: 0.9923\t Train Loss: 0.01371\n",
      "Train Accuracy: 0.9967\t Val Accuracy: 0.9923\t Train Loss: 0.01249\n",
      "Train Accuracy: 0.9961\t Val Accuracy: 0.9918\t Train Loss: 0.01293\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9915\t Train Loss: 0.01227\n",
      "Train Accuracy: 0.9964\t Val Accuracy: 0.9917\t Train Loss: 0.01225\n",
      "Train Accuracy: 0.9965\t Val Accuracy: 0.9916\t Train Loss: 0.01216\n",
      "Train Accuracy: 0.9969\t Val Accuracy: 0.9918\t Train Loss: 0.01086\n",
      "Train Accuracy: 0.9970\t Val Accuracy: 0.9920\t Train Loss: 0.01099\n",
      "Train Accuracy: 0.9970\t Val Accuracy: 0.9920\t Train Loss: 0.01030\n",
      "Train Accuracy: 0.9970\t Val Accuracy: 0.9920\t Train Loss: 0.01059\n",
      "Train Accuracy: 0.9971\t Val Accuracy: 0.9921\t Train Loss: 0.01031\n",
      "Train Accuracy: 0.9974\t Val Accuracy: 0.9926\t Train Loss: 0.00969\n",
      "Train Accuracy: 0.9972\t Val Accuracy: 0.9925\t Train Loss: 0.00974\n",
      "Train Accuracy: 0.9974\t Val Accuracy: 0.9927\t Train Loss: 0.00946\n",
      "Train Accuracy: 0.9973\t Val Accuracy: 0.9925\t Train Loss: 0.00917\n",
      "Train Accuracy: 0.9975\t Val Accuracy: 0.9929\t Train Loss: 0.00856\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9928\t Train Loss: 0.00858\n",
      "Train Accuracy: 0.9976\t Val Accuracy: 0.9927\t Train Loss: 0.00854\n",
      "Train Accuracy: 0.9977\t Val Accuracy: 0.9929\t Train Loss: 0.00835\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9923\t Train Loss: 0.00752\n",
      "Train Accuracy: 0.9978\t Val Accuracy: 0.9928\t Train Loss: 0.00758\n",
      "Train Accuracy: 0.9982\t Val Accuracy: 0.9931\t Train Loss: 0.00702\n",
      "Train Accuracy: 0.9979\t Val Accuracy: 0.9929\t Train Loss: 0.00734\n",
      "Train Accuracy: 0.9981\t Val Accuracy: 0.9928\t Train Loss: 0.00699\n",
      "Train Accuracy: 0.9983\t Val Accuracy: 0.9924\t Train Loss: 0.00670\n",
      "Train Accuracy: 0.9982\t Val Accuracy: 0.9923\t Train Loss: 0.00696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 60\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.2\n",
    "lr = 0.1\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name], lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9983\t Val Accuracy: 0.9934\t Train Loss: 0.00601\n",
      "Train Accuracy: 0.9984\t Val Accuracy: 0.9929\t Train Loss: 0.00547\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9930\t Train Loss: 0.00498\n",
      "Train Accuracy: 0.9985\t Val Accuracy: 0.9930\t Train Loss: 0.00512\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9932\t Train Loss: 0.00499\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9930\t Train Loss: 0.00501\n",
      "Train Accuracy: 0.9986\t Val Accuracy: 0.9930\t Train Loss: 0.00506\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9933\t Train Loss: 0.00515\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9932\t Train Loss: 0.00472\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9932\t Train Loss: 0.00488\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9933\t Train Loss: 0.00478\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9933\t Train Loss: 0.00474\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9931\t Train Loss: 0.00430\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9931\t Train Loss: 0.00462\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9930\t Train Loss: 0.00481\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9933\t Train Loss: 0.00480\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9932\t Train Loss: 0.00466\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9929\t Train Loss: 0.00452\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9934\t Train Loss: 0.00454\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9930\t Train Loss: 0.00452\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9929\t Train Loss: 0.00473\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9930\t Train Loss: 0.00431\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9931\t Train Loss: 0.00404\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9930\t Train Loss: 0.00445\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9928\t Train Loss: 0.00431\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9929\t Train Loss: 0.00424\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9929\t Train Loss: 0.00423\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9930\t Train Loss: 0.00423\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9930\t Train Loss: 0.00417\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9931\t Train Loss: 0.00361\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9929\t Train Loss: 0.00426\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9928\t Train Loss: 0.00382\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9933\t Train Loss: 0.00426\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9931\t Train Loss: 0.00409\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9930\t Train Loss: 0.00401\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9930\t Train Loss: 0.00402\n",
      "Train Accuracy: 0.9987\t Val Accuracy: 0.9930\t Train Loss: 0.00430\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9932\t Train Loss: 0.00416\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9930\t Train Loss: 0.00420\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9932\t Train Loss: 0.00363\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9935\t Train Loss: 0.00350\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9932\t Train Loss: 0.00402\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9931\t Train Loss: 0.00366\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9933\t Train Loss: 0.00405\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9927\t Train Loss: 0.00390\n",
      "Train Accuracy: 0.9992\t Val Accuracy: 0.9931\t Train Loss: 0.00354\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9931\t Train Loss: 0.00381\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9932\t Train Loss: 0.00357\n",
      "Train Accuracy: 0.9988\t Val Accuracy: 0.9933\t Train Loss: 0.00403\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9931\t Train Loss: 0.00382\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9933\t Train Loss: 0.00396\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9932\t Train Loss: 0.00394\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9932\t Train Loss: 0.00393\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9931\t Train Loss: 0.00398\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9931\t Train Loss: 0.00365\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9933\t Train Loss: 0.00407\n",
      "Train Accuracy: 0.9989\t Val Accuracy: 0.9932\t Train Loss: 0.00398\n",
      "Train Accuracy: 0.9991\t Val Accuracy: 0.9934\t Train Loss: 0.00368\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9936\t Train Loss: 0.00385\n",
      "Train Accuracy: 0.9990\t Val Accuracy: 0.9934\t Train Loss: 0.00359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round = 60\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "coe_pb = 1.2\n",
    "lr = 0.01\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        \n",
    "        noise = data_grad.asnumpy()\n",
    "        for j in range(batch_size):\n",
    "            noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name], lr)\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    val_acc = acc_normal(model, val_iter,arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.4f\\t Val Accuracy: %.4f\\t Train Loss: %.5f\" % (train_acc, val_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Validation: 0.9934\n",
      "Fixed set perturbation: 0.9822\n",
      "L0 perturbation: 0.9859\n",
      "L2 perturbation: 0.9632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha gradien being 0 : 79\n",
      "Alpha perturbation: 0.9627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print('Normal Validation: %.4f' % acc_normal(model,val_iter,arg_map, grad_map))\n",
    "print('Fixed set perturbation: %.4f' % acc_normal(model, perb_iter,arg_map, grad_map))\n",
    "print('L0 perturbation: %.4f' % acc_perb_L0(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('L2 perturbation: %.4f' % acc_perb_L2(model, val_iter, 1.5,arg_map, grad_map))\n",
    "print('Alpha perturbation: %.4f' % acc_perb_alpha(model, val_iter, 1.5,arg_map, grad_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
