{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = mx.gpu()\n",
    "batch_size = 100\n",
    "data_shape = (1, 28, 28)\n",
    "\n",
    "train_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/train-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/train-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = True,\n",
    "        flat        = False)\n",
    "\n",
    "val_iter = mx.io.MNISTIter(\n",
    "        image       = \"../data/mnist/t10k-images-idx3-ubyte\",\n",
    "        label       = \"../data/mnist/t10k-labels-idx1-ubyte\",\n",
    "        input_shape = data_shape,\n",
    "        batch_size  = batch_size,\n",
    "        flat        = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(theta):\n",
    "    max_val = np.max(theta, axis=1, keepdims=True)\n",
    "    tmp = theta - max_val\n",
    "    exp = np.exp(tmp)\n",
    "    norm = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / norm\n",
    "\n",
    "def SoftmaxGrad(arr, idx):\n",
    "    grad = np.copy(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        p = grad[i, idx]\n",
    "        grad[i, :] *= -p\n",
    "        grad[i, idx] = p * (1. - p)\n",
    "    return grad\n",
    "\n",
    "def LogLossGrad(alpha, label):\n",
    "    grad = np.copy(alpha)\n",
    "    for i in range(alpha.shape[0]):\n",
    "        grad[i, label[i]] -= 1.\n",
    "    return grad\n",
    "\n",
    "def SGD(weight, grad, lr=0.1, grad_norm=batch_size):\n",
    "    weight[:] -= lr * grad / batch_size\n",
    "\n",
    "def CalAcc(pred_prob, label):\n",
    "    pred = np.argmax(pred_prob, axis=1)\n",
    "    return np.sum(pred == label) * 1.0\n",
    "\n",
    "def CalLoss(pred_prob, label):\n",
    "    loss = 0.\n",
    "    for i in range(pred_prob.shape[0]):\n",
    "        loss += -np.log(max(pred_prob[i, label[i]], 1e-10))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acc_normal(model, val_iter, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=False)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        val_acc += CalAcc(alpha, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    return(val_acc / num_samp)\n",
    "    \n",
    "def acc_perb_L0(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = np.sign(grad_map[\"data\"].asnumpy())\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])):\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "            \n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) \n",
    "        num_samp += batch_size\n",
    "    if  nn>0:\n",
    "        print('L0 gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)\n",
    "\n",
    "def acc_perb_L2(model, val_iter, coe_pb, arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_batch = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        \n",
    "        grad = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = grad\n",
    "        model.backward([out_grad])\n",
    "        noise = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): #1ï¼š #\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "            else:\n",
    "                noise[j] = 0\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /  batch_size \n",
    "        num_batch += 1\n",
    "    if  nn>0:\n",
    "        print('L2 gradien being 0 :', nn)\n",
    "    return(val_acc / num_batch)\n",
    "\n",
    "\n",
    "def acc_perb_alpha(model, val_iter, coe_pb,arg_map, grad_map):\n",
    "    val_iter.reset()\n",
    "    val_acc = 0.0\n",
    "    num_samp = 0\n",
    "    nn=0\n",
    "    for dbatch in val_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        batch_size = label.asnumpy().shape[0]\n",
    "        arg_map[\"data\"][:] = data    \n",
    "\n",
    "        T = np.zeros((10, batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "        noise = np.zeros(data.shape)\n",
    "        #===================\n",
    "        for i in range(10):\n",
    "            arg_map[\"data\"][:] = data   \n",
    "            model.forward(is_train=True)\n",
    "            theta = model.outputs[0].asnumpy()\n",
    "            alpha = Softmax(theta)\n",
    "            \n",
    "            grad = LogLossGrad(alpha, i*np.ones(alpha.shape[0]))\n",
    "            for j in range(batch_size):\n",
    "                grad[j] = -alpha[j][i]*grad[j]\n",
    "            out_grad[:] = grad\n",
    "            model.backward([out_grad])\n",
    "            T[i] = grad_map[\"data\"].asnumpy()\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            y = label.asnumpy()[j]\n",
    "            if (y == np.argmax(alpha[j])): \n",
    "                perb_scale = np.zeros(10)\n",
    "                for i in range(10):\n",
    "                    if (i == y):\n",
    "                        perb_scale[i] = np.inf\n",
    "                    else:\n",
    "                        perb_scale[i] = (alpha[j][y] - alpha[j][i])/np.linalg.norm((T[i][j]-T[y][j]).flatten(),2)\n",
    "                noise[j] = T[np.argmin(perb_scale)][j]-T[y][j]\n",
    "        #====================\n",
    "        for j in range(batch_size):\n",
    "            if np.linalg.norm(noise[j].flatten(),2) ==0:\n",
    "                nn+=1\n",
    "            else:\n",
    "                noise[j] = noise[j]/np.linalg.norm(noise[j].flatten(),2)\n",
    "        pdata = data.asnumpy() + coe_pb * noise\n",
    "        arg_map[\"data\"][:] = pdata\n",
    "        model.forward(is_train=False)\n",
    "        raw_output = model.outputs[0].asnumpy()\n",
    "        pred = Softmax(raw_output)\n",
    "        \n",
    "        val_acc += CalAcc(pred, label.asnumpy()) /batch_size\n",
    "        num_samp += 1\n",
    "    if  nn>0:\n",
    "        print('Alpha gradien being 0 :', nn)\n",
    "    return(val_acc / num_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.symbol.Variable('data')\n",
    "# first conv\n",
    "conv1 = mx.symbol.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.symbol.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.symbol.Pooling(data=tanh1, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# second conv\n",
    "conv2 = mx.symbol.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.symbol.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.symbol.Pooling(data=tanh2, pool_type=\"max\",\n",
    "                              kernel=(2,2), stride=(2,2))\n",
    "# first fullc\n",
    "flatten = mx.symbol.Flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=100)\n",
    "tanh3 = mx.symbol.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_shape = (batch_size, 1, 28, 28)\n",
    "arg_names = fc2.list_arguments() # 'data' \n",
    "arg_shapes, output_shapes, aux_shapes = fc2.infer_shape(data=data_shape)\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "grad_arrays = [mx.nd.zeros(shape, ctx=dev) for shape in arg_shapes]\n",
    "reqs = [\"write\" for name in arg_names]\n",
    "\n",
    "model = fc2.bind(ctx=dev, args=arg_arrays, args_grad = grad_arrays, grad_req=reqs)\n",
    "arg_map = dict(zip(arg_names, arg_arrays))\n",
    "grad_map = dict(zip(arg_names, grad_arrays))\n",
    "data_grad = grad_map[\"data\"]\n",
    "out_grad = mx.nd.zeros(model.outputs[0].shape, ctx=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in arg_names:\n",
    "    if \"weight\" in name:\n",
    "        arr = arg_map[name]\n",
    "        arr[:] = mx.rnd.uniform(-0.07, 0.07, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.880\t Val Aacc: 0.968\t Train Loss: 0.41817\n",
      "Train Accuracy: 0.972\t Val Aacc: 0.980\t Train Loss: 0.09560\n",
      "Train Accuracy: 0.981\t Val Aacc: 0.984\t Train Loss: 0.06474\n",
      "Train Accuracy: 0.986\t Val Aacc: 0.986\t Train Loss: 0.05038\n",
      "Train Accuracy: 0.989\t Val Aacc: 0.987\t Train Loss: 0.04139\n",
      "Train Accuracy: 0.991\t Val Aacc: 0.988\t Train Loss: 0.03496\n",
      "Train Accuracy: 0.992\t Val Aacc: 0.989\t Train Loss: 0.03003\n",
      "Train Accuracy: 0.993\t Val Aacc: 0.989\t Train Loss: 0.02603\n",
      "Train Accuracy: 0.994\t Val Aacc: 0.990\t Train Loss: 0.02270\n",
      "Train Accuracy: 0.995\t Val Aacc: 0.990\t Train Loss: 0.01990\n",
      "Train Accuracy: 0.996\t Val Aacc: 0.990\t Train Loss: 0.01751\n",
      "Train Accuracy: 0.996\t Val Aacc: 0.990\t Train Loss: 0.01545\n",
      "Train Accuracy: 0.997\t Val Aacc: 0.991\t Train Loss: 0.01368\n",
      "Train Accuracy: 0.998\t Val Aacc: 0.991\t Train Loss: 0.01215\n",
      "Train Accuracy: 0.998\t Val Aacc: 0.991\t Train Loss: 0.01083\n",
      "Train Accuracy: 0.998\t Val Aacc: 0.991\t Train Loss: 0.00969\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00871\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00785\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00711\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00647\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00590\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00541\n",
      "Train Accuracy: 0.999\t Val Aacc: 0.991\t Train Loss: 0.00497\n",
      "Train Accuracy: 1.000\t Val Aacc: 0.991\t Train Loss: 0.00458\n",
      "Train Accuracy: 1.000\t Val Aacc: 0.991\t Train Loss: 0.00423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_round =25\n",
    "train_acc = 0.\n",
    "nbatch = 0\n",
    "for i in range(num_round):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    nbatch = 0\n",
    "    train_iter.reset()\n",
    "    for dbatch in train_iter:\n",
    "        data = dbatch.data[0]\n",
    "        label = dbatch.label[0]\n",
    "        arg_map[\"data\"][:] = data\n",
    "        model.forward(is_train=True)\n",
    "        theta = model.outputs[0].asnumpy()\n",
    "        alpha = Softmax(theta)\n",
    "        train_acc += CalAcc(alpha, label.asnumpy()) / batch_size\n",
    "        train_loss += CalLoss(alpha, label.asnumpy()) / batch_size\n",
    "        losGrad_theta = LogLossGrad(alpha, label.asnumpy())\n",
    "        out_grad[:] = losGrad_theta\n",
    "        model.backward([out_grad])\n",
    "        for name in arg_names:\n",
    "            if name != \"data\":\n",
    "                SGD(arg_map[name], grad_map[name])\n",
    "        \n",
    "        nbatch += 1\n",
    "    train_acc /= nbatch\n",
    "    train_loss /= nbatch\n",
    "    valid_acc = acc_normal(model, val_iter,  arg_map, grad_map)\n",
    "    print(\"Train Accuracy: %.3f\\t Val Aacc: %.3f\\t Train Loss: %.5f\" % (train_acc, valid_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:19: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:141: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9906\t L2:| 0.9906\t Alpha: 0.9906\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9850\t L2:| 0.9799\t Alpha: 0.9793\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9754\t L2:| 0.9555\t Alpha: 0.9545\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9587\t L2:| 0.9159\t Alpha: 0.9154\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9383\t L2:| 0.8540\t Alpha: 0.8499\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.9121\t L2:| 0.7692\t Alpha: 0.7618\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.8729\t L2:| 0.6736\t Alpha: 0.6604\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.8221\t L2:| 0.5645\t Alpha: 0.5482\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.7685\t L2:| 0.4662\t Alpha: 0.4502\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.7064\t L2:| 0.3840\t Alpha: 0.3660\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.6372\t L2:| 0.3117\t Alpha: 0.3005\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.5711\t L2:| 0.2585\t Alpha: 0.2498\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.5052\t L2:| 0.2122\t Alpha: 0.2036\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.4414\t L2:| 0.1791\t Alpha: 0.1706\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.3845\t L2:| 0.1534\t Alpha: 0.1447\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.3326\t L2:| 0.1281\t Alpha: 0.1211\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.2917\t L2:| 0.1085\t Alpha: 0.1049\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.2537\t L2:| 0.0918\t Alpha: 0.0899\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.2185\t L2:| 0.0795\t Alpha: 0.0789\t\n",
      "Alpha gradien being 0 : 94\n",
      "L0: 0.1900\t L2:| 0.0678\t Alpha: 0.0684\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:143: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    scale = 0.2*i    \n",
    "    print('L0: %.4f\\t L2:| %.4f\\t Alpha: %.4f\\t' % (acc_perb_L0(model, val_iter, scale, arg_map, grad_map), acc_perb_L2(model, val_iter, scale, arg_map, grad_map), acc_perb_alpha(model, val_iter, scale, arg_map, grad_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
